\chapter{Circuit Compiler} 
% finite field arithmetics https://johnkerl.org/doc/ffcomp.pdf
It is possible to verify the the correct execution of arbitrary bounded computation in terms of rank-$1$ contraints systems and it is possible to encode such computations in terms of algebraic circuits. However writing actual computer programs as circuits and the associated verification in terms of rank-1 constraint systems is as least as troublesome as writing any other low level language like assembler code. 

From a practical point of view it is therefore necessary to have some kind of compiler framework at hand, capable to transform high level languages into arithmetic circuits and associated rank-1 constraint systems. 

As we have seen in XXX as well as XXX and XXX, both arithmetic circuits and rank-1 constraint systems have a modularity property by which it is possible to synthezise  complex circuits and constraint systems from simpler ones. A basic approach taken by circuit/R1CS compilers like (LIST) is therefore to provide a library of atomic and simple circuits and then define a way to combine them into arbitrary complex systems. 

In this chapter we will provide an introduction to basic concepts of so called \textit{circuit/R1CS compilers} and derive a toy language which we can "compile" in a pen and paper approach into algebraic circuits and their associated rank-1 constraints systems. The language is very simple, informal, somewhat typed and similar to the hardware description language paradigm but without a notion of time. 

We start with a general introduction to our language and then intoduce atomic types like booleans, unsigned integers and define the fundamental control flow primitives like the ternery operator or the bounded loop. We will also look at basic functionality primitives like elliptic curve cryptography and cryptographic hash functions. Primitives like those are often called \textbf{gadgets} in the literature. We also give a very shallow introduction to how compilers for complex circuit design might be constructed, also we only scratch the surface. In particular we focus on a kind of typed hardware description language style of compilers and will only look into more elaborate approaches like RAM-model compilers in future versions of the book.

From a developers perspective, it is desireable to design statements in formal languages like normal computer programs and then let a compiler handle the technical details. 

However in contrast to normal executable programs, programs for circuit compilers have two modes of execution. The first mode is usally called \textit{setup phase} is executed in order to generate the circuit and its associated rank-1 constraint system, the letter of which is then usually used as input to some zero knowledge proofing system.

The second mode of execution is usually called the \textit{proofer phase} and in this phase a proofer usually computes a valid assignment to the circuit. Depending on the usecase this valid assignment is then either directly used as constructive proof for proper circuit execution or is transfered as input to the proof generation algorithm of some zero knowledge proofing system, where the full size, non hiding constructive proof is processed into a succinct proof with various levels of zero-knowledge.

Modern circuit languages and their associated compilers abstract over those two phase and provide a unified interphase to the developer, who then writes a single program that can be used in both phases. 

Since both algebraic circuits and their associated rank-1 constraint systems are defined over a finite field, elements from that field are the atomic informational units in those models. In this sense field elements $x\in \F$ are for algebraic circuits what bits are for computers. 

In contrast to other well known models of computation, algebraic circuits have no notion of memory and all assignments of values to variables are immutable. It is true however that mutability can be simulated in high level languages, by transfering the problem to the compiler. In addition, algebraic circuits have no notion of branching 

\subsection{The PAPER Language} To explain basic concepts of circuit compiler and their associated high level languages, we derive a mostly informal toy language and associates brain compiler named \texttt{PAPER} (\textbf{P}en \textbf{A}nd \textbf{P}aper \textbf{E}xecution \textbf{R}ules). \texttt{PAPER} allows programmers to define statements in Rust-like pseudo-code and provides a simple set of rules to compile the high level description into pen and paper circuits. It is mostly inspired by \texttt{ZOKRATES} and \texttt{circom}.

In \texttt{PAPER} any statement is defined as an ordered list of functions, where any function has to be declared in the list before it is called in other functions of that list. The last entry in a statement has to be a special function, called \texttt{main}. Any statement is parameterized over the field that the circuit will be defined over and it has optinal parameters of unsigned type. The following definition makes this precise: 
% used the command line style formalized e.g. here: http://docopt.org/
\begin{lstlisting}
statement <Name> {F:<Field> [ , <N_1: unsigned>,... ] } {
  [def <Name>([[pub]<Arg>:<Type>,...])->([[pub]<Rslt>:<Type>,...]){
    [ rslt <== expression(arg) ;... ]
  } ;...]
  def main([[pub]<Arg>:<Type>,...])->(){
    [ expression(Arg,...) ;... ]
  } ;
}
\end{lstlisting}
Function arguments are private by default but can be declared as public by the \texttt{pub} specifier. Declaring variables as public always overwrites any previous or conflicting private declaration. 

\begin{lstlisting}
fn <Name>([[pub]<Arg>:<Type>,...])->([pub]<Rslt>:<Type>){
  [ constant <Const>:<Type>=<Value> ;... ]
  Rslt<==(fn([<Arg>|<Const>,...])|(<Arg>|<Const>)) ;
}
\end{lstlisting}

In \texttt{PAPER} any function is a high level description of a circuit. The compiler draws a box-node for every argument of a function 

of the circuit labeled with the arguments name that represent variables and there are no other source nodes labeled with variables in the functions circuit. 

If the function has return values, they are compiled to sink nodes of the circuit labeled with the result name that represent variables and there are no other sink nodes labeled with variables in the functions circuit. 

The compiler draws a box-node for every constant that is declared in the function and labels it with the value of that constant.

THEN ASSOCIATES CONSTRAINT CIRCUITS TO THE TYPES. THEN VALIDITY AND TYPE CHECK

The compiler checks, if every result value is the left side of an \texttt{<==} operator. If the associated right side is not a function an edge is drawn from the right side to the left side. LABEL!!! 

If the right side is a function, the compiler substitutes the box-node that represents that function. If that function is in the \texttt{PAPER} library, the box node will be replaced by the circuit from the library. 

The compiler draws a box-node for every occurence of the \texttt{<==} operator and lables it with the right side of that operator. It then draws an edge to the node on the left side of the operator. If the left side represents a private result it gets label $W$ and if public it gets label $I$ indexed by a unique symbol.

If the right side of the \texttt{<==} operator is another function, the compiler substitutes the box-node that represents that function. If that function is in the \texttt{PAPER} library, the box node will be replaced by the circuit from the library. 

OPTIMIZATION: COLLAPSING BOX NODES IN PATHS.

\texttt{PAPER} is then build on simple substitution rules and a circuit library that provides basic circuit building blocks.  









\begin{itemize}
\item Compilation starts with an empty circuit and on function \texttt{main}.
\begin{itemize}
\item  draw an input node and label the node with the name of the input.
\item If the input is a variable draw the edge label $S_j$ on ever outgoing edge
\item draw the constraining circuit of the type
\end{itemize}
\item For every output:
\begin{itemize}
\item  draw an output node and label the node with the name of the out.
\item If the output is a variable draw the edge label $S_j$ on the ingoing edge
\item If the output is a constant draw the edge label $S_value$ on its ingoing edge
\end{itemize}
\item check if all modules are defined
\item type check the module body 
\item Draw edges from the input nodes to the output nodes by inductively rewriting all modules in the module body with their associated circuits and replace all edges label with witness labels.
\end{itemize}
\begin{example}[A trivial Circuit] To give an intuition of how to write and compile circuits in the \texttt{PAPER} language, consider the following statement description:
\begin{lstlisting}
statement trivial_circuit {F:F_13} {
  def main{F}(in1 : F, pub in2 : F) -> (out1:F, out2:F){

    let const outc1 : F = 0 ;   
    let const inc1 : F = 7 ;

    out1 <== inc1;
    out2 <== in1;
    outc1 <== in2;
  }
}
\end{lstlisting} 
To compile this statement into an algebraic circuit we use \texttt{PAPER}. According to those rule we start with an empty circuit evaluate function \texttt{main}, which is the only function in this statement. 

We then draw box-nodes for every argument and every return value of the function as well as for every constant that was declared in the function. Then we have to draw a constraint circuit for every type. As we will see in paragraph XXX, the field type has no associated constraints, so we don't need to draw any circuits associated the types of the variables. 

After that we check that there is one \texttt{<==} operator for every result value of the function. We then check the validity of every expression in the \texttt{circuit} section of the module, including a type check. Since the circuit only wires inputs to outputs and all inputs are of the same type as all outputs, the check is valid.

We evaluate those operators. Since in our case the right side of each edge operator is not a function, we draw edges from the box-nodes on the right side to the associated box node on the left side. We get the following edges
\begin{align*}
E(inc1, out1) \\
E(in1, out2) \\
E(in2, outc1)
\end{align*}
To label those edges, we use the general rules of algebraic circuits as defined in XXX. According to those rules every incoming edge of a sink node has a label and every outgoing edge of a source node has a label, if the node is labeled with a variable.

Since nodes that represent constants are implicitly assumed to be private and since the private public specifiers determine if we have to choose the symbol $W$ or $I$ for the edge labels, we get the following circuit:
\begin{center}
\digraph[scale=0.4]{TRIVIAL1}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 [shape=box, label="in_1"];
	n2 [shape=box, label="in_2"];
	n3 [shape=box, label="7"];
	n4 [shape=box, label="out_1"];
	n5 [shape=box, label="out_2"];
	n6 [shape=box, label="0"];
	
	n1 -> n5 [xlabel="W_(in_1)"] ;
	n2 -> n6 [xlabel="I_(in_2)"] ;
	n3 -> n4 ;
}
\end{center}
\end{example}
\subsection{Primitive Types} 
% https://zeroknowledge.fm/172-2/ reference for all the languages
Primitive data types like booleans, (unsigned) integers, or strings  are the most basic building blocks one might expect in every general high level programing language. In order to write statements as computer programs that compile into circuits, it is therefore necessary to implement primitive types as constraints systems and define their associated operations as circuits.

In this section we will look at some common ways to achieve this. After a recapitulation of the atomic type of prime field elements, we start with an implementation of the boolean type and its associated boolean algebra as circuits. After that we define unsigned integers on top of the boolean type. and leave the implementation of signed integers as an exercise to the reader. 

It should be noted however that while in common programing languages like C, Go, or Rust primitive data types have a one-to-one correspondence with objects in the computer's memory. This is different for most languages that compile into algebraic circuits. As we will see in the following paragraphs, common primitives like booleans or unsigned integers require many constraints and memory. Primitives different from the underlying field elements can be expensive.

\subsubsection{The Basefield type} 
The most basic type of any algebraic circuit is the type $\F$ of its underlaying field. Field elements $x\in\F$ are for algebraic circuits what machine words are for computers. They are the fundamental computational units that everything else is expressed in and no circuit or constraint is needed to represent this type. 

In \texttt{PAPER} we write \texttt{F} for this type and specify the actual field instance for every statement in curly brackets after the name of that statement. Two functions are associated to this type, which are induced by the \textit{addition} and \textit{multiplication} law in the field \texttt{F}. We write
\begin{equation}
\mathtt{MUL}:\; \mathtt{F} \times \mathtt{F} \to \mathtt{F}\;;\; (x,y) \mapsto \mathtt{MUL}(x,y)
\end{equation}
\begin{equation}
\mathtt{ADD}:\; \mathtt{F} \times \mathtt{F} \to \mathtt{F}\;;\; (x,y) \mapsto \mathtt{ADD}(x,y)
\end{equation}
Circuit compilers have to compile these functions into the algebraic gates, as explained in XXX. Every other function has to be expressed in terms of them and proper wireing.

To represent addition and multiplication in the \texttt{PAPER} language, we defin the following two functions:
\begin{lstlisting}
fn MUL(x : F, y : F) -> (MUL(x,y):F){}
\end{lstlisting}  
\begin{lstlisting}
fn ADD(x : F, y : F) -> (ADD(x,y):F){}
\end{lstlisting}
The compiler then compiles every occurence of the $\mathtt{MUL}$ or the $\mathtt{ADD}$ function into the following circuits:
\begin{center}
\digraph[scale=0.5]{FNMUL}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 [shape=box, label="x"];
	n2 [shape=box, label="y"];
	n3 [label="*"];	
	n4 [label="MUL(x,y)"];
	
	n1 -> n3 [xlabel="W_x"] ;
	n2 -> n3 [xlabel="W_y"];
	n3 -> n4 [xlabel="W_STUFF"] ;
	
	n5 [shape=box, label="x"];
	n6 [shape=box, label="y"];
	n7 [shape=box, label="+"];	
	n8 [shape=box, label="ADD(x,y)"];
	
	n5 -> n7 [xlabel="W_x"] ;
	n6 -> n7 [xlabel="W_y"];
	n7 -> n8 [xlabel="W_STUFF"] ;
}
\end{center}
\begin{example}[Basic gates] To give an intuition of how \texttt{PAPER} compiles field addition and multiplication into circuit gates, consider the following program:
\begin{lstlisting}
module basic_ops {F_13} {
	input private in_1 : F ; 
	input public in_2 : F ; 
	output public product : F ;
	output private sum : F ; 

	circuit
		product <== MUL(in_1,in_2) ;
		sum <== ADD(in_1,in_2) ;
}
\end{lstlisting} 

\begin{lstlisting}
def basic_ops<F_13>(in_1 : F, in_2 : F)-> (F,F) {  

    let x : F 

	circuit
		prod == MUL(in_1,in_2) ;
		sum <== ADD(in_1,in_2) ;
}
\end{lstlisting} 

Using \texttt{PAPER}, we start with an empty circuit and then add $2$ input nodes and $2$ output nodes to that circuit. All these nodes are decorated with the associated variable names. 

After that we evaluate the \texttt{circuit} descriptors. In the first descriptor we have one multiplication operator and with \texttt{PAPER} we replace that operator by a multiplication gate in the circuit, such that its outgoing edge is connected to the node label with \texttt{product} and the incoming edges are connected to the nodes labeled with \texttt{in\_1} and \texttt{in\_2}. We proceed accordingly for the second descriptor. We get the following circuit: 
\begin{center}
\digraph[scale=0.5]{SIMPLEMUL}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 [shape=box, label="in_1"];
	n2 [shape=box, label="in_2"];
	n3 [shape=box, label="product"];
	n4 [shape=box, label="sum"];
	n5 [shape=box, label="*"];
	n6 [shape=box, label="+"];	
	
	n1 -> n5 [xlabel="W_(in_1)"] ;
	n1 -> n6 [xlabel="W_(in_1)"];
	n2 -> n5 [xlabel="I_(in_2)"] ;
	n2 -> n6 [xlabel="I_(in_2)"];
	n5 -> n3 [xlabel="W_1 = I_(product)"] ;
	n6 -> n4 [xlabel="W_(sum)"];
}
\end{center} 
Using the general process of deriving an associated rank-1 constraint system to this circuit, we get
\begin{align*}
W_{in_1} \cdot I_{in_2} &= I_{product} \\
( W_{in_1} + I_{in_2} ) \cdot 1 &= W_{sum}
\end{align*}
\end{example} 

\begin{example}[$3$-factorization] Consider our $3$-factorization problem from example XXX and the associated circuit $C_{3.fac\_zk}(\F_{13})$ we provided in example XXX. We want to define a module, that we can compile into an algebraic circuit equivalent to $C_{3.fac\_zk}(\F_{13})$. We write
\begin{lstlisting}
module 3_fac_zk (F_13) (
	input private x_1 : F ; 
	input private x_2 : F ;
	input private x_3 : F ;  
	output public f_3.fac_zk : F ; 

	circuit
		f_3.fac_zk <== MUL( MUL( x_1 , x_2 ) , x_3 );
)
\end{lstlisting} 
Using \texttt{PAPER}, we start with an empty circuit and then add $3$ input nodes and $1$ output node to that circuit. All these nodes are decorated with the associated variable names. 

After that we evaluate the single \texttt{circuit} descriptor. We have two nested multiplication operators and we replace them by multiplication gates, starting with the most outer operator. We get: 
\begin{center}
\digraph[scale=0.5]{PAPER3FUC}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 [shape=box, label="x_1"];
	n2 [shape=box, label="x_2"];
	n3 [shape=box, label="x_3"];
	n4 [shape=box, label="f_3.fac_zk"];
	n5 [shape=box, label="*"];
	n6 [shape=box, label="*"];	
	
	n1 -> n5 [xlabel="W_(x_1)"] ;
	n2 -> n5 [xlabel="W_(x_2)"] ;
	n3 -> n6 [xlabel="W_(x_3)"];
	n5 -> n6 [xlabel="W_1"] ;
	n6 -> n4 [xlabel="W_2 = I_(f_3.fac_zk)"];
}
\end{center} 
Using the general process of deriving an associated rank-1 constraint system to this circuit, we get
\begin{align*}
W_{x_1} \cdot I_{x_2} &= W_{1} \\
W_{1} \cdot W_{x_3}   &= I_{f\_3.fac\_zk}
\end{align*}
\end{example}
\paragraph{The Subtraction Constraints System} By definition, algebraic circuits only contain addition and multiplication gates and it follows thar there is no single gate for field subtraction, despite the fact that subtraction is a native operation in every field.

High level languages and their associated circuit compilers therefore need another way to deal with subtraction. To see how this can be achieved, recall that subtraction is defined by addition with the additive inverse and that the inverse can be computed efficiently by multiplication with $-1$. A circuit for field subtraction is therefore given by
\begin{center}
\digraph[scale=0.4]{BTSUB}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n5 [xlabel="S_1    "];
	n2 -> n4 [xlabel="S_2    "];
	n3 -> n4 ;
	n4 -> n5 ;
	n5 -> n6 [xlabel="S_3    "];
	n1 [shape=box, label="x"];
	n2 [shape=box, label="y"];
	n3 [shape=box, label="-1"];
	n4 [label="*"];	
	n5 [label="+"];
	n6 [shape=box, label="SUB(x,y)"]
}
\end{center}
Using the general mothod from XXX, the circuits associated rank-1 constraint system is given by:
\begin{equation}
\left(S_1 + (-1)\cdot S_2\right)\cdot 1 = S_3
\end{equation}
Any valid assignment $\{S_1,S_2, S_3\}$ to this circuit therefore enforces $S_3$ to be the difference $S_1- S_2$ and in \texttt{PAPER} we define the following subtraction module that compiles to the previous circuit:
\begin{lstlisting}
module SUB {F_13} {
	input private x : F ; 
	input private y : F ; 
	input private c : F = -1 ; 
	output public SUB : F ; 

	circuit
		SUB <== ADD(x , MUL( y ,  c) );
}
\end{lstlisting}
\texttt{PAPER} compiles every occurence of the $\mathtt{SUB}$ operator into an instance of the subtraction circuit XXX, such that both the left and the right input are replacements for the input nodes of that circuit. Edge labels are generated according to the rules from XXX.
\begin{example}

\end{example} 

\paragraph{The Inversion Constraint System} By definition, algebraic circuits only contain addition and multiplication gates and it follows thar there is no single gate for field inversion, despite the fact that inversion is a native operation in every field. 

If the underlying field is a prime field, one approach would be to use Fermat's little theorem XXX to compute the multiplicative inverse inside the circuit. To see how this works let $\F_p$ be the prime field. The multiplicative inverse $x^{-1}$ of a field element $x\in\F$ with $x\neq 0$ is then given by $x^{-1}= x^{p-2}$ and computing $x^{p-2}$ in the circuit therefore computes the multiplicative inverse. 

Unfortunately, real world primes $p$ are large and computing $x^{p-2}$ by repeaded multiplication of $x$ with itself is infeasible. A double and multiply approach as described in XXX is faster as it computes the power in roughly $log_2(p)$ steps, but still adds a lot of constraints to the circuit. 

Computing inverses in the circuit makes no use of the fact, that inversion is a native operation in any field. A more constraints friendly approach is therefore to compute the multiplicative inverse outside of the circuit and then only enforce correctness of the computation in the circuit. 

To understand how this can be achieved, observe that a field element $y\in \F$ is the mutiplicative inverse of a field element $x\in \F$, if and only if $x\cdot y =1$ in $\F$. We can use this and define a circuit, that has two inputs $x$ and $y$ and enforces $x\cdot y =1$. It is then guranteed that $y$ is the multiplicative inverse of $x$. The price we pay is that we can not compute $y$ by circuit execution, but auxillary data is needed to tell any proofer which value of $y$ is needed for a valid circuit assignment.  The following circuit defines the constraint
\begin{center}
\digraph[scale=0.4]{BTINV}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n3 [xlabel="S_1  "];
	n2 -> n3 [xlabel="S_2  "];
	n3 -> n4 [xlabel="S_3 =1  "];
	n1 [shape=box, label="x"];
	n2 [shape=box, label="x^(-1)"];
	n3 [label="*"];	
	n4 [shape=box, label="1"];	
}
\end{center}
Using the general method from XXX, the circui is transformed into the following rank-1 constraint system:
\begin{equation}
S_1 \cdot S_2 = 1
\end{equation}
Any valid assignment $\{S_1,S_2\}$ to this circuit enforces that $S_2$ is the multiplicative inverse of $S_1$ and since there is no field element $S_2$, such that $0\cdot S_2=1$, it also handles the fact, that the multiplicative inverse of $0$ is not defined in any field. In \texttt{PAPER} we define the following inversion module that compiles to the previous circuit:
\begin{lstlisting}
module INV {F_13} {
	input private x : F ; 
	input private x^(-1) : F ; 
	output const c : F = 1 ;

	circuit
		c <== MUL( x ,  x^(-1) ) );
}
\end{lstlisting}
\texttt{PAPER} compiles every occurence of the binary $\mathtt{INV}$ operator into an instance of the inversion circuit XXX, such that the left input is replacements for the input nodes of that circuit and the right input is an additional input node to the circuit, labeled by $x^{-1}$. Edge labels are generated according to the rules from XXX.
\paragraph{The Division Constraint System} By definition, algebraic circuits only contain addition and multiplication gates and it follows thar there is no single gate for field division, despite the fact that division is a native operation in every field.

Implementing division as a circuit, we use the fact that division is multiplication with the multiplicative inverse. We therefore define divsion as a circuit using the inversion circuit and constraint system from the prvious paragraph. Expensive inversion is computed outside of the circuit. We get 
\begin{center}
\digraph[scale=0.4]{BTDIV}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n6 [xlabel="S_1  "];
	n2 -> n4 [xlabel="S_2  "];
	n3 -> n6 [xlabel="S_3  "];
	n3 -> n4 [xlabel="S_3  "];
	n4 -> n5 [xlabel="1  "];
	n6 -> n7 [xlabel="S_4  "];
	n1 [shape=box, label="x"];
	n2 [shape=box, label="y"];
	n3 [shape=box, label="y^(-1)"];
	n4 [label="*"];	
	n5 [shape=box, label="1"];
	n6 [label="*"];	
	n7 [shape=box, label="DIV(x,y)"];	
}
\end{center} 
Using the mothod from XXX, we transform this circuit into the following rank-1 constraint system:
\begin{align*}
S_2 \cdot S_3 &= 1\\
S_1 \cdot S_3 &= S_4
\end{align*}
Any valid assignment $\{S_1,S_2,S_3,S_4\}$ to this circuit enforces $S_4$ to be the field divsion of $S_1$ by $S_2$. It handles the fact, that division by $0$ is not defined, since there is no valid assignment in case $S_2=0$. In \texttt{PAPER} we define the following division module that compiles to the previous circuit:
\begin{lstlisting}
module DIV {F_13} {
	input private x : F ; 
	input private y : F ;
	input private y^(-1) : F ; 
	output private DIV : F ;

	circuit
		DIV <== MUL( x ,  INV( y, y^(-1) ) ) );
}
\end{lstlisting}
\texttt{PAPER} compiles every occurence of the binary $\mathtt{INV}$ operator into an instance of the inversion circuit XXX, such that the left input is replacements for the input nodes of that circuit and the right input is an additional input node to the circuit, labeled by $x^{-1}$. Edge labels are generated according to the rules from XXX.


\paragraph{Modularity} Implementing bounded computation in algebraic circuits it is often necessary to deal with complex expressions of the field type. As we have seen in XXX and XXX, both algebraic circuits and R1CS have a modularity property, which enables a compiler to derive algebraic circuit implementations for arbitrary circuits. 
\begin{example}
To given an intuition of how real world circuit compilers make use of the modularity property to synthezise complex circuits from high level programs, we derive a circuit for the following \texttt{PAPER} code:
\begin{lstlisting}
def main<F_13>(private x : F, private y : F, private y^(-1) : F ) -> () {

	circuit:
		outc1 == DIV( ADD( , ) , , );
}
\end{lstlisting}
\end{example}
\begin{comment}
\begin{example} Consider the prime field $\F_{13}$. In this example, we want to derive an algebraic circuit and associated R1CS that enforces a pair $(x,y)\in \F_{13}^2$ to be the sum of two tiny jubjub curve points $(x_1,y_1)$ and $(x_2,y_2)$. We assume that we already know that $(x_1,x_2)$ as well as $(x_2,y_2)$ are tiny jubjub points, that is we assume that they are the inputs to valid assignments of circuit XXX. 

To synthezise the associated circuit, we start with the twisted Edwards addition law XXX of the tiny jubjub curve:
$$
(x,y) = \left(\frac{x_1y_2+y_1x_2}{1+8x_1y_1x_2y_2}, \frac{y_1y_2-3x_1x_2}{1-8x_1y_1x_2y_2} \right)
$$ 
To transformation this expression into a circuit we rewrite it in terms of the binary operators $ADD$, $SUB$, $MUL$, $DIV$ that represent the four fundamental field operations in $\F_{13}$. We get
\begin{align*}
(x,y) & = (\\
  & \scriptstyle DIV(ADD(MUL(x_1,y_2),MUL(y_1,x_2)),
         ADD(1,MUL(8,MUL(MUL(x_1,y_1),MUL(x_2,y_2))))), \\   
  & \scriptstyle DIV(ADD(MUL(y_1,y_2),MUL(MUL(3,x_1),x_2)),
         ADD(1,MUL(8,MUL(MUL(x_1,y_1),MUL(x_2,y_2)))))\\
  & )
\end{align*}
We then proceed inductively choosing circuits for the outer most operators, which in this case are two division circuits. We don't expand their inputs into circuits yet, but only represent the inputs symbolically. For better readability we use the symbols of the next operator only, because otherwise the circuit becomes unreadable. We get:
\begin{center}
\digraph[scale=0.4]{TEA}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	// x-value
	nx1 -> nx6 [xlabel="Ex_1  "];
	nx2 -> nx4 [xlabel="Ex_2  "];
	nx3 -> nx6 [xlabel="Ex_3  "];
	nx3 -> nx4 [xlabel="Ex_3  "];
	nx4 -> nx5 [xlabel="Ex_4  "];
	nx6 -> nx7 [xlabel="Ex_5  "];
	nx1 [shape=box, label="ADD(.,.)"];
	nx2 [shape=box, label="ADD(.,.)"];
	nx3 [shape=box, label="ADD(.,.)_INV"];
	nx4 [label="*"];	
	nx5 [shape=box, label="1"];
	nx6 [label="*"];	
	nx7 [shape=box, label="DIV(ADD(.,.),ADD(.,.))"];
	// y-value
	ny1 -> ny6 [xlabel="Ey_1  "];
	ny2 -> ny4 [xlabel="Ey_2  "];
	ny3 -> ny6 [xlabel="Ey_3  "];
	ny3 -> ny4 [xlabel="Ey_3  "];
	ny4 -> ny5 [xlabel="Ey_4  "];
	ny6 -> ny7 [xlabel="Ey_5  "];
	ny1 [shape=box, label="ADD(.,.)"];
	ny2 [shape=box, label="ADD(.,.)"];
	ny3 [shape=box, label="ADD(.,.)_INV"];
	ny4 [label="*"];	
	ny5 [shape=box, label="1"];
	ny6 [label="*"];	
	ny7 [shape=box, label="DIV(ADD(.,.),ADD(.,.))"];	
}
\end{center}

\end{example}
\end{comment}

\subsubsection{The Boolean Type} 
% implementations can be found here: https://github.com/filecoin-project/zexe/tree/master/snark-gadgets/src/bits
Booleans are a classical primitive type, implemented by virtually every higher programing language. It is therefore a importance to implement booleans in circuits. One of the most common ways to do this is by interpreting the additive and multiplicative neutral element $\{0,1\}\subset \F$ as the two boolean values, such that $0$ represents $false$ and $1$ represents $true$. Boolean operators like $and$, $or$, or $xor$ are then expressable as algebraic computations inside $\F$. 

Representing booleans this way is convinient because the elements $0$ and $1$ are defined in any field. The representation is therefore independent of the actual field in consideration. 

To fix Boolean algebra notation we write $0$ to represent $false$ and $1$ to represent $true$ and we write $\wedge$ to represent the boolean AND as well as $\vee$ to represent the boolean OR operator. The boolean NOT operator is written as $\lnot$. 
\paragraph{The Boolean Constraint System} Once a circuit compiler has decided to represent the booleans by the additive and multiplicative neutral elements of a field, a constraint is required to actually enforces variables of boolean type to be either $1$ or $0$. In fact many of the following circuits that represent boolean functions, are only correct under the assumption that their input variables are constraint to be either $0$ or $1$. Not constraining boolean variables is a common issue in circuit design.

In order to constrain an arbitrary field element $x\in \F$ to be $1$ or $0$, the key observation is that the equation $x \cdot (1-x) =0$ has only two solutions $0$ and $1$. Implementing this equation as a circuit therefore generates the correct constraint:
\begin{center}
\digraph[scale=0.4]{BOOLCONS}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;
  nCONS1 -> nCONS4 [xlabel="S_1"] ;
  nCONS1 -> nCONS6 [xlabel="S_1  "] ;
  nCONS2 -> nCONS5 ;
  nCONS3 -> nCONS4 ;
  nCONS4 -> nCONS5 ;
  nCONS5 -> nCONS6 ;
  nCONS6 -> nCONS7 [xlabel="0  "] ;
  nCONS1 [shape=box, label="x"] ;
  nCONS2 [shape=box, label="1"] ;
  nCONS3 [shape=box, label="-1"] ;
  nCONS4 [label="*"] ;
  nCONS5 [label="+"] ;
  nCONS6 [label="*"] ;
  nCONS7 [shape=box, label="0"] ;
}
\end{center}
Using the mothod from XXX, we transform this circuit into the following rank-1 constraint system:
$$
S_1 \cdot (1-S_1) = 0
$$
Any valid assignment $\{S_1\}$ to this circuit enforces $S_1$ to be either $0$ or $1$. In \texttt{PAPER} we define the following boolean constraint module that compiles to the previous circuit:
\begin{lstlisting}
module BOOL {F_13} {
	input private x : F ; 
	output const c1 : F = 0 ;
	output const c2 : F = 1 ;
	output const c3 : F = -1 ;

	circuit
		c1 <== MUL( x ,  ADD( c2 , MUL( x , c3) ) ) );
}
\end{lstlisting}
\texttt{PAPER} compiles every occurence of a variable of boolean type into an instance of the boolean constraint circuit XXX. Edge labels are generated according to the rules from XXX.

In designing more complex circuits from simple ones it is often conceptually as well as visually useful to collaps circuits into simple representative description. To do so, we write 
\begin{center}
\digraph[scale=0.6]{BOOLMETA}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;
  n1 [shape=box, label="BOOL"] ;
  n2 [shape=none, label="  "] ;
  n2 -> n1 ;
}
\end{center}
indicating that the boolean constraint circuit takes one input, has no outputs and constraints the input to be either $0$ or $1$.
\paragraph{The AND operator constraint system} Given two field elements $x$ and $y$ from $\F$ that are constrained to represent boolean variables, we want to find a circuit that computes the logical \textit{and} operator $AND(x,y)$ as well as its associated R1CS, that enforces $x$, $y$, $AND(x,y)$ to satisfy the constraint system if and only if $x\; \&\& \; y =AND(x,y)$ holds true. 

Assuming that three variables $x$, $y$ and $z$ are boolean constraint, the equation $x\cdot y = z$ is satisfied in $\F$ if and only if the equation $x\text{ AND }y = z$ is satisfied in boolean algebra. The logical operator AND is therefore implementable in $\F$ as multiplication of its arguments. 

The following circuit computes the AND operator in $\F$, assuming all inputs are restricted to be $0$ or $1$:
\begin{center}
\digraph[scale=0.4]{BOOLAND}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;
  nAND1 -> nAND3 [xlabel="E_1  "] ;
  nAND2 -> nAND3 [xlabel="E_2"] ;
  nAND3 -> nAND4 [xlabel="E_3  "] ;

  nAND1 [shape=box, label="x"] ;
  nAND2 [shape=box, label="y"] ;
  nAND3 [label="*"] ;
  nAND4 [shape=box, label="AnANDD(x,y)"] ;
}
\end{center}
The associated rank-1 constraint system can be deduced from the general process XXX and consists of the following constraint
\begin{equation}
 W_1 \cdot W_2 = W_3
\end{equation}
In designing more complex circuits from simple ones it is often conceptually as well as visually useful to collaps circuits into simple representative description. To do so, we write 
\begin{center}
\digraph[scale=0.6]{ANDMETA}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;
  n1 [shape=box, label="AND"] ;
  n2 [shape=none, label="  "] ;
  n3 [shape=none, label="  "] ;
  n4 [shape=none, label="  "] ;
  n2 -> n1 ;
  n3 -> n1 ;
  n1 -> n4 ;
}
\end{center}
indicating that the AND circuit takes two input, has one outputs and constraints the output to be the logical AND of the inputs, providing the inputs are boolean constraint.
\paragraph{The OR operator constraint system} Given two field elements $x$ and $y$ from $\F$ that are constrained to represent boolean variables, we want to find a circuit that computes the logical \textit{or} operator $OR(x,y)$ as well as its associated R1CS, that enforces $x$, $y$, $OR(x,y)$ to satisfy the constraint system if and only if $x\; || \; y =OR(x,y)$ holds true. 

Assuming that three variables $x$, $y$ and $z$ are boolean constraint, the equation $1-(1-x)\cdot(1-y) = z$ is satisfied in $\F$ if and only if the equation $x\text{ OR }y = z$ is satisfied in boolean algebra. The logical operator OR is therefore implementable in $\F$ by the following circuit, assuming all inputs are restricted to be $0$ or $1$:
\begin{center}
\digraph[scale=0.4]{BOOLOR}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;
   nOR1 -> nOR5 [xlabel="E_1  "] ;
  nOR2 -> nOR7 [xlabel="E_2  "] ;
  nOR3 -> {nOR5, nOR7, nOR10} ;
  nOR4 -> {nOR6, nOR8, nOR11} ;
  nOR5 -> nOR6; 
  nOR6 -> nOR9 ;
  nOR7 -> nOR8 ;
  nOR8 -> nOR9 ;
  nOR9 -> nOR10 [xlabel="E_3  "] ;
  nOR10 -> nOR11 ;
  nOR11 -> nOR12 [xlabel="E_4  "] ;

  nOR1 [shape=box, label="x"] ;
  nOR2 [shape=box, label="y"] ;
  nOR3 [shape=box, label="-1"] ;
  nOR4 [shape=box, label="1"] ;
  nOR5 [label="*"] ;
  nOR6 [label="+"] ;
  nOR7 [label="*"] ;
  nOR8 [label="+"] ;
  nOR9 [label="*"] ;
  nOR10 [label="*"] ;
  nOR11 [label="+"] ;
  nOR12 [shape=box, label="OR(x,y)"] ;
}
\end{center}
The associated rank-1 constraint system can be deduced from the general process XXX and consists of the following constrainst
\begin{align*}
 (1- W_1) \cdot (1-W_2) & = W_3\\
  (1-W_3)\cdot 1 &= W_4
\end{align*}
In designing more complex circuits from simple ones it is often conceptually as well as visually useful to collaps circuits into simple representative description. To do so, we write 
\begin{center}
\digraph[scale=0.6]{ORMETA}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;
  n1 [shape=box, label="OR"] ;
  n2 [shape=none, label="  "] ;
  n3 [shape=none, label="  "] ;
  n4 [shape=none, label="  "] ;
  n2 -> n1 ;
  n3 -> n1 ;
  n1 -> n4 ;
}
\end{center}
indicating that the OR circuit takes two input, has one outputs and constraints the output to be the logical OR of the inputs, providing the inputs are boolean constraint.
\begin{exercise} Let $\F$ be a finite field and let $b_1$ as well as $b_2$ two boolean constraint variables from $\F$. Show that the equation 
$OR(b_1,b_2) = b_1 + b_2 - b_1\cdot b_2$ holds true.

Use this equation to derive an algebraic circuit with ingoing variables $b_1$ and $b_2$ and outgoing variable $OR(b_1,b_2)$, such that $b_1$ and $b_2$ are boolean constraint and the circuit has a valid assignment, if and only if $OR(b_1,b_2) = b_1 \vee b_2$.  

Use the technique from XXX to transform this circuit into a rank-1 constraint system and find its full solution set. 
\end{exercise}
\paragraph{The NOT operator constraint system} Given a field element $x$ from $\F$ that is constrained to represent a boolean variable, we want to find a circuit that computes the logical \textit{NOT} operator $NOT(x)$ as well as its associated R1CS, that enforces $x$, $NOT(x)$ to satisfy the constraint system if and only if $\lnot x = NOT(x)$ holds true. 

Assuming that two variables $x$ and $y$ are boolean constraint, the equation $(1-x) = y$ is satisfied in $\F$ if and only if the equation $\lnot x = y$ is satisfied in boolean algebra. The logical operator NOT is therefore implementable in $\F$ by the following circuit, assuming all inputs are restricted to be $0$ or $1$:
\begin{center}
\digraph[scale=0.4]{BOOLNOT}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;
  nNOT1 -> nNOT4 [xlabel="E_1  "] ;
  nNOT2 -> nNOT4 ;
  nNOT3 -> nNOT5 ;
  nNOT4 -> nNOT5 ;
  nNOT5 -> nNOT6 [xlabel="E_2  "] ;

  nNOT1 [shape=box, label="x"] ;
  nNOT2 [shape=box, label="-1"] ;
  nNOT3 [shape=box, label="1"] ;
  nNOT4 [label="*"] ;
  nNOT5 [label="+"] ;
  nNOT6 [shape=box, label="nNOT(x)"] ;
}
\end{center}
The associated rank-1 constraint system can be deduced from the general process XXX and consists of the following constrainst
\begin{align*}
  (1-W_1)\cdot 1 &= W_2
\end{align*}
In designing more complex circuits from simple ones it is often conceptually as well as visually useful to collaps circuits into simple representative description. To do so, we write 
\begin{center}
\digraph[scale=0.6]{NOTMETA}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;
  n1 [shape=box, label="NOT"] ;
  n2 [shape=none, label="  "] ;
  n4 [shape=none, label="  "] ;
  n2 -> n1 ;
  n1 -> n4 ;
}
\end{center}
indicating that the NOT circuit takes one input, has one outputs and constraints the output to be the logical NOT of the input, providing the input is boolean constraint.
\begin{exercise}
Let $\F$ be a finite field. Derive the algebraic circuit and associated rank-1 constraint system for the following operators: OR, XOR, NAND, EQU.
\end{exercise}
\paragraph{Modularity} Implementing bounded computation in algebraic circuits it is often necessary to deal with complex boolean expressions. As we have seen in XXX and XXX, both algebraic circuits and R1CS have a modularity property, which enables a compiler to derive algebraic circuit implementations for arbitrary boolean circuits. 

This is quite remarkable property, because it shows that the expressiveness of algebraic circuits and therefore rank-1 constraint systems is as general as the expressiveness of boolen circuits.  
\begin{example} To give an intuitive example of how a very simple compiler might construct complex boolean circuit representations in algebraic circuits and how to derive associated rank-1 constraint systems, lets look at the following VERILOG like pseudo code:
\begin{lstlisting}
module boolean_circuit (
	input (public) b_1 : BOOL ; 
	input (public) b_2 : BOOL ;
	input (public) b_3 : BOOL ; 
	input (public) b_4 : BOOL ; 
	output (public) b_5 : BOOL ; 

	begin
		b_5 <= (b_1 or b_2) and (b_3 and not b_4 ) ;
	end ;
)
\end{lstlisting}
The code describes a circuit, that takes four public inputs $b_1$, $b_2$, $b_3$ and $b_4$ of boolean type and computes a public output $b_5$, such that the following boolean expression holds true:
$$
\left( b_1 \vee b_2 \right) \wedge (b_3 \wedge \lnot b_4) = b_5
$$
In order to understand a possible way to transform this hardware description language style expression into a circuit, we first rewrite the actual computation of the boolean expression into operator notation. We get
$$
b_5 \leftarrow AND(OR(b_1,b_2),AND(b_3,NOT(b_4))
$$
Using the boolean operator notion, makes it conceptually more clear to derive the circuit. To see how the circuit is computed we start at the outer most operator and  write down its defining circuit as well as it associated R1CS using placeholder names for its arguments. We then inductively substitute every placeholder by its defining circuit and add the associated constraint, to the constrain system. We get
\begin{center}
\digraph[scale=0.4]{BOOLCOMPLEX}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;

  subgraph clusterORb1b2 {
    nOR1 -> nOR5 [xlabel="E_1  "] ;
    nOR2 -> nOR7 [xlabel="E_2  "] ;
    nOR3 -> {nOR5, nOR7, nOR10} ;
    nOR4 -> {nOR6, nOR8, nOR11} ;
    nOR5 -> nOR6; 
    nOR6 -> nOR9 ;
    nOR7 -> nOR8 ;
    nOR8 -> nOR9 ;
    nOR9 -> nOR10 [xlabel="E_3  "] ;
    nOR10 -> nOR11 ;
    nOR11 -> nOR12 [xlabel="E_4  "] ;
    nOR1 [shape=box, label="b_1"] ;
    nOR2 [shape=box, label="b_2"] ;
    nOR3 [shape=box, label="-1"] ;
    nOR4 [shape=box, label="1"] ;
    nOR5 [label="*"] ;
    nOR6 [label="+"] ;
    nOR7 [label="*"] ;
    nOR8 [label="+"] ;
    nOR9 [label="*"] ;
    nOR10 [label="*"] ;
    nOR11 [label="+"] ;
    nOR12 [shape=box, label="OR( b1 , b2 )", color=lightgray] ;
    label = "OR( b1 , b2 )";
    color=lightgray;
    label="Circuit_2 -- OR-Circuit"
  }

  subgraph clusterANDb3NOTb4 {
    nAND21 -> nAND23 [xlabel="E_1  "] ;
    nAND22 -> nAND23 [xlabel="E_2"] ;
    nAND23 -> nAND24 [xlabel="E_3  "] ;

    nAND21 [shape=box, label="b3"] ;
    nAND22 [shape=box, label="NOT( b4 )", color=lightgray ] ;
    nAND23 [label="*"] ;
    nAND24 [shape=box, label="AND( b3 , NOT( b4 ) )", color=lightgray] ;
    color=lightgray;
    label="Circuit_3 -- AND-Circuit"
  }

  subgraph clusterNOTb4 {
    nNOT1 -> nNOT4 [xlabel="E_1  "] ;
    nNOT2 -> nNOT4 ;
    nNOT3 -> nNOT5 ;
    nNOT4 -> nNOT5 ;
    nNOT5 -> nNOT6 [xlabel="E_2  "] ;

    nNOT1 [shape=box, label="b4"] ;
    nNOT2 [shape=box, label="-1"] ;
    nNOT3 [shape=box, label="1"] ;
    nNOT4 [label="*"] ;
    nNOT5 [label="+"] ;
    nNOT6 [shape=box, label="NOT( b4 )", color=lightgray ] ;
    color=lightgray;
    label="Circuit_4 -- NOT-Circuit"
  }

  subgraph clusterAND1 {
    nAND1_1 -> nAND1_3 [headlabel="E_1    ."] ;
    nAND1_2 -> nAND1_3 [xlabel="E_2  "] ;
    nAND1_3 -> nAND1_4 [xlabel="E_3  "] ;
    nAND1_1 [shape=box, label="OR( b1 , b2 )", color=lightgray ] ;
    nAND1_2 [shape=box, label="AND( b3 , NOT( b4 ) )", color=lightgray] ;
    nAND1_3 [label="*"] ;
    nAND1_4 [shape=box, label="AND( OR( b1 , b2 ) , AND( b3 , NOT( b4 ) )"] ;
    color=lightgray;
    label="Circuit_1 -- AND-Circuit"
  }

  // outer circuit
    nNOT6 -> nAND22 [style=dashed, color=grey] ;
    nOR12 -> nAND1_1 [style=dashed, color=grey] ;
    nAND24 -> nAND1_2 [style=dashed, color=grey] ;
}
\end{center}
For better readability, this circuit does not boolean constraint the four input variables $b_1$, $b_2$, $b_3$ and $b_4$. After adding the boolean constraints, relabeling the edges and optimizing the constants, using graphviz, the following circuit represents the boolean expression:
\begin{center}
\digraph[scale=0.5]{BOOLCOMPLEXOPTI}{
  forcelabels=true;
  center=true;
  splines=ortho;

  one -> nCONSb15 ;
  minusone -> nCONSb14 ;
  nCONSb14 -> nCONSb15 ;
  nCONSb15 -> nCONSb16 ;
  nCONSb16 -> zero [taillabel="  W_1"] ;
  nCONSb14 [label="*"] ;
  nCONSb15 [label="+"] ;
  nCONSb16 [label="*"] ;

  one -> nCONSb25 ;
  minusone -> nCONSb24 ;
  nCONSb24 -> nCONSb25 ;
  nCONSb25 -> nCONSb26 ;
  nCONSb26 -> zero [taillabel="W_2 "] ;
  nCONSb24 [label="*"] ;
  nCONSb25 [label="+"] ;
  nCONSb26 [label="*"] ;

  one -> nCONSb35 ;
  minusone -> nCONSb34 ;
  nCONSb34 -> nCONSb35 ;
  nCONSb35 -> nCONSb36 ;
  nCONSb36 -> zero [xlabel="W_3"] ;
  nCONSb34 [label="*"] ;
  nCONSb35 [label="+"] ;
  nCONSb36 [label="*"] ;

  one -> nCONSb45 ;
  minusone -> nCONSb44 ;
  nCONSb44 -> nCONSb45 ;
  nCONSb45 -> nCONSb46 ;
  nCONSb46 -> zero [xlabel="W_4"] ;
  nCONSb44 [label="*"] ;
  nCONSb45 [label="+"] ;
  nCONSb46 [label="*"] ;

  minusone -> {nOR5, nOR7, nOR10} ;
  one -> {nOR6, nOR8, nOR11} ;
  nOR5 -> nOR6; 
  nOR6 -> nOR9 ;
  nOR7 -> nOR8 ;
  nOR8 -> nOR9 ;
  nOR9 -> nOR10 [headlabel="W_5  "] ;
  nOR10 -> nOR11 ;
  nOR5 [label="*"] ;
  nOR6 [label="+"] ;
  nOR7 [label="*"] ;
  nOR8 [label="+"] ;
  nOR9 [label="*"] ;
  nOR10 [label="*"] ;
  nOR11 [label="+"] ;

  nAND23 [label="*"] ;
  minusone -> nNOT4 ;
  one -> nNOT5 ;
  nNOT4 -> nNOT5 ;
  nNOT4 [label="*"] ;
  nNOT5 [label="+"] ;

  nOR11 -> nAND1_3;
  nAND23 -> nAND1_3 [xlabel="W_6  "] ;
  nAND1_3 -> nAND1_4 [xlabel="I_5  "] ;
  nAND1_3 [label="*"] ;
  nAND1_4 [shape=box, label="AND( OR( b1 , b2 ) , AND( b3 , NOT( b4 ) )"] ;

  // outer circuit
    nNOT5 -> nAND23 ;
    b1 -> {nOR5, nCONSb14, nCONSb16} [taillabel="I1 "] ;
    b2 -> {nOR7, nCONSb24, nCONSb26} [taillabel=" I2"] ;
    b3 -> {nAND23, nCONSb34, nCONSb36} [taillabel="I3 "] ;
    b4 -> {nNOT4, nCONSb44, nCONSb46} [taillabel="I4"] ;
    b1 [shape=box, label="b1"] ;
    b2 [shape=box, label="b2"] ;
    b3 [shape=box, label="b3"] ;
    b4 [shape=box, label="b4"] ;
    minusone [shape=box, label="-1"] ;
    one [shape=box, label="1"] ;
    zero [shape=box, label="0"] ;
}
\end{center}
Valid assignments to this circuits consists of public inputs $I_1$, $I_2$, $I_3$, $I_4$ and $I_5$ from $\F_{13}$, such that the equation $I_5 = \left( I_1 \vee I_2 \right) \wedge (I_3 \wedge \lnot I_4)$ has to hold true. In addition a valid assignment also has to contain private inputs $W_1$, $W_2$, $W_3$, $W_4$, $W_4$ and $W_6$, which can be derived from circuit execution. The inputs $W_1$, $\ldots$, $W_4$ ensure that the first four public inputs are either $0$ or $1$ but not any other field element and the others enforce the boolean expression.  

To compute the associated R1CS we can use the general method from XXX and look at every labeled outgoing edge not coming from a source node. Declaring the edges coming from input nodes as well as the edge going to the single output node as public and every other edge as private input. In this case we get:
\begin{align*}
W_1:\;\; & I_1 \cdot (1- I_1) = 0  & \text{boolean constraints}\\
W_2:\;\; & I_2 \cdot (1- I_2) = 0 \\
W_3:\;\; & I_3 \cdot (1- I_3) = 0 \\
W_4:\;\; & I_4 \cdot (1- I_4) = 0 \\
W_5:\;\; & (1- I_1)\cdot (1-I_2) = W_5 & \text{OR-operator constraint}\\
W_6:\;\; & I_3 \cdot (1-I_4) = W_6 & \text{AND(.,NOT(.))-operator constraints}\\
I_5:\;\; & (1-W_5) \cdot W_6 = I_5 & \text{AND-operator constraints}\\
\end{align*}
The reason why this R1CS only contains a single contraint for the OR-operator, while the general definition XXX requires two, is that the second constraint in XXX only appears since the final addition gate is connected to a sink node. In this example however the addition gate is sub-circuit and internal addition gates do not lead to new constraints. The same holds true for the negation circuit. 
\end{example}
\subsubsection{The Unsigned Integer Type} In computer science, an unsigned integer of size $N$, where $N$ is usually a power of two, is an atomic type that represents counting numbers in the range $0\ldots 2^N-1$ together with addition, subtraction and multiplication laws that are somewhat similar to the (semi) ring laws of natural numbers except for overflow and underflow effects. The associated type is usually written as $uN$ or $uIntN$.

On compuer hardware elements of the unsigned integer type $uIntN$ are commonly represented as $N$-tuples of bits, that is if $x : uIntN$ is of $uIntN$ type it is represented as
$$
x = (b_0,b_1,\ldots, b_{N-1})
$$
For suteable $N$ like $N=32$ or $N=64$, addition, subtraction and multiplication is realized in hardware by appropriate digital circuits like the binary adder oder the binary multiplier. 

To understand how unsigned integer types can be represented as algebraic circuits, basically two different approaches can be taken.

To understand the first approach, recall that addition and multiplication in a prime field $\F_p$ is equal to addition and multiplication of integers, as long as the sum or the product does not exceed the modulus $p$. It is therefore possible to represent the $uIntN$ type inside the basefield type, whenever $N$ is small enough. However care has to be taken to never overflow the modulus. It is also important to make sure that in subtraction the subtrahend is never larger then the minuent.

An advantage of this approach is that it is very efficient to represent elements of the uIntN type in this way, as they can be storred in a single element of the base field type. The diadvantage is that care must be taken to constrain the elements and to enforce that no overflow or underflow situations occure.

The second approach in conceptually cleaner but requires more space and constraints for addition and multiplication. Much like machines represents uInt's as binary tuples, this approach represents elements of uIntN types as $N$-typles $(b_0,b_1,\ldots, b_{N-1})$ of elements from the base field $\F$, such that each $b_j$ itself is of boolean type. All operations, like addition, multiplication, bit-shifts and so on, are then realized by addoptations of the digital circuits that implement these operations in hardware.

An advantage of this representation is that the number $N$ is independend of the modulus of the underlying prime field and the representation moreover works over arbitrary fields. It can therefore abstract over the field.


In what follows we will describe the second approach in more detail.


\paragraph{The uIntN Constraint System} In the approach we are taking in this section, elements of uIntN type are represented by $N$-tuples of field elements that are themself binary constraint. Declaring an element of uIntN type therefore means to declare $N$ elements of boolean type. We write this as 
\begin{center}
\digraph[scale=0.6]{UINTN}{
  forcelabels=true;
  center=true;
  splines=ortho;
  //nodesep= 2.0;
  n1 [shape=box, label="UINT_N"] ;
  n2 [shape=none, label="  "] ;
  n3 [shape=none, label="  "] ;
  n4 [shape=none, label="  "] ;
  n5 [shape=none, label="  "] ;
  n2 -> n1 ;
  n3 -> n1 ;
  n4 -> n1 [style=dashed, color=lightgrey] ;
  n5 -> n1 ;
}
\end{center}
To enfore an $N$-tuple of field elements $(b_0,\ldots,b_{N_1})$ to represent an element of UintN type we therefore need $N$ constraints 
\begin{align*}
E_0 \cdot (1-E_0) & = 0\\
E_1 \cdot (1-E_1) & = 0\\
\cdots &\\
E_{N-1} \cdot (1-E_{N-1}) & = 0\\
\end{align*}
\begin{example}
Consider the Uint4 type over the prime field $\F_{17}$. Since $2^4=16$, Uint4 can represent the numbers $0,\ldots, 15$ and it would be possible to interpret them as elements in $\F_{17}$. However addition 
\end{example} 
\paragraph{UintN Addition} Since we representat the unsigned integer type as an $N$-tuple of field elements that are boolean constraint, we can define addition in the same way as hardeare does. The way this is usually done is by first defining the \textit{full adder} circuit and then combining $N$ of this these circuits into a circuit that add to elements from the UintN type.

To understand the algebraic circuit for the $1$-bit full, recall that we already defined circuits for boolean algebra in the previous section. Abstracting over those circuits, a full adder circuit can then be defined as:
\begin{center}
\digraph[scale=0.4]{ONEBFULLADD}{
  forcelabels=true;
  center=true;
  splines=ortho;
  nodesep= 2.0;
  
  subgraph clusterin {
    nADD01 [shape=box, label="bx_j"] ;
    nADD02 [shape=box, label="by_j"] ;
    nADD03 [shape=box, label="c_(j-1)"] ;
    color = white ;
  }
  
  subgraph clustermid {
    nADD04 [shape=box, label="XOR"] ;
    nADD05 [shape=box, label="XOR"] ;
    nADD06 [shape=box, label="AND"] ;
    nADD07 [shape=box, label="AND"] ;
    nADD08 [shape=box, label="OR"] ;
    
    nADD04 -> {nADD05, nADD06} ;
    nADD06 -> nADD08 ;
    nADD07 -> nADD08 ;
    
    color = white ;
  }
  
  subgraph clusterout {
    nADD09 [shape=box, label="bz_j"] ;
    nADD010 [shape=box, label="c_j"] ;
    color = white ;
  }
  
  nADD01 -> {nADD04, nADD07} ;
  nADD02 -> {nADD04, nADD07} ;
  nADD03 -> {nADD05, nADD06} ;
  nADD05 -> nADD09 ;
  nADD08 -> nADD010 ; 
}
\end{center}
In this circuit the output $bz_j$ is the result of the binary input $bx_j$ and $by_j$, where $bx_j$ is the $j$-th bit of the binary representation of the first summand and $by_j$ is the $j$-th bit of the binary representation of the second summand. The output $c_j$ is the carry bit of the addition and the input $c_{j-1}$ is is the carry bit which is supposed to be either $0$ for $j=0$ or the carry bit output of the previous full adder circiut. Abstacting the $1$-bit adder, we write:
\begin{center}
\digraph[scale=0.6]{BADDMETA}{
  forcelabels=true;
  center=true;
  splines=ortho;
  //nodesep= 2.0;
  n1 [shape=box, label="FULLADD"] ;
  n2 [shape=none, label="bx_j"] ;
  n3 [shape=none, label="by_j"] ;
  n4 [shape=none, label="c_(j-1)"] ;
  n5 [shape=none, label="bz_j"] ;
  n6 [shape=none, label="c_j"] ;
  n2 -> n1 ;
  n3 -> n1 ;
  n4 -> n1 ;
  n1 -> {n5, n6} ;
}
\end{center}
With a circuit definition of the $1$-bit full adder at hand, addition of two uIntN type elements can then be defined as
\begin{center}
\digraph[scale=0.4]{UINTADD}{
  forcelabels=true;
  center=true;
  splines=ortho;
  //nodesep= 2.0;
  
  subgraph clusterin0 {
    nADD01 [shape=box, label="FULLADD"] ;
    nADD02 [shape=none, label="bx_0"] ;
    nADD03 [shape=none, label="by_0"] ;
    nADD05 [shape=none, label="bz_0"] ;
    nADD02 -> nADD01 ;
    nADD03 -> nADD01 ;
    nADD01 -> nADD05 ;
    color = white ;
  }
  
  subgraph clusterin1 {
    nADD11 [shape=box, label="FULLADD"] ;
    nADD12 [shape=none, label="bx_1"] ;
    nADD13 [shape=none, label="by_1"] ;
    //nADD14 [shape=none, label="c_0"] ;
    nADD15 [shape=none, label="bz_1"] ;
    nADD12 -> nADD11 ;
    nADD13 -> nADD11 ;
    //nADD14 -> nADD11 ;
    nADD11 -> nADD15 ;
    color = white ;
  }

  subgraph clusterin2 {
    nADD21 [shape=box, label="FULLADD", color=lightgray] ;
    nADD22 [shape=none, label="bx_j", color=lightgray] ;
    nADD23 [shape=none, label="by_j", color=lightgray] ;
    //nADD24 [shape=none, label="c_(j-1)", color=lightgray] ;
    nADD25 [shape=none, label="bz_j", color=lightgray] ;
    nADD22 -> nADD21 [color=lightgray];
    nADD23 -> nADD21 [color=lightgray];
    //nADD24 -> nADD21 ;
    nADD21 -> nADD25 [color=lightgray] ;
    color = white ;
  }
  
  subgraph clusterinN {
    nADDN1 [shape=box, label="FULLADD"] ;
    nADDN2 [shape=none, label="bx_(N-1)"] ;
    nADDN3 [shape=none, label="by_(N-1)"] ;
    //nADDN4 [shape=none, label="c_(N-2)"] ;
    nADDN5 [shape=none, label="bz_(N-1)"] ;
    nADDN2 -> nADDN1 ;
    nADDN3 -> nADDN1 ;
    //nADDN4 -> nADDN1 ;
    nADDN1 -> nADDN5
    color = white ;
  }
  
  nADD04 [shape=none, label="0"] ;
  nADD04 -> nADD01 ;
  nADD01 -> nADD11 ;
  nADD11 -> nADD21 [style=dashed, color=lightgrey] ;
  nADD21 -> nADDN1  [style=dashed, color=lightgrey] ;
  nADDN6 [shape=none, label="c_out"] ;
  nADDN1 -> nADDN6 ;
  
}
\end{center}
Depending on how the output carry bit is handled we get different definition of addition in this type. One way would be to enforce it to be zero. This way addition in the circuit is only possible if the sum does not exceed $2^N-1$. On the other hand if the carry bit is unconstraint, then the resulting addition is equivalent to modulo $2^N$ arithmetics. Good  compilers should therefore always describe explicitly how exactly their implementation of the uintN type behaves, such that users don't build their system on false assumptions. 

The associated constraint system consists of XXX constraints, including the boolean constraints of the representing bits
\paragraph{The Boolean Operators} In implementations it is often necesarry to execute boolean operations like $ans$, $or$, or $xor$ on elements of the uInt type. Fortunately this easily done by simply applying those operatons to every bit seperately as shown in XXX.  
\begin{exercise}
Let $k$ be a counting number with $k<N$. Define circuits and associated R1CS for the left and righr bishift operators $x<<k$ as well as $x>>k$ for the uint type. 
\end{exercise}
\begin{exercise}
Define the multiplication circuits for the uintN type.
\end{exercise}
\begin{exercise} Let $N=4$ be fixed and consider the finite field $\F_{13}$ from example XXX. The following pseudo code describes a high level circuit description in a VERILOG like style. Transform the pseudo code into a circuit and then derive the associated R1CS. 
\begin{lstlisting}
module mask_merge(N) (
	input (public) a : Uint_N ;
	input (public) b : Uint_N ;
	input (public) mask : Uint_N ;
	output (public) r : Uint_N ;

	begin
		r == a xor ((a xor b) & mask) ;
	end ;
)
\end{lstlisting}
Let $L_{mask\_merge}$ be the language defined by the R1CS of the circuit. Provide a knowledge proof in $L_{mask\_merge}$ for the instance $I=(I_a, I_b, I_{mask}, I_r) = (14, 5, 10, 4)$. Also show that there is no knowledge proof in $L_{mask\_merge}$ for the instance $(11, 6, 10, 7)$.
\end{exercise}
\subsubsection{Arrays}

\subsection{Control Flow}
\subsubsection{The Conditional Assignment} Implementing complex control flow in circuits, it is often necessary to have a way for conditional assignment of values or computational output to variables.

One way to realize this in more common programming languages is by the conditional ternary operator $?:$, that branches the control flow of a program according to some condition and then assigns the output of the computational branch to some variable. A common way to write this is as
\begin{lstlisting}
	variable = condition ? value_if_true : value_if_false  
\end{lstlisting}
where \textsc{condition} is a boolean expression and \textsc{value\_if\_true} as well as \textsc{value\_if\_false} are expressions that evaluate to the same type as \textsc{variable}.

In programming languages like Rust another way to write the conditional assignment operator that is more familiar to many programmers is given by 
\begin{lstlisting}
	variable = if condition { value_if_true } else { value_if_false } 
\end{lstlisting}
One particular property of this operator is that the expression \textsc{value\_if\_true} is only evaluated if \textsc{condition} evaluates to true and the expression \textsc{value\_if\_false} is only evaluated if \textsc{condition} evaluates to false. In fact computer programs would soon become very inefficient if the operator would evaluate both expressions regardless of the value of \textsc{condition}.

If drop the requirement that only one branch of the conditional operator is executed, we can implement it in a simple way as a circuit. To see that observe that if $b$, $c$ and $d$ are values from a finite field, such that $b$ is boolean constraint (XXX), we can use the following equation to enforce a field element $x$ to be the result of the conditional assignment operator: 
\begin{equation}
x = b\cdot c + (1-b)\cdot d
\end{equation}
Flattening this equation into an algebraic circuit gives
\begin{center}
\digraph[scale=0.4]{CONDASSIGN}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;

  n1 [shape=box, label="b"]
  n2 [shape=box, label="c"]
  n3 [shape=box, label="d"]
  n4 [shape=box, label="b ? c : d"]
  n5 [shape=box, label="-1"]
  n6 [shape=box, label="1"]
  n7 [shape=box, label="*"]
  n8 [shape=box, label="*"]
  n9 [shape=box, label="*"]
  n10 [shape=box, label="+"]
  n11 [shape=box, label="+"]
 
  n1 -> n7 [taillabel= "E_1  "] ;
  n1 -> n8 [taillabel= "E_1 "] ;
  n2 -> n7 [xlabel= "E_2"] ;
  n3 -> n9 [xlabel= "E_4"] ;
  n5 -> n8 ;
  n6 -> n10 ;
  n7 -> n11 [xlabel= "E_3  "] ;
  n8 -> n10 ;
  n9 -> n11 [xlabel= "E_5"] ;
  n10 -> n9 ;
  n11 -> n4 [xlabel= "E_6  "] ;
}
\end{center}
Note that in order to compute a valid assignment to this circuit, both values for $W_?$ and $W_?$ are necessary. If the inputs to thoses edges are circuits themself, both circuits needs valid assignments. As a consequence this implementation of the conditional assigment opperator has to execute alll branches of all circuits, which is very different from the execution of common computer programs. 

Starting at this circuit we can use the general tenchnique from XXX to derive its associated rank-1 constraint system. We get
\begin{align*}
E_1 \cdot E_2 & = E_3 \\
(1 - E_1) \cdot E_4 & = E_5 \\
(E_3 + E_5)\cdot 1 &= E_6
\end{align*}
\begin{example} Let $N=4$ be fixed.
\begin{lstlisting}
module conditional_bit_set(N) (
	input (public) c : BOOL ;
	input (public) mask : Uint_N ;
	input (public) w : Uint_N ;
	output (public) r : Uint_N ;

	begin
		r == if c { w or mask } else { w and not mask } ;
	end ;
)
\end{lstlisting}
\end{example}

% NOTE: ZK-Podcast with Alex Özdemir for the proper branching thing in version 2 of the book.

\subsubsection{Loops} Circuits and R1CS are not general enough to express arbitrary computations, but bounded computations only. As a consequence it is not possible to represent unbounded loops like $while TRUE do {}$ in algebraic circuits or rank-1 constraints systems. This can be easily seen since circuits are acyclic graphs and hence unbounded loops would require circuits of unbounded sizes. However bounded loops are expressible, simply by enrolling the loop. 

\begin{example}
\begin{lstlisting}
module counting_bits(N) (
for (c = 0; v; v >>= 1)
{
  c += v & 1;
}

	begin
		r == a xor ((a xor b) & mask) ;
	end ;
)
\end{lstlisting}
\end{example}

\subsection{Gadgets}
\subsubsection{Binary representations}
If the underlying field has a modulus $p$, such that $2^N-1 < p$, then there is a standard way to transform field elements $x\in \F_p$ of size $x<2^N$ into a UIntN bit representation and vice versa.

To make the UintN type more human readable, compilers might introduce some synthactic suggar and outside of the circuit converging back and forth between the base $2$ and base $10$ representation of the UintN type. A standard way to do it is as follows: 

Consider a base $10$ representation $x$ of a UintN type. Then its binary representation 
$(b_0,\ldots,b_{N-1})$ can be computed by 
\begin{lstlisting}
input x : UINT_N ; 
output b[N] : BOOL ; 
var lc1=0;
var e2=1;
for (var i = 0; i < N; i++) {
    b[i] <-- (in >> i) & 1;
    lc1 += b[i] * e2;
    e2 = e2+e2;
}
\end{lstlisting}
This computation is of course done outside of the circuit as a high level inteface for human friendly input. On the other hand if the internal representation $(b_1,\ldots, b_{N-1})$ is given, then the human readable base $10$ representation is given by:
\begin{lstlisting}
input b[N] : BOOL ; 
output x : UINT_N ; 
var lc1=0;
var e2=1;
for (var i = 0; i < N; i++) {
    b[i] <-- (in >> i) & 1;
    lc1 += b[i] * e2;
    e2 = e2+e2;
}
\end{lstlisting}



In computations like scalar multiplication of elliptic curve points its is often necessary to use a binary representation of elements from the base field type. It is therefore necesaary to have a way to transform field elements into their binary representation and vice versa in circuits.

To derive such a circuit over a prime field $\F_p$, let $m=|p_{base_2}|$ be the smallest number of bits necessary to represent the prime modulus $p$ itself. Then a bitstring $(b_0,\ldots,b_{m-1})\in \{0,1\}^m$ is a binary representation of a field element $x\in\F_p$, if and only if
$$
x = b_0\cdot 2^0 + b_1\cdot 2^1 + \ldots + b_m\cdot 2^{m-1}
$$ 
In this expression, addition and exponentiation is considered to be executed in $\F_p$, which is well defined, since all terms $2^j$ for $0\leq j \leq m$ are elements of $\F_p$. Note however that in contrast to the binary representation of counting numbers $n\in\N$, this representation is not unique in prime fields for odd prime numbers. 
\begin{example} Considering the prime field $\F_{13}$. To compute binary representations of elements from that field, we start with the binary representation of prime modulus $13$, which is $13_{base_2} = (1,0,1,1)$ since 
$13= 1\cdot 2^0 + 0\cdot 2^1 + 1\cdot 2^2 + 1\cdot 2^3$. So $m=4$ and we need up to $4$ bits to represent any element $x\in\F_{13}$.

To see that binary representations are not unique in general, consider the element $2\in \F_{13}$. It has the binary representations $2_{base_2}=(0,1,0,0)$ as well as $2_{base_2}=(1,1,1,1)$, since in $\F_{13}$ we have
$$
2 = \begin{cases}
0\cdot 2^0 + 1\cdot 2^1 + 0\cdot 2^2 + 0\cdot 2^3\\
1\cdot 2^0 + 1\cdot 2^1 + 1\cdot 2^2 + 1\cdot 2^3
\end{cases}
$$
\end{example}
Considering that the underlying prime field is fixed and the most significant bit of the prime modulus is $m$, the following circuit flattens equation XXX, assuming all inputs $b_1$, $\ldots$, $b_m$ are restricted to be either $0$ or $1$:
\begin{center}
\digraph[scale=0.3]{BINARYREP}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;

  subgraph cluster0 {
    n1 [shape=box, label="b_0"] ;
    n2 [shape=box, label="2^0"] ;
    n3 [label="*"] ;

    n1 -> n3 ;
    n2 -> n3  [xlabel="I_0"] ;
    color=white ;
  }

  subgraph cluster1 {
    n4 [shape=box, label="b_1"] ;
    n5 [shape=box, label="2^1"] ;
    n6 [label="*"] ;

    n4 -> n6 ;
    n5 -> n6  [xlabel="I_1  "] ;
    color=white ;
  }

  subgraph cluster2 {
    n7 [shape=box, label="b_2"] ;
    n8 [shape=box, label="2^2"] ;
    n9 [label="*"] ;

    n7 -> n9 ;
    n8 -> n9 [xlabel="I_2  "];
    color=white ;
  }

  subgraph cluster3 {
    n10 [shape=box, label="...", color=lightgrey] ;
    color=white ;
  }

  subgraph cluster4 {
    n11 [shape=box, label="b_(m-1)"] ;
    n12 [shape=box, label="2^(m-1)"] ;
    n13 [label="*"] ;

    n11 -> n13 ;
    n12 -> n13  [xlabel="I_(m-1)  "] ;
    color=white ;
  }

  subgraph cluster5 {
    n18 [shape=box, label="x"] ;
    n19 [shape=box, label="-1"] ;
    n20 [label="*"] ;

    n18 -> n20  [xlabel="I_m"] ;
    n19 -> n20 ;
    color=white ;
  }

  n14 [label="+"] ;
  n15 [label="+"] ;
  n16 [label="+", color=lightgrey] ;
  n17 [label="+"] ;
  n21 [label="+"] ;
  n22 [shape=0, label="0"] ;
  n3 -> n14 ;
  n6 -> n14 ; 
  n14 -> n15 ;
  n9 -> n15 ;
  n10 -> n16 [style=dashed, color=lightgrey] ;
  n15 -> n16 [style=dashed, color=lightgrey] ;
  n13 -> n17 ;
  n16 -> n17 [style=dashed, color=lightgrey] ;
  n20 -> n21 ;
  n17 -> n21 ;
  n21 -> n22  [xlabel="W_1=0  "] ;
}
\end{center}
Applying the general transformation rule into rank-1 constraint systems, we see that we actually only need a single constraint to enforce a binary representation of any field element. We get 
$$
(b_0\cdot 2^0 + b_1\cdot 2^1 + b_2\cdot 2^2 + \ldots + b_{m-1}\cdot 2^{m-1} -x)\cdot 1 = 0
$$
In designing more complex circuits from simple ones it is often conceptually as well as visually useful to collaps circuits into simple representative description. To do so, we write 
\begin{center}
\digraph[scale=0.6]{BINREPMETA}{
  forcelabels=true;
  center=true;
  splines=ortho;
  //<nodesep= 2.0;
  n1 [shape=box, label="BASE2"] ;
  n2 [shape=none, label="  "] ;
  n3 [shape=none, label="  "] ;
  n4 [shape=none, label="  "] ;
  n5 [shape=none, label="  "] ;
  n6 [shape=none, label="x"] ;
  n2 -> n1 ;
  n3 -> n1 ;
  n4 -> n1 [style=dashed];
  n5 -> n1 ;
  n6 -> n1 ;
}
\end{center}
indicating that the BASE2 circuit takes $m$ input, has no output and constraints the $x$ input to be the BLABLABLA
\begin{example} Considering the prime field $\F_{13}$, we want to enforce the binary representation of $7\in \F_{13}$. We know $m=4$ from example XX and we have to enforce a $4$-bit representation for $7$, which is $(1,1,1,0)$, since $7= 1\cdot 2^0 + 1\cdot 2^1 + 1\cdot 2^2 + 0\cdot 2^3$.

A valid circuit assignment is therefore given by $(I_0,I_1,I_2,I_3,I_4)=(1,1,1,0,7)$ and indeed we satify the required 5 constraints including the $4$ boolean constraints for $I_0$, $\ldots$, $I_3$ as 
\begin{align*}
1\cdot (1-1) &= 0 & \text{// boolean constraints}\\
1\cdot (1-1) &= 0 \\
1\cdot (1-1) &= 0 \\
0\cdot (1-0) &= 0  \\
(1 + 2 + 4 + 0 -7)\cdot 1 &= 0  & \text{// binary rep. constraint}
\end{align*}
\end{example}

\subsubsection{Range Proofs}
$x>5$...


\subsection{Cryptographic Primitives}
\subsubsection{Twisted Edwards curves}
Sometimes it required to do elliptic curve cryptography "inside of a circuit". This means that we have to implement the algebraic operations (addition, scalar multiplication) of an elliptic curve as a R1CS. To do this efficiently the curve that we want to implement must be defined over the same base field as the field that is used in the R1CS. 

% implmentations https://github.com/iden3/circomlib/blob/master/circuits/babyjub.circom

\begin{example}
So for example when we consider an R1CS over the field $\F_{13}$ as we did in example XXX, then we need a curve that is also defined over $\F_{13}$. Moreover it is advantegous to use a (twisted) Edwards curve inside a circuit, as the addition law contains no branching (See XXX). As we have seen in XXX our Baby-Jubjub curve is an Edwards curve defined over $\F_{13}$. So it is well suited for elliptic curve cryptography in our pend and paper examples
\end{example}

\paragraph{Twisted Edwards curves constraints} As we have seen in XXX, an Edwards curve over a finite field $F$ is the set of all pairs of points $(x,y)\in \F\times \F$, such that $x$ and $y$ satisfy the equation $a\cdot x^2+y^2= 1+d\cdot x^2y^2$. 

We can interpret this equation as a constraint on $x$ and $y$ and rewrite it as a R1CS by applying the flattenin technique from XXX.
$$
\begin{array}{lcr}
x \cdot x &=& x\_sq\\
y \cdot y &=& y\_sq\\
x\_sq \cdot y\_sq &=& xy\_sq\\
(a\cdot x\_sq+y\_sq)\cdot 1 &=& 1+d\cdot xy\_sq
\end{array}
$$
So we have the statement $w=(1,x,y,x\_sq, y\_sq, xy\_sq)$ and we need 4 constraints to enforce that $x$ and $y$ are points on the Edwards curve $x^2+y^2= 1+d\cdot x^2y^2$. Writing the constraint system in matrix form, we get:
\begingroup
    \fontsize{9pt}{9pt}\selectfont
$$
\begin{pmatrix}
0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & a & 1 & 0 
\end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \\ x\_sq \\ y\_sq \\ xy\_sq \end{pmatrix}\odot
\begin{pmatrix}
0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
1 & 0 & 0 & 0 & 0 & 0 
\end{pmatrix}  \begin{pmatrix} 1 \\ x \\ y \\ x\_sq \\ y\_sq \\ xy\_sq \end{pmatrix} =
\begin{pmatrix}
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 & d 
\end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \\ x\_sq \\ y\_sq \\ xy\_sq \end{pmatrix}
$$
\endgroup
EXERCISE: WRITE THE R1CS FOR WEIERSTRASS CURVE POINTS 
\begin{example}[Baby-JubJub]
Considering our pen and paper Baby JubJub curve over from XXX, we know that the curve is defined over $\F_{13}$ and that $(11,9)$ is a curve point, while $(2,3)$ is not a curve point. 

Starting with $(11,9)$, we can compute the statement $w=(1,11,9,4,3,12)$. Substituting this into the constraints we get
$$
\begin{array}{lcr}
11 \cdot 11 &=& 4\\
9 \cdot 9 &=& 3\\
4 \cdot 3 &=& 12\\
(1\cdot 4+3)\cdot 1 &=& 1+7\cdot 12
\end{array}
$$
which is true in $\F_{13}$. So our statement is indeed a valid assignment to the twisted Edwards curve constraining system.

Now considering the non valid point $(2,3)$, we can still come up with some kind of statement $w$ that will satisfy some of the constraints. But fixing $x=2$ and $y=3$, we can never satisfy all constraints. For example $w=(1,2,3,4,9,10)$ will satisfy the first three constraints, but the last constrain can not be satisfied. Or $w=(1,2,3,4,3,12)$ will satisfy the first and the last constrain, but not the others.
\end{example}
\paragraph{Twisted Edwards curves addition} As we have seen in XXX one the major advantages of working with (twisted) Edwards curves is the existence of an addition law, that contains no branching and is valid for all curve points. Moreover the neutral element is not "at infinity" but the actual curve poin $(0,1)$.

As we know from XXX, give two points $(x_1,y_1)$ and $(x_2,y_2)$ on a twisted Edwards curve their sum is given by
$$
(x_3,y_3) = \left(\frac{x_1y_2+y_1x_2}{1+d\cdot x_1x_2y_1y_2}, \frac{y_1y_2-a\cdot x_1x_2}{1-d\cdot x_1x_2y_1y_2}\right)
$$
% https://z.cash/technology/jubjub/
We can use the division circuit from XXX to flatten this equation into an algeraic circuit. Inputs to the circuit are then the two curve points $(x_1,y_1)$ abd $(x_2,y_2)$ as well as the the two denominators $denum_1 = 1+d\cdot x_1x_2y_1y_2$ as well as $denum_2= 1-d\cdot x_1x_2y_1y_2$. We get
\begin{center}
\digraph[scale=0.6]{EDWARDSADD}{
  forcelabels=true;
  center=true;
  splines=ortho;
  //nodesep= 2.0;
  
  subgraph clusterin {
    n1 [shape=box, label="x_1"] ;
    n2 [shape=box, label="x_2"] ;
    n3 [shape=box, label="y_1"] ;
    n4 [shape=box, label="y_2"] ;
      
    n22 [shape=box, label="denom_1"] ;
    n23 [shape=box, label="denom_2"] ;
  
    color=white ;
  }
  
  subgraph clusterout {
    n29 [shape=box, label="x_3"] ;
    n30 [shape=box, label="y_3"] ;
  
    color=white ;
  }

    n5 [shape=box, label="a"] ;
    n6 [shape=box, label="d"] ;
    n7 [shape=box, label="1"] ;
    n8 [shape=box, label="-1"] ;
    
    n9 [label="*"] ; // x_1*y_2
    n10 [label="*"] ; // x_1*x_2
    n11 [label="*"] ; // y_1*x_2
    n12 [label="*"] ; // y_1*y_2
    n13 [label="*"] ; // a*(x_1*x_2)
    n14 [label="*"] ; // -a*(x_1*x_2)
    n15 [label="+"] ; // x_1*y_2 + y_1*x_2
    n16 [label="+"] ; // y_1*y_2 - a*x_1*x_2
    n17 [label="*"] ; // (x_1*x_2)*(y_1*y_2)
    n18 [label="*"] ; // d*(x_1*x_2)*(y_1*y_2)
    n19 [label="*"] ; // -d*(x_1*x_2)*(y_1*y_2)
    n20 [label="+"] ; // 1 + d*(x_1*x_2)*(y_1*y_2)
    n21 [label="+"] ; // 1 - d*(x_1*x_2)*(y_1*y_2)
    
    n24 [label="*"] ; // (1 + d*(x_1*x_2)*(y_1*y_2))*denom_1 =1 
    n25 [label="*"] ; // (1 - d*(x_1*x_2)*(y_1*y_2))*denom_2 =1 
    n26 [shape=box, label="1"] ;
    n27 [label="*"] ; // denom_1*(x_1*y_2 + y_1*x_2) 
    n28 [label="*"] ; // denom_2*(y_1*y_2 - a*x_1*x_2) 
    
    n1 -> n9 [headlabel=" E_1"];
    n1 -> n10 [taillabel="E_1"];
    n2 -> {n10, n11} [taillabel="E_2"];
    n3 -> n11 [headlabel=" E_3"];
    n3 -> n12 [taillabel="E_3"];
    n4 -> n9 [taillabel="E_4"];
    n4 -> n12 [headlabel="  E_4"];
    n5 -> n13 ;
    n6 -> n18 ;
    n7 -> {n20, n21}
    n8 -> {n14, n19} ;
    n9 -> n15 [headlabel=" E_7"] ;
    n10 -> n13 [xlabel="E_8"] ;
    n10 -> n17 [xlabel="E_8"] ;
    n11 -> n15 [xlabel="E_9"] ;
    n12 -> n16 [taillabel="E_10 "] ;  
    n12 -> n17 [xlabel="  E_10"] ;   
    n13 -> n14 ;
    n14 -> n16 ;
    n15 -> n27 ;
    n16 -> n28 ; 
    n17 -> n18 [xlabel="E_11"] ;
    n18 -> {n19, n20} ;
    n19 -> n21 ;
    n20 -> n24 ;
    n21 -> n25 ;
    n22 -> {n24, n27} [xlabel="E_5"] ;
    n23 -> {n25, n28}  [xlabel="E_6"] ;
    n24 -> n26 [xlabel="E_12=1"] ;
    n25 -> n26 [xlabel="E_13=1"] ;
    
    n27 -> n29 [xlabel="E_14"] ;
    n28 -> n30 [xlabel="E_15"] ;
    
}
\end{center}
Using the general technique from XXX to derive the associated rank-1 constraint system, we get the following result:
\begin{align*}
E_1 \cdot E_4 & = E_7 \\
E_1 \cdot E_2 & = E_8 \\
E_2 \cdot E_3 & = E_9 \\
E_3 \cdot E_4 & = E_{10} \\
E_8 \cdot E_{10} & = E_{11} \\
E_5 \cdot (1+ d\cdot E_{11}) & = 1 \\
E_6 \cdot (1 - d\cdot E_{11}) & = 1 \\
E_5 \cdot (E_9 + E_7) & = E_{14} \\
E_6 \cdot (E_{10} - a\cdot E_8) & = E_{15}
\end{align*}

So we have the statement $w=(1,x_1,y_1,x_2,y_2,x_3,y_3,x_{12},y_{12},xy_{12},yx_{12},xy_{1212})$ and we need 7 constraints to enforce that $(x_1,y_1)+(x_2,y_2)=(x_3,y_3)$ 
\begin{example}[Baby-JubJub]
Considering our pen and paper Baby JubJub curve over from XXX. We recall from XXX that $(11,9)$ is a generator for the large prime order subgroup. We therefor already know from XXX that
$(11,9) + (7,8) = (11,9) + [3](11,9) = [4](11,9) = (2,9)$. So we compute a valid statement as 
$w=(1,11,9,7,8,2,9,12,7,10,11,6)$. Indeed
$$
\begin{array}{lcl}
11\cdot 7 &=& 12\\
9\cdot 8 &=& 7\\
11\cdot 8 &=& 10\\
9\cdot 7 &=& 11\\
10\cdot 11 &=& 6\\
2\cdot (1+7\cdot 6) &=& 10 + 11\\
9\cdot (1-7\cdot 6) &=& 7 -1\cdot 12
\end{array}
$$
\end{example}
There are optimizations for this using only 6 constraints, available:
% https://github.com/filecoin-project/zexe/blob/master/snark-gadgets/src/groups/curves/twisted_edwards/mod.rs#L129

\paragraph{Twisted Edwards curves inversion} Similar to elliptic curves in Weierstrass form, inversion is cheap on Edwards curve as the negative of a curve point $-(x,y)$ is given by $(-x,y)$. So a curve point $(x_2,y_2)$ is the additive inverse of another curve point $(x_1,y_1)$ precisely if the equation $(x_1,y_1) = (-x_2,y_2)$ holds. We can write this as
$$
\begin{array}{lcl}
x_1 \cdot 1 &=& -x_2 \\
y_1 \cdot 1 &=& y_2
\end{array}
$$
We therefor have a statement of the form $w=(1,x_1,y_1,x_2,y_2)$ and can write the constraints into a matrix equation as
$$
\begin{pmatrix}
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0
\end{pmatrix} \begin{pmatrix} 1 \\ x_1 \\ y_1 \\ x_2 \\ y_2 \end{pmatrix}\odot
\begin{pmatrix}
1 & 0 & 0 & 0 & 0\\
1 & 0 & 0 & 0 & 0
\end{pmatrix} \begin{pmatrix} 1 \\ x_1 \\ y_1 \\ x_2 \\ y_2 \end{pmatrix} =
\begin{pmatrix}
0 & 0 & 0 & -1 & 0\\
0 & 0 & 0 & 0 & 1
\end{pmatrix} \begin{pmatrix} 1 \\ x_1 \\ y_1 \\ x_2 \\ y_2 \end{pmatrix}
$$

In addition we need the following constraints:
$$
\begin{array}{lcl}
x_1 \cdot 1 &=& -x_2 \\
y_1 \cdot 1 &=& y_2
\end{array}
$$

\paragraph{Twisted Edwards curves scalar multiplication} 
% original circuit is here https://iden3-docs.readthedocs.io/en/latest/_downloads/33717d75ab84e11313cc0d8a090b636f/Baby-Jubjub.pdf

Although there are highly optimzed R1CS implementations for scal multiplication on elliptic curves, the basic idea is somewhat simple: Given an elliptic curve $E/\F_r$, a scalar $x\in \F_r$ with binary representation $(b_0,\ldots,b_m)$ and a curve point $P\in E/\F_r$, the scalar multiplication $[x]P$ can be written as
$$
[x]P = [b_0]P + [b_1]([2]P) + [b_2]([4]P) + \ldots + [b_m]([2^m] P)
$$
and since $b_j$ is either $0$ or $1$, $[b_j](kP)$ is either the neutral element of the curve or $[2^j]P$. However $[2^j]P$ can be computed inductively by curve point doubling, since $[2^j]P= [2]([2^{j-1}]P)$.

So scalar multiplication can be reduced to a loop of length $m$, where the original curve point is repeadedly douled and added to the result, whenever the appropriate bit in the scalar is equal to one.

So to enforce that a curve point $(x_2,y_2)$ is the scalar product $[k](x_1,y_1)$ of a scalar $x\in F_r$ and a curve point $(x_1,y_1)$, we need an R1CS the defines point doubling on the curve (XXX) and an R1CS that enforces the binary representation of $x$ (XXX). 

In case of twisted Edwards curve, we can use ordinary addition for doubling, as the constraints works for both cases (doublin is addition, where both arguments are equal). Moreover $[b](x,y)=(b\cdot x, b\cdot y)$ for boolean $b$. Hence flattening equation XXX gives
$$
\begin{array}{lclr}
b_0\cdot x_1 &=& x_{0,1} & // [b_0]P\\
b_0\cdot y_1 &=& y_{0,1}\\

\end{array}
$$
In addition we need to constrain $(b_0,\ldots, b_N)$ to be the binary representation of $x$ and we need to constrain each $b_j$ to be boolean.

As we can see a R1CS for scalar multiplication utilizes many R1CS that we have introduced before. For efficiency and readability it is therefore useful to apply the concept of a gadget (XXX). A pseudocode method to derive the associated R1CS could look like this:

%\begin{algorithmic}
%\Require $m$ Bitlength of modulus
%\Statement $w \gets [x,b[m],mid[m]]$
%\State $tmp \gets 0$
%\For{$j\gets 1,\ldots, m$}
%	\State \textbf{Constrain:} $b[j]\cdot (1-b[j]) == 0$
%	\State \textbf{Constrain:} $b[j] \cdot 2^j == mid[j]$
%	\State $tmp = tmp + mid[j]$
%\EndFor
%\State \textbf{Constrain:} $tmp \cdot 1 == x$
%\end{algorithmic}

%\begin{codebox}
%\Procname{$\proc{Insertion-Sort}(A)$}
%\li \For $j \gets 2$ \To $\id{length}[A]$
%\li     \Do$\id{key} \gets A[j]$
%\li         \Comment Insert $A[j]$ into the sorted sequence $A[1 \twodots j-1]$.
%\li         $i \gets j-1$\li         \While $i > 0$ and $A[i] > \id{key}$
%\li             \Do$A[i+1] \gets A[i]$
%\li                 $i \gets i-1$\End
%\li         $A[i+1] \gets \id{key}$\End
%\end{codebox}

\subsubsection{A Simple Pen and Paper Compiler Example}
% Set membership proof?


\subsection{Outlook on Real World Implementations}
many circuits can be found here:
% https://github.com/iden3/circomlib

Use the description of Özdemir in Ana's podcast. 

