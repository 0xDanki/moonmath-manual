\chapter{Statements}
% https://people.cs.georgetown.edu/jthaler/ProofsArgsAndZK.pdf
% https://docs.zkproof.org/reference.pdf
As we have seen in the informal introduction XXX, a snarks is a short non-interactive argument of knowledge, where the knowledge-proof attests to the correctness of statements like "I know the prime factorization of a given number" or "I know the preimage to a given SHA3 digest value" and similar things. However human readable statements like those are imprecise and not very useful from a formal perspective. 

In this chapter we therefore look more closely at ways to formalize statements in mathematically rigorous ways, useful for snark development. We start by introducing formal languages as a way to define statements properly. We will then look at algebraic circuits and rank-1 constraint systems as two particulary useful ways to define statements in certain formal languages.

As in many other parts of the book the emphesis is on an introduction from the developers point of view. Proper statement design should be of high priority in the development of snarks, since unintended true statements can lead to potentially severe and almost undetectable attacks on the applications of snarks.

\section{Formal Languages} Formal languages provide the theoretical backround in which logical statements can be formulated in a logically regious way. One might argue that understanding of formal languages is not very important in snark development and associated statement design, but terms from that field of research are standard jargon in many papers on zero knowledge proofs. We therefore believe that at least some introduction to formal languages and how they fit into the picture of snark development is beneficial, mostly to give developers a better intuition where all this is located in the bigger picture of the logic landscape.

Roughly speaking a formal language is nothing but a set of words, that are strings of letters taken from some alphabet and formed according to some defining rules of that language. 

To be more precise, let $\Sigma$ be any set and $\Sigma^*$ the set of all finite tupels $(x_1,\ldots,x_n)$ of elements $x_j$ from $\Sigma$ including the empty tupel $(\;)\in \Sigma^*$. Then a \textbf{formal language} $L$ is in its most general definition nothing but a subset of $\Sigma^*$. In this context, the set $\Sigma$ is called the \textbf{alphabet} of the language $L$, elements from $\Sigma$ are called letters and elements from $L$ are called \textbf{words}. The rules that specify which tupels from $\Sigma^*$ belong to the language and which don't, are called the \textbf{grammar} of the language. 

\paragraph{Checking Relations}
In the context of snark development it is common to formalize the grammar of a laguage by a so called \textbf{checking relation} $R\subset \Sigma^*$. If a checking relation is given, a tupel $x\in \Sigma^*$ is a word $x\in L_R$ in the language $L_R$ associated to the relation $R$, if $x\in R$, which is usually written as $R(x)$.

Unfortunately, writing $R(x)$ for the relation $R$ is a bit misleading since it makes $R$ look like as if it is a function. It is nevertheless the common way of describing checking relations. For the sake of this book we therefore adopt a different point of view and work with what we might call \textbf{checking functions} inatead:
\begin{equation}
R: \Sigma^* \to \{TRUE, FALSE\}
\end{equation}
A checking function therefore decides if a tupel $x\in \Sigma^*$ is an element of the language or not. In this case the language itself can be written as the set of all tupels that satisfies the grammar, i.e as:
\begin{equation}
L := \{x\in \Sigma^*\;|\; R(x)=TRUE\}
\end{equation}
In our context a \textbf{statement} is then a claim, that language $L$ contains a word $x$, i.e a statement claims $x\in L$ and one way to constructively \textit{proof} a staitment is to provided an actual \textbf{instance}, that is an actual word of the language.

Of course formal languages should not be confused with the more intuitive concept of a natural langual. The following examples will provide some intuition about formal languages.
\begin{example}[Alternating Binary strings] To consider a very basic formal language with an almost trivial grammar consider the set $\{0,1\}$ of the two letters $0$ and $1$ as our alphabet $\Sigma$ and imply the rule that a proper word must consist of alternating binary letters of arbitrary length. 

Then the associated language $L_{alt}$ is the set of all finite binary tupels, where a $1$ must follow a $0$ and vice versa. So for example $(1,0,1,0,1,0,1,0,1)\in L_{alt}$ is a proper word as well as $(0)\in L_{alt}$ or the empty word $(\;)\in L_{alt}$. However the binary tupel $(1,0,1,0,1,0,1,1,1)\in \{0,1\}^*$ is not a proper word as it violates the grammer of $L_{alt}$. In addition the tupel $(0,A,0,A,0,A,0)$ is not a proper word as its letter are not from the proper alphabet. 

Inside language $L_{alt}$ it makes sense to claim the following statement: "There exists an alternating string." One way to proof this statement would be by proving an actual instance, that is finding actual alternating string like 
$x = (1,0,1)$. Constructing a string like $(1,0,1)$ therefore proofs that statement "There exists an alternating string."

Atempting to write the grammar of this language in a more formal way, we can define the following checking relation:
$$
R: \{0,1\}^* \to \{TRUE,FALSE\}\;;\; (x1,x_2,\ldots,x_n) \mapsto 
\begin{cases}
TRUE & x_j \neq x_{j+1} \text{ for all } 1\leq j < n \\
FALSE & \text{ else}
\end{cases}
$$
We can use this relation to check if arbitrary binary tupels are words in $L_{alt}$ or not. For example $R(1,0,1)=TRUE$, $R(0)=TRUE$ and $R()=TRUE$, but $R(1,1)=FALSE $ and so on. 
\end{example}
\begin{example}[Programing Language]Programming languages are a very important class of formal languages. In this case the alphabet is usually (a subset) of the ASCII Table and the grammar is defined by the rules of the programming language's compiler. Words are then nothing but properly written computer programms that the compiler accepts. The compiler can therefore be interpreted as the checking relation.

To give an unusual example strange enough to highlight the point, consider the programing language Malbolge as defined in XXX. This language was specifically designed to be almost impossible to use and writing programs in this language is a difficult task. An intersting claim is therefore the statement: "There exists a computer program in Malbolge". As it turned out proofing this statement by providing an actual instance was not an easy task as it took two years after the introduction of Malbolge, to write a program that its compiler accepts. So for two years no one was able to proof the statement.

To look at this high level description more formally, we write $L_{Malbolge}$ for the language, that uses the ASCII table as its alphabet and words are tuples of ASCII letters that the Malbolge compiler accepts. Prooving the statement "There exists a computer program in Malbolge" is the equivalent to the task of finding some word $x\in L_{Malbolge}$. The string
$$
\scriptstyle (=<'\#9]~6ZY327Uv4-QsqpMn\&+Ij"'E\%e\{Ab~w=\_:]Kw\%o44Uqp0/Q?xNvL:'H\%c\#DD2\wedge WV>gY;dts76qKJImZkj
$$
is an example of such a proof as it is excepted by the Malbolge compiler and is compiled to an executable binary that displays "Hello, World." (See XXX)
\end{example}
\begin{example}[3-Factorization]
As one of our main runing examples in this book, we want to develop a snark that proofs knowledge of three factors of some element from the finite field $\F_{13}$. There is nothing particulary useful about this example from an application point of view, however in a sense it is the most simple example that gives rise to a non trivial snark in some of the most common zero knowledge proofing systems. 

Formalizing the a high level description, we can define $\Sigma := \F_{13}$ as the alphabet of our language and then define a language $L_{3.fac}$ to consists of those tupels of field elements from $\F_{13}$, that contain exactly $4$ letters $w_1,w_2,w_3,w_4$ which satisfy the equation $w_1\cdot w_2\cdot w_3 =w_4$.   

So for example the tuple $(2, 12, 4, 5)$ is a word in $L_{3.fac}$, while neither $(2, 12, 11)$, nor $(2, 12, 4, 7)$ nor $(2, 12, 7, 168)$ are words in $L_{3.fac}$ as they dont satisfy the grammar or are not define over the proper alphabet. 

We can describe the language $L_{3.fac}$ more formally by introducing a checking function as described in XXX:
$$
R_{3.fac} : \F_{13}^* \to \{TRUE, FALSE\}\;;\;
(x_1,\ldots,x_n) \mapsto
\begin{cases}
TRUE & n=4 \text{ and } x_1\cdot x_2 \cdot x_3 = x_4\\
FALSE & else
\end{cases}
$$
Having defined the language $L_{3.fac}$ it then makes sense to claim the statement like "There is a word in $L_{3.fac}$" which is logically equivalent to say "There are four elements $w_1,w_2,w_3,w_4$ from the finite field $\F_{13}$" such that the equation $w_1\cdot w_2\cdot w_3 =w_4$ holds. Proofing the correctness of this statement could then be achieved by finding actual field elements like $x_1= 2$, $x_2 =12$, $x_3=4$ and $x_4 = 5$ giving the $L_{3.fac}$ instance $(2,12,4,5)$, which is a proof for our statement.
\end{example}
\paragraph{Instance and Witness}
% https://www.claymath.org/sites/default/files/pvsnp.pdf
As we have seen statements can formulated as membership claims in formal languages and instances can serve as constructive proofs for those claims. However, in the context of \textit{zero-knowledge} proofs  its possible to hide parts of the proofing instance and still be able to proof the statement. In such a context the instance is therefore split into a \textit{public part} which again is called the \textbf{instance} and a not publically known part (a private part) called the \textbf{wittness}.

To acknowledge for this seperation of a proof instance into a public and a private part, our previous definition of a formal language needs a refinement in the context of zero-knowledge proofs. Instead of a single alphabet we consider two alphabets $\Sigma_I$ and $\Sigma_W$, such that the words of the languages are tupels $(i|w)\in \Sigma_I^* \times \Sigma_W^*$ subject to a checking relation
\begin{equation}
R: \Sigma_I^* \times \Sigma_W^* \to \{TRUE, FALSE\}
\end{equation}
that decides if a tupel $(i|w)\in \Sigma_I^* \times \Sigma_W^*$ is an element of the language or not. Again the language itself can be written as the set of all tupels that satisfies the grammar, i.e as:
\begin{equation}
L := \{(i,w)\in \Sigma_I^* \times \Sigma_W^*\in \Sigma^*\;|\; R(i|w)=TRUE\}
\end{equation}
In this case we call a public input $i$ an \textbf{instance} and a private input $w$ a \textbf{wittness} of the relation $R$.

In this context a \textbf{statement} is the claim, that given an instance $i\in\Sigma_I^*$ there is a wittness $w\in \Sigma_W^*$, such that language $L$ contains a word $(i|w)$. As in the situation of more general languages one way to constructively \textit{proof} such a statement is to provided an actual pair $(i|w)$ with $R(i|w)$, however the point of zero-knowledge proofing systems, as we will see in XXX, is to proof such a statement, without revealing any knowledge about $w$. 

So while statements in the sense of the previous chapter can be seen as membership proofs, statements in this refined definition in combination with publically known instances are knowledge-proofs rather.
\begin{example}[3-factorization]
Consider the language $L_{3.fac}$ from example XXX again. Providing instances for the membership statement in this languages is equivalent to providing knowledge of 4 elements $x_1$, $x_2$, $x_3$ and $x_4$ from $\F_{13}$, such that the modular $13$ product of the first three elements is equal to the $4$'th element. 

Splitting instances into private and public parts, we can reformulate the problem introducing different levels of zero-knowledge into the problem. For example we could reformulate the non-hiding membership statement "there are $4$ field elements $x_1$, $x_2$, $x_3$ and $x_4$ from $\F_{13}$, such that $x_1\cdot x_2\cdot x_3 = x_4$ into a statement, where all factors $x_1$, $x_2$, $x_3$ of $x_4$ are private and only the product $x_4$ is public. 

A statement would then be something like "Given a publically known field element $x_4$, there are three factors of $x_4$" and as we will see in XXX a zero-knowledge proofing system is able to proof this statement without revealing anything about the factors $x_1$, $x_2$, or $x_3$.

Formalizing this new language, which we might call $L_{3.fac\_zk}$ we define a checking relation by 
\begin{multline*}
R_{3.fac\_zk} : \F_{13}^* \times \F_{13}^* \to \{TRUE, FALSE\}\;;\;\\
((i_1,\ldots,i_n)|(w_1,\ldots, w_m)) \mapsto
\begin{cases}
TRUE & n=1,\; m=3,\; i_1 = w_1\cdot w_2 \cdot w_3\\
FALSE & else
\end{cases}
\end{multline*}
and as usual $L_{3.fac\_zk}$ is then defined by all tupels from $\F_{13}^* \times \F_{13}^*$ that are mapped onto $TRUE$ under $R_{3.fac\_zk}$. 

Since words in $L_{3.fac\_zk}$ are tuples $(i|w)$ consisting of an instance and a wittness, there are different possibilities to formulate statements. The most general one would be equivalent to the one in XXX claiming "there are words in $L_{3.fac\_zk}$" and a proof, could be given by a concrete pair $(i|w)\in L_{3.fac\_zk}$, such as$(5|2,12,4)$. 

However as explained in XXX, in the context of zero knowlege proofs, statements are rather knowledge-claims like "Given public input $i$, there is a private input $w$. So for example in $L_{3.fac\_zk}$ with public input $i=5$ a proof for the associated statement could be given by $w=(2,12,4)$.

As we will see in XXX, zero-knowledge proofing systems provide techniques to statements like this without revealing anything about the wittness.

One question that arises in this context might be why we decided the factors $x_1$, $x_2$ and $x_3$ to be the wittness and the product $x_4$ to be the instance. This of course was just an arbitrary choice and we could have decided on any other constellation. For example nothing stops us from declaring all variables as private or just $x_1$ or whatever. The actual choice is determined by the application only.
\end{example}
\begin{example}[SHA256 -- Knowlege of Preimage] A standard example to show the power of zero knowlege proofs is proving the knowledge of some preimage of a cryptographic hash function like the SHA256 function, without actually revealing it. 

To understand this example in detail, lets start with introducing a language well suited to build a snark for this problem. Since SHA256 is a function
$$
SHA256: \{0,1\}^* \to {0,1}^{256}
$$
that maps binary string of arbitrary length onto binary strings of length $256$ and we want to proof knowledge of preimages, we have to consider binary strings of size $256$ as instances and binary strings of arbitrary length as witnesses. 

An appropriate alphabet for both the set of all witnesses and the set of all instances is therefore the set $\{0,1\}$ of the two binary letters. We then define a checking relation by
\begin{multline*}
R_{SHA256} : \{0,1\}^* \times \{0,1\}^* \to \{TRUE, FALSE\}\;;\;\\
(i|w) \mapsto
\begin{cases}
TRUE & i.len()=256,\; i = SHA256(w)\\
FALSE & else
\end{cases}
\end{multline*}
and we write $L_{SHA256}$ for the associated language that consists of instance, witness pairs $(i|w)\in L_{SHA256}$ where the instance $i$ is the SHA256 image of the witness $w$. 

Given an instance $i\in \{0,1\}^{256}$ a statements in $L_{SHA256}$ then is the claim, that there is a wittness $w\in \{0,1\}^{*}$, such that $i$ is the image of SHA256 of $w$. One way to proof such a statement would therefore be to actually provide some data that hashes onto $i$. 
\end{example}
\begin{example}[Knowledge of a Discrete Logarithm] As we have explained in XXX computing discrete logarithms can be hard in certain prime fields. An interesting problem is therefore a system that can proof knowledge of discrete logarithms without revealing them.

To formalize a proper statement, lets look at the problem a bit closer. In a more precise sense proofing knowledge of discrete logarithms, is the same thing as finding a solution $x$ to an equation
$$
b^x = y
$$ 
providing that both the base $b$ and the value $y$ are given and that it is understood that $b$ and $y$ are elements from the multiplicative group $\F_{p}^*$ of a prime field for some prime number $p$ and that $x\in \Z_{p-1}$ is a number from modular $(p-1)$-arithmetics, where $(p-1)$ is the order of the multiplicative group of $\F_p$.

We moreover assume $b$ and $p$ to be fixed system parameters and that $y$ is the public input to the problem, while $x$ is the private input. 

We can then define the alphabet $\Sigma_I \times \Sigma_W := \F_p^* \times \Z_{p-1}$ and a checking function
\begin{multline*}
R_{LOG_b(\cdot)} : (\F_p^*)^* \times (\Z_{p-1})^* \to \{TRUE, FALSE\}\;;\;\\
(i|w) \mapsto
\begin{cases}
TRUE & i.len()=1,\; w.len()=1,\; b^w = i\\
FALSE & else
\end{cases}
\end{multline*}
Given an instance $i\in \F_p^*$ a statements in $L_{LOG_b(\cdot)}$ then is the claim, that there is a number $w\in \Z_{p-1}$, such that $w$ is base $b$ discrete logarithm of $i$ in the prime field $\F_p$. One way to proof such a statement would therefore be to actually provide some concrete $w$. 
\end{example}
\paragraph{Modularity} From a developers perspective it is often useful to construct complex statements and their representing languages from simple ones. In the context of zero knowledge proofs those simple building blocks are often called \textit{gadgets} and mature proofing systems usually contain libraries that contain many useful gadgets, like representations of basic types as booleans, unit32, preimage proofs for Hash functions, elliptic curve cryptography and so on. Implementers can then combine these fundamental building blocks to write complex real world applications. 

To understand what this means on the level of formal languages defined by checking relations as explained in XXX, we need to look at the \textit{intersection} of two formal languages, which can be constructed whenever both languages are defined over the same alphabet. In this case the intersection language consists of words that are contained in both languages. To be more precise, let $L_1$ and $L_2$ be two formal languages defined over the instance and witness alphabets $\Sigma_I$ and $\Sigma_W$. Then
\begin{equation}
L_1 \cap L_2 := \{x\;|\; x\in L_1 \text{ and } x\in L_2\}
\end{equation} 
If both languages are defined by checking functions $R_1$ and $R_2$ as explained in XXX, the following function is a checking function for the intersection language $L_1 \cap L_2$:
\begin{equation}
R_{L_1 \cap L_2}: \Sigma_I^* \times \Sigma_W^* \to \{TRUE, FALSE\}\;;\;
(i,w) \mapsto R_1(i,w) \text{ and } R_2(i,w)
\end{equation}
This is an important fact from an implementations point of view as it allows to construct complex checking relations and their statements from simple building blocks. Given a publically known instance $i\in \Sigma_I^*$ a statement in an intersection language then claims knowledge of a wittness that satisfies all relations simultaniously.

\section{Statement Representations} As we have seen in the previous section, formal languages and their definition by checking relations are a powerful tool to describe statements in a formaly regurous manner. 

However from the perspective of existing zero knowledge proofing systems not all ways to actually represent checking relations are equally useful. Depending on the proofing system ad hand some are more suitable then others. In this section will therefore describe the most common ways to represent checking relation and their statements.
\subsection{Rank-1 Quadratic Constraint Systems}
Although checking relations for formal languages can be expressed in various ways, many zk-proof systems require the ralation to be expressed in terms of a systems of rank-1 quadratic equations over a finite field. This true in particular for pairing based proofing systems like XXX, roughly because it is possible to check solutions to those equations "in the exponent" of pairing friendly cryptographic groups. 

\paragraph{R1CS representation} To understand what \textit{rank-1 (quadratic) constraint systems} are in detail, let $\F$ be a finite field, $n$, $m$ and $k$ three numbers and $a_j^i$, $b_j^i$ and $c_j^i$ constants from $\F$ for every $0\leq j \leq n+m$ and $1\leq i \leq k$. Then a R1CS is given by: 
\begin{align*}
\scriptstyle\left(a^1_0 + \sum_{j=1}^n a^1_j \cdot I_j + \sum_{j=1}^m a^1_{n+j} \cdot W_j  \right) \cdot 
\left(b^1_0 + \sum_{j=1}^n b^1_j \cdot I_j + \sum_{j=1}^m b^1_{n+j} \cdot W_j  \right) &= 
\scriptstyle c^1_0 + \sum_{j=1}^n c^1_j \cdot I_j + \sum_{j=1}^m c^1_{n+j} \cdot W_j\\
       & \vdots\\
\scriptstyle\left(a^k_0 + \sum_{j=1}^n a^k_j \cdot I_j + \sum_{j=1}^m a^k_{n+j} \cdot W_j  \right) \cdot 
\left(b^k_0 + \sum_{j=1}^n b^k_j \cdot I_j + \sum_{j=1}^m b^k_{n+j} \cdot W_j  \right) &= 
\scriptstyle c^k_0 + \sum_{j=1}^n c^k_j \cdot I_j + \sum_{j=1}^m c^k_{n+j} \cdot W_j       
\end{align*}
If such a R1CS is given, the parameter $k$ is called the \textbf{number of constraints} and if a tuple $(I_1,\ldots, I_n,W_1,\ldots,W_m)$ of field elements satisfies theses equations, $(I_1,\ldots, I_n)$ is called an \textbf{instance} and $(W_1,\ldots,W_m)$ is called a \textbf{wittness} of the system.

\begin{remark}[Matrix notation] The presentation of rank-1 constraint systems can be greatly siplified using the notation of vectors and matrices. In fact if
$x= (1,I,W)\in \F^{1+n+m}$ is a $(n+m+1)$-dimensional vector, $A$, $B$, $C$ are $(n+m+1)\times k$-dimensional matrices and $\odot$ is the Schur/Hadamard product, then a R1CS can be written as
$$
Ax \odot Bx = Cx
$$
However since we did not introduced matrix calculus in the book, we stick to XXX as the defining equations for rank-1 constraints systems. The latter notation just contains more syntactic suggar and the one frequently used in the research literature.
\end{remark}
A major property of rank-1 constraint systems is, that is can be shown, that every bounded computation can be expressed as a R1CS. R1CS are therefore a universal model for bounded computation. We will see in XXX how "normal" computations can be translated into R1CS and how to construct R1CS compilers that transform high level (bounded) computer programs into R1CS.

On a very general level, it can be said that during computations, the idea of R1CS is to keep track of all the values that each variable assumes and to bind the relationships among all those variables that are implied by the computation itself. This way doing a computation in the expected way is ensured by enforcing relations between every consecutive steps in the computation.
\begin{example}[3-Factorization]Consider the language $L_{3.fac\_zk}$ from example XXX again. As we have seen $L_{3.fac\_zk}$ consist of words $(I_1;W_1,W_2,W_3)$ over the alphabet $\F_{13}$, such that $I_1 = W_1\cdot W_2\cdot W_3$. We show how to reformulate the defining grammar as a rank-1 constraint system.

Since R1CS are quadratic equations, expressions like $W_1\cdot W_2\cdot W_3$ that contain products which contain more then two factors, i.e. which are not quadratic, needs to be rewritten in a process often called "flattening". Do do so, we can introcuce a new variable $W_4$ and then define the following two constraints
\begin{align*}
W_1 \cdot W_2 & = W_4 & \text{constraint } 1\\
W_4 \cdot W_3 & = I_1 & \text{constraint } 2
\end{align*}
It can be shown, that given $I_1$, any solution $(W_1,W_2,W_3,W_4)$ to this system provides a solution to the original equation $I_1 = W_1\cdot W_2\cdot W_3$ and vice versa. Both equations are therefore equivalent in the sense that solutions are in a 1:1 correspondence.

To see that XXX is a rank-1 constraint system, choose the parameter $n=1$, $m=4$ and $k=2$ as well as
$$
\begin{array}{llllll}
a_0^1 = 0 & a_1^1= 1 & a_2^1= 0 & a_3^1 = 0 & a_4^1= 0  & a_5^1= 0 \\ 
a_0^2 = 0 & a_1^2= 0 & a_2^2= 0 & a_3^2 = 0 & a_4^2= 1  & a_5^2= 0 \\ 
\\
b_0^1 = 0 & b_1^1= 0 & b_2^1= 1 & b_3^1 = 0 & b_4^1= 0  & b_5^1= 0 \\ 
b_0^2 = 0 & b_1^2= 0 & b_2^2= 0 & b_3^2 = 1 & b_4^2= 0  & b_5^2= 0 \\ 
\\
c_0^1 = 0 & c_1^1= 0 & c_2^1= 0 & c_3^1 = 0 & c_4^1= 1  & c_5^1= 0 \\ 
c_0^2 = 0 & c_1^2= 0 & c_2^2= 0 & c_3^2 = 0 & c_4^2= 0  & c_5^2= 1 
\end{array} 
$$
Then the R1CS of our $3$-factorization problem contains $4$ witness variables $W_1$, $W_2$, $W_3$ and $W_4$ and one instance variable $I_4$. The witness $W_4$ expresses and intermediate computational state and is constrained to be the product of $W_1$ and $W_2$. Summarizing the definition the R1CS  can be written as follows:
\begin{align*}
\scriptstyle
\left(a^1_0 + a_1^1 I_1 + a_2^1 W_1 + a_3^1 W_2 + a_4^1 W_3 + a_5^1 W_4\right)\cdot
\left(b^1_0 + b_1^1 I_1 + b_2^1 W_1 + b_3^1 W_2 + b_4^1 W_3 + b_5^1 W_4\right) &=
\scriptstyle
\left(c^1_0 + c_1^1 I_1 + c_2^1 W_1 + c_3^1 W_2 + c_4^1 W_3 + c_5^1 W_4\right)\\
\scriptstyle
\left(a^2_0 + a_1^2 I_2 + a_2^2 W_2 + a_3^2 W_2 + a_4^2 W_3 + a_5^2 W_4\right)\cdot
\left(b^2_0 + b_1^2 I_2 + b_2^2 W_2 + b_3^2 W_2 + b_4^2 W_3 + b_5^2 W_4\right) &=
\scriptstyle
\left(c^2_0 + c_1^2 I_2 + c_2^2 W_2 + c_3^2 W_2 + c_4^2 W_3 + c_5^2 W_4\right)
\end{align*}
\end{example}

\paragraph{R1CS Satisfiability}To understand how rank-1 constraint systems give rise to formal languages, oberserve that every given R1CS over a fields $\F$ defines a checking relations over the alphabet $\Sigma_I \times \Sigma_W = \F \times \F$ in the following way:
\begin{equation}
R_{R1CS} : \F^* \times \F^* \to \{TRUE, FALSE\}\;;\;
(I;W) \mapsto
\begin{cases}
TRUE & (I;W) \text{ satisfies R1CS}\\
FALSE & else
\end{cases}
\end{equation}
We write $L_{R1CS-SAT}$ for the associated language and call it \textbf{$R1CS$-satisfyability}. The grammar of this language is exapressed by the constraints in the R1Cs and words in this language are solutions to the R1CS. 

Now since every R1CS gives rise to a formal language, a \textbf{statement} for the given R1CS, is a knowledge claim "Given instance $I$, there is a witness $W$, such that $(I;W)$ satisfies the constraints system". One way to proof such a claim is therefore to to produce an assignment to the variables which satisfies the constraints. If the R1CS represents a computations, such a solution is then a proof proper execution of the computation.
\begin{example}[3-Factorization]Consider the language $L_{3.fac\_zk}$ from example XXX and the R1CS defined in example XXX. As we have seen in XXX solutions to the R1CS are in 1:1 correspondense with solutions to the checking relations of $L_{3.fac\_zk}$. Both languages are therefore equivalent in the sense that there is a 1:1 correspondence between words in both languages. Due to this identification, from now on we weite $L_{3.fac\_zk}$ for the language defined by the R1CS XXX.

To give an intuition of how a naive proof of a statement in $L_{3.fac\_zk}$ looks like, consider the instance $I_1= 11$. To provide a proof for the statement "There exist a witness $W$, such that $(I_1;W)$ is a word in $L_{3.fac\_zk}$" a proofer therefore has to privide proper values for the variables $W_1$, $W_2$, $W_3$ and $W_4$. Since the alphabet is $\F_{13}$, such a proof might be given by
$W=(2,3,4,6)$ since $(I_1;W)$ satisfies the R1CS
\begin{align*}
W_1 \cdot W_2 &= W_4 & \text{since } 2\cdot 3 = 6\\
W_4 \cdot W_3 &= I_1 & \text{since } 6\cdot 4 = 11
\end{align*}
\end{example}
\paragraph{Modularity} As we already discussed in XXX, it is often useful to construct complex statements and their representing languages from simple ones. Rank-1 constraint systems are particulary useful for this as the combination of two different R1CS over the same alphabet results in a new R1CS over that same alphabet. 

To be more precise let $S_1$ and $S_2$ be two R1CS over $\F$, the the union 
$S_3 = S_1\cup S_2$ is an R1CS over $\F$ obtained by by first relabelling all variables, such they have different names in both systems and then consider all the constraints from $S_1$ as well as all the constraints from $S_2$. Solutions to $S_3$ therefore have to satisfy both $S_1$ as well as $S_2$. 

This construction then implies, that developers are able to construct complex R1CS from simple building blocks, providing that the are careful with variable naming. 

\subsection{Algebraic Circuits} As we have seen in the previous paragraphs, rank-1 constraint systems define equations that instance witness pairs have to satisfy in order to provide words in the associated language.

However R1CS equations can be large and are non-linear in general. It is therefore not posible to solve those equations efficiently in order to find proper witnesses for given instances. A practical question is therefore how to actually compute witneses for R1CS?

In this paragraph we look at a statement representation called \textit{algebraic circuits}, that is closely related to R1CS but which is suitable to not just define the constraints, but to provide a mechanism to compute witnesses efficiently.

\paragraph{Algebraic circuit representation}
The general idea of an algebraic circuit is that, given some finite field $\F$ every rational function over $\F$ can be seen as a directed acyclic (multi)graph: Inner nodes have exactly two incoming edges and represent the fundamental field operationd \textit{addition} and \textit{multiplication}. Nodes with only outgoing edges (source nodes) represent the variables and constants of the functions and nodes with only incoming edges (sink nodes) represent the results of the function. Edges in the graph then represent the connections between the operations of the individual nodes.

To be more precise let $\F$ be a finite field. Then a directed, acyclic multi-graph $C(\F)$ is called an \textbf{algebraic circuit} over $\F$, if the graph has an ordering, every edge has either an intance label $I_j$ or a witness label $W_j$, where $j$ is the edge's position in the graph's order and every node has two labels in the following way:
\begin{itemize}
\item Every source node (called input) has either a symbol that represents a variable from $\F$ or a constant $c\in\F$ as label.
\item Every sink node (called output) has a symbol that represents a variable from $\F$ as label.
\item Every other node has exactly two incoming edges and has a label that represents either addition or multiplication in $\F$.
\end{itemize}
Internal nodes that are neither source nor sink nodes are also often called \textbf{aithmetic gates}. Arithmetic gates that are decorated with the "$+$"-label are called \textbf{addition-gates} and gates that are decorated with the "$\cdot$"-label are called \textbf{multiplication-gates}.

It should be noted that subtraction gates can be simulated in a two step process by first multiplying the subtrahend with $-1$ and then use an addition gate. Also division gates can be simulated for example by using Fermat's little theorem, which states that the multiplicative inverse of a field element $x\in\F$ is given by the power $x^{p-2}$ and powers are nothing but repeated multiplication. However simulating division this way might be inefficient.

It should also be noted that our definition differs slighly from the way algebraic circuits are used in many other parts of mathematics. They are however useful in the context of zero-knowledge proofing systems, because the edge labels define witness and instances, while the restriction on the number of incoming edges simplifies the translation into R1CS. We will look at this more closely in XXX.

It can be shown that every rational function over $\F$ can be transformed into an algebraic circuit, a process often called \textit{flattening}.
\begin{example}[Generalized factorization snark] To give a simple example of an algebraic circuit, consider our $3$-factorization problem from example XXX again.  To express the problem in the algebraic circuit model, consider the following function 
\[
f_{3.fac}:\mathbb{F}_{13}\times\mathbb{F}_{13}\times\mathbb{F}_{13}\to\mathbb{F}_{13};(x_{1},x_{2},x_{3})\mapsto(x_{1}\cdot x_{2})\cdot x_{3}
\]
The zero-knowledge $3$-factorization problem as described in XXX can then be described in the following way: Given instance $I_1\in \F_{13}$ a valid witness a preimage of $f_{3.fac}$ at the point $I_1$, i.e. a valid witness are three values $W_1$, $W_2$ and $W_3$ from $\F_{13}$, such that $f_{3.fac}(W_1,W_2,W_3)=I_1$. 

To flatten function $f_{3.fac}$, that is to find some circuit that describes the function, observe that $f_{3.fac}$ needs two multiplications to compute its output. We therefore need a circuit that contains two multiplication gates and nothing more. We get:
\[
\xymatrix{x_1\ar^{W_1}[dr] &  & x_2\ar_{W_2}[dl]\\
 & \cdot \ar^{W_4}[drr] &   & & x_3\ar_{W_3}[dl]\\
  &  &  & \cdot\ar_{I_1}[d]\\
  &  &  & f(x_1,x_2,x_3)
}
\]
So our directed acyclic multi-graph, is is binary tree with three leaves (the source nodes) labeled by $x_1$, $x_2$ and $x_3$, one root (the single sink node) labeled by $f(x_1,x_2,x_3)$ and two internal nodes, which are labeled as multiplication gates. 

The order we used to label the edges is choosen to make the edge labeling consistent with our decision from example XXX. Its not an order that arises from common ordering algorithms like deapth-first or breath-first ordering. In addition we declear the function output as the instance and the inputs as well as every step in the computation as private inputs.
\end{example}
\begin{example} To give a more realistic example of an algebraic circuit look at the defining equation XXX of the tiny-jubjub curve again. A pair of field elements 
$(x,y)\in \F_{13}^2$ is a curve point, precisely if
$$
3\cdot x^2 + y^2 = 1+ 8\cdot x^2\cdot y^2
$$  
We can rewrite this equation by shifting all terms to the right and get
\begin{align*}
3\cdot x^2 + y^2 & = 1+ 8\cdot x^2\cdot y^2 & \Leftrightarrow\\
0 & = 1+ 8\cdot x^2\cdot y^2 - 3\cdot x^2 - y^2 & \Leftrightarrow\\
0 & = 1+ 8\cdot x^2\cdot y^2 + 10\cdot x^2 +12 y^2
\end{align*}
And we can use this expression to define a function, such that all points of tiny-jubjub are characterized its the preimages of $0$:
$$
f_{tiny-jj}: \F_{13}\times \F_{13}\to \F_{13}\; ; \;
(x,y)\mapsto 1+ 8\cdot x^2\cdot y^2 + 10\cdot x^2 +12 y^2
$$
then of course every pair $(x,y)\in \F_{13}^2$ with $f_{tiny-jj}(x,y)=0$ is a point on the tiny-jubjub curve.

We can flatten this function into an algebraic circuit over $\F_{13}$. To do so we have to decide which edges to declare instance and which to declare witness. In principle we could choose any combination, but for the sake of zero-knowledge proofs as we will discuss later, we only declare the output and the pair $(x,y)$ to be the instance. Everything else should be part of the witness. A proper circuit then might be:
\begingroup
    \fontsize{8pt}{10pt}\selectfont
\[
\xymatrix{
 & &  & &
x\ar^{I_1}@/^/[d]\ar_{I_1}@/_/[d]  &  & & 
y\ar^{I_2}@/^/[d]\ar_{I_2}@/_/[d]\\
 & 10 \ar[dr]& & &
\cdot \ar^{W_1}[drr] \ar_{W_1}[dll]&  & &
\cdot \ar_{W_2}[dl] \ar^{W_2}[ddr] &  &
12 \ar[ddl] \\
1 \ar[dr] & &  
 \cdot \ar[dl] & & 
 8 \ar[dr] & &
 \cdot \ar_{W_3}[dl] & & 
 \\
 & 
 + \ar[drr] & & & &
 \cdot \ar[dll] & & &
 \cdot \ar[ddlll]\\
 & & & 
 + \ar[drr]& & & 
 \\
 & & & & & 
 + \ar_{I_3}[d]\\
 & & & & & 
 f_{tiny-jj}(x,y)
}
\]
\endgroup
The ordering we applied to index instance and witness variable is the pre-order. 

Of course this is just one proper circuit from an infinite set of possible circuit. To see this oberseve that we can always add some constant and then later subtract the same constant again. It follows that circuit representations are not unique, not even up to ordering of the edges and nodes in the graph.
\end{example}
Algebraic circuits are usually derived by compilers, that transform  higher languages to circuits. An example of such a compiler is XXX. Note: Different Compiler give very different circuit representations and Compiler optimization is important. We will see in XXX how such compilers might be costructed by deriving building blocks for major imperative primitives.
\paragraph{Circuit Execution} Algebraic circuits are directed, acyclic multi-graphs, where nodes are decorated with variables and constants as well as addition and multiplication symbols. In particular every algebraic circuit with $n$ input nodes decorated with variable symbols and $m$ output nodes can be seen a function that transforms an input tuple $(x_1,\ldots, x_n)$ from $\F^n$ into an output tupel $(f_1,\ldots,f_m)$ $m$ from $\F^m$. The transformation is done by sending values associated to a node along the outgoing edges to other nodes. If those nodes are gates, then the values are transformed according to the label. This is repeatedly done for all edges until a sink node is reached.

Executing a circuit this way, it is possible to not only to compute the output values of the circuit but field elements for all edge labels of the circuit. The result is a tupel $(I_1,\ldots,I_n; W_1,\ldots,W_m)$ of field elements, that is called a \textbf{valid asignment} of the circuit. 

Valid asignments of circuit execution can be seen as a kind of record, that not just hold the output of a computation but intermediate steps as well. What distinguishes a \textit{valid} assigment to edge labels from any other assignment is the fact that it is the result of a circuit eecution. An Assignment is valid, if the field element arise from executing the circuit, and every other assignment is invalid. Valid assignments are therefore \textit{proofs for proper circuit execution}.

So to summarize, algebraic circuits (over a field $\mathbb{F}$) are directed acyclic graphs, that express arbitrary, but bounded computation. Vertices with only outgoing edges (leafs, sources) represent inputs to the computation, vertices with only ingoing edges (roots, sinks) represent outputs from the computation and internal vertices represent field operations (Either addition or multiplication). It should be noted however that there are many circuits that can represent the same laguage.
\begin{example}[3-factorization] Consider the $3$-factorization problem from example XXX and its representation as an algebraic circuit from XXX again. We know that the set of edge labels is given by $S:=\{I_{1},W_{1},W_{2},W_{3}, W_{4}\}$. To see how circuit execution looks in this example, lets consider the inputs $x_1=2$, $x_2=3$ as well as $x_3=4$. So we instantiated the variable labels $x_1$, $x_2$ and $x_3$ with actual values from the prime field $\F_{13}$. Executing the circuit then means to follow the edges and assign values to all edge labels. 

We immediately get $W_1=2$, $W_2=3$ and $W_3=4$. Then the values $W_1$ and $W_2$ enter a multiplication gate and the output of the gate is $2\cdot 3 = 6$, which we assign to $W_4$, i.e. $W_4=6$. The values $W_4$ and $W_3$ then enter the second multiplication gate and the output of the gate is $6\cdot 4 = 11$, which we assign to $I_1$, i.e. $I_1=11$. 

Summarizing this computation a valid assignment to our circuit $C_{3.fac}(\F_{13})$ is therefore the set $S_{valid}:=\{2,3,4,6,10\}$, with an assigned circuit given by
\[
\xymatrix{x_1\ar^{W_1=2}[dr] &  & x_2\ar_{W_2=3}[dl]\\
 & \cdot \ar^{W_4=6}[drr] &   & & x_3\ar_{W_3=4}[dl]\\
  &  &  & \cdot\ar_{I_1=11}[d]\\
  &  &  & f(x_1,x_2,x_3)
}
\]
To see how an invalid assignment looks like, consider the assignment $S_{err}:=\{2,3,4,7,8\}$. In this assignment the input values are the same as in the previous case. The associated circuit is:
\[
\xymatrix{x_1\ar^{W_1=2}[dr] &  & x_2\ar_{W_2=3}[dl]\\
 & \cdot \ar^{W_4=7}[drr] &   & & x_3\ar_{W_3=4}[dl]\\
  &  &  & \cdot\ar_{I_1=8}[d]\\
  &  &  & f(x_1,x_2,x_3)
}
\]
So this assignment is invalid as the assignments of $I_1$ and $W_4$ can not be obtained by executing the circuit.
\end{example}
\begin{example} To give a more realistic example of algebraic circuit execution and assignment, consider the defining circuit $C_{tiny-jj}(\F_{13})$ from example XXX again. We already know from the way this circuit is constructed that any valid assignment with $I_1=x$, $I_2=y$ and $I_3=0$ will ensure that the pair $(x,y)$ is a point on the tiny jubjub curve XXX in its Edwards representation. 

From example XXX we know that the pair $(11,6)$ is a point on the tiny-jubjub Edwards curve so we execute the circuit with $I_1=11$ and $I_2=6$. Executing the circuit we get:
\begingroup
    \fontsize{8pt}{10pt}\selectfont
\[
\xymatrix{
 & &  & &
x\ar^{I_1=11}@/^/[d]\ar_{I_1=11}@/_/[d]  &  & & 
y\ar^{I_2=6}@/^/[d]\ar_{I_3=6}@/_/[d]\\
 & 10 \ar[dr]& & &
\cdot \ar^{W_1=4}[drr] \ar_{W_1=4}[dll]&  & &
\cdot \ar_{W_2=10}[dl] \ar^{W_2=10}[ddr] &  &
12 \ar[ddl] \\
1 \ar[dr] & &  
 \cdot \ar^{[10\cdot 4 = 1]}[dl] & & 
 8 \ar[dr] & &
 \cdot \ar_{W_3=1}[dl] & & 
 \\
 & 
 + \ar^{[1+1=2]}[drr] & & & &
 \cdot \ar^{[8\cdot 1 = 1]}[dll] & & &
 \cdot \ar^{[10\cdot 12=3]}[ddlll]\\
 & & & 
 + \ar^{[2+8=10]}[drr]& & & 
 \\
 & & & & & 
 + \ar_{I_3=0}[d]\\
 & & & & & 
 f_{tiny-jj}(x,y)
}
\]
\endgroup
Executing the circuit we indeed compute $I_3=0$ as expected, which proofs that $(11,6)$ is a point on the tiny-jubjub curve in its Edwards representation. A valid assignment of $C_{tiny-jj}(\F_{13})$ is therefore given by 
$$
S_{tiny-jj} = \{I_1, I_2, I_3; W_1, W_2, W_3\} = \{11, 6, 0; 4, 10, 1\}
$$
\end{example}

\paragraph{Circuit Satisfyability} To understand how algebraic circuits give rise to formal languages, oberserve that every given algebraic circuit $C_{\F}$ over a fields $\F$ defines a checking relations over the alphabet $\Sigma_I \times \Sigma_W = \F \times \F$ in the following way:
\begin{equation}
R_{R1CS} : \F^* \times \F^* \to \{TRUE, FALSE\}\;;\;
(I;W) \mapsto
\begin{cases}
TRUE & (I;W) \text{ is a valid assignment}\\
FALSE & else
\end{cases}
\end{equation}
We write $L_{Circuit-SAT}$ for the associated language and call it \textbf{algebraic circuit satisfyability}. The grammar of this language is exapressed by the structure of the circuit and words in this language are valid circuit assignments. 

Now since every algebraic circuit gives rise to a formal language, a \textbf{statement} for a given circuit, is a knowledge claim "Given instance $I$, there is a witness $W$, such that $(I;W)$ can be optained by executing the circuit". One way to proof such a claim is therefore to actually execute the circuit. A proof for proper execution of the computation. In the context of zero knowledge proofing systems, executing circuits is also often called \textbf{witness generation}, since in prectise the instance part is usually public, while its the task of a proofer to compute the witness part.
\begin{example}[3-Factorization]Consider the circuit $C_{3.fac}$ from example XXX again. We call the associated language $L_{3.fac\_circ}$.

To give an intuition of how a naive proof of a statement in $L_{3.fac\_circ}$ looks like, consider the instance $I_1= 11$. To provide a proof for the statement "There exist a witness $W$, such that $(I_1;W)$ is a word in $L_{3.fac\_circ}$" a proofer therefore has to provide proper values for the variables $W_1$, $W_2$, $W_3$ and $W_4$. Those values can be computed by circuit execution and we therefore know from example that $(2,3,4,6)$ is a proper proof.
\end{example}
\paragraph{Circuit to R1CS compilers} As we have seen in XXX, R1CS provide a compact way to represent statements in terms of a system of quadratic equations over finite fields. As we will see in XXX they are particulary useful in the context of pairing based zero knowledge proofing systems. However R1CS provide no practical way for proofer to actually \textit{compute} a proof. They are just a system of \textit{checking} any given proof against a set of constraints.

On the other hand algebraic circuits have a notation of execution and executing any such circuit provides a way to compute valid assignments and proof for statements in associated languages. 

The question therefore remains, if we can combine the both approaches to have a representation, suteable for pairing based zero knowledge proofing systems that can also be used to compute proofs efficiently.

Fortunately it is straight forward to transform any algebraic circuit into a rank-1 constraint systems. To see how this is done, let $C(\F)$ be an algebraic circuit over a finite field $\F$, with edge labeling $(I_1,\ldots,I_n;W_1,\ldots,W_m)$. The task is to compute a R1CS $S$ over $\F$ from $C(\F)$.  

Every multiplication gate in $C(\F)$ that has a label on its outgoing edges, defines a new quadratic constraint in $S$. If $L_j\in\{I_j,W_j\}$ is the label of the outgoing edge, then the constraint is given by
\begin{equation}
(\text{left input})\cdot (\text{right input}) = L_j
\end{equation}  
where $(\text{left input})$ respectively $(\text{right input})$ is the output from the execution of the subgraph that consists of the left respectively right input edge of this gate and all edges and nodes that have this edge in their path, starting with constant inputs or labeled outgoing edges of other gates.

Every addition gate that has a label on its output, defines a new quadratic constraint in $S$. If $L_j\in\{I_j,W_j\}$ is the label of the outgoing edge, then the constraint is given by
\begin{equation}
(\text{left input} + \text{right input})\cdot 1 = L_j
\end{equation}  
where $(\text{left input})$ respctively $(\text{right input})$ is the computation of all previous constant inputs or labels that result in the left respectively right input to this addition gate.  

It can be shown, that a tupel $(I_1,\ldots, I_n;W_1,\ldots,W_m)$ is a solution to the synthesized R1CS if and only if the same tupel is a valid assignment to the associated circuit.

As addition gates only add constraints if their result is an output, addition is \textit{essentially for free} in R1CS.
 \begin{example}[$3$-factorization] Consider our $3$-factorization problem from example XXX and the associated circuit $C_{3.fac}(\F_{13})$ again:
 
\[
\xymatrix{x_1\ar^{W_1}[dr] &  & x_2\ar_{W_2}[dl]\\
 & \cdot \ar^{W_4}[drr] &   & & x_3\ar_{W_3}[dl]\\
  &  &  & \cdot\ar_{I_1}[d]\\
  &  &  & f(x_1,x_2,x_3)
}
\]
Out task is to transform this circuit into an equivalent rank-1 constraint system. Since the circuit consists of two multiplication gates that have labels on their outgoing edges, the resulting R1CS will consist of two quadratic constraints. 

To find those constraints, we begin at the up-left gate. Since its outgoing edge is labeled as $W_4$ and all imcoming edges have labels themself we get the following constraint
\begin{align*}
(\text{left input})\cdot (\text{right input})  &= L_j & \Leftrightarrow\\
W_1\cdot W_2  &= W_4
\end{align*}
Then we consider the second multiplication gate. Since its outgoing edge is labeled as $I_1$ and all imcoming edges have labels themself we get the following constraint
\begin{align*}
(\text{left input})\cdot (\text{right input})  &= L_j & \Leftrightarrow\\
W_4\cdot W_3  &= I_1
\end{align*}
Since there are no more gates with labeled outgoing adges, we are done deriving the constraints. Combining all constraints together, we get the associated R1CS as
\begin{align*}
 W_1\cdot W_2 & = W_4\\
 W_4\cdot W_3 & = I_1
\end{align*}
which is equivalent to the R1CS we derived in example XXX. The languages $L_{3.fac\_zk}$ and $L_{3.fac\_circ}$ are therefore equivalent and both the circuit as well as the R1CS are just two different ways to express the same thing.
\end{example}
\begin{example} To consider a more general transformation, lets consider the tiny-jubjub circuit from example XXX  again. A proper circuit is given by
\begingroup
    \fontsize{8pt}{10pt}\selectfont
\[
\xymatrix{
 & &  & &
x\ar^{I_1}@/^/[d]\ar_{I_1}@/_/[d]  &  & & 
y\ar^{I_2}@/^/[d]\ar_{I_2}@/_/[d]\\
 & 10 \ar[dr]& & &
\cdot \ar^{W_1}[ddr] \ar_{W_1}[dll]&  
8 \ar[dr]& &
\cdot \ar_{W_2}[dl] \ar^{W_2}[ddr] &  &
12 \ar[ddl] \\
1 \ar[dr] & &  
 \cdot \ar[dl] & & & &
 \cdot \ar[dl] & & 
 \\
 & 
 + \ar[drr] & & & &
 \cdot \ar_{W_3}[dll] & & &
 \cdot \ar[ddlll]\\
 & & & 
 + \ar[drr]& & & 
 \\
 & & & & & 
 + \ar_{I_3}[d]\\
 & & & & & 
 f_{tiny-jj}(x,y)
}
\]
\endgroup
To compute the number of constraints, observe that we have $3$ multiplication gates, that have labeles on their outgoing edges and $1$ addition gate that has a label on its outgoing edge. We therefore have to cmpute $4$ quadratic constraints.
Looking at the multiplication gate
\begingroup
    \fontsize{8pt}{10pt}\selectfont
\[
\xymatrix{
 & &  & &
x\ar^{I_1}@/^/[d]\ar_{I_1}@/_/[d]  &  & & 
\\
 & & & &
\cdot \ar^{W_1}[ddr] \ar_{W_1}[dll]&  
& &
 &  &
 \\
  & &  
 \cdot & & & &
 & & 
 \\
 & 
  & & & &
 \cdot & & & 
 \\
}
\]
\endgroup
we see that both the left input to this gate as well as the right input to this gate are given by the label $I_1$. Since the label of the outgoing adges is $W_1$, the associated constraint is given by
$$
I_1 \cdot I_1 = W_1
$$
We can look at the subgraph of the second multiplication gate in a similar way. We get 
\begingroup
    \fontsize{8pt}{10pt}\selectfont
\[
\xymatrix{
 & &  & &
 &  & & 
y\ar^{I_2}@/^/[d]\ar_{I_2}@/_/[d]\\
 & & & &
&  
& &
\cdot \ar_{W_2}[dl] \ar^{W_2}[ddr] &  &
\\
& &  
 & & & &
 \cdot & & 
 \\
 & 
 & & & &
 & & &
 \cdot\\
}
\]
\endgroup
We see that both the left input to this gate as well as the right input to this gate are given by the label $I_2$. Since the label of the outgoing adges is $W_2$, the associated constraint is given by
$$
I_2 \cdot I_2 = W_2
$$
The next multiplication gate that has a label on its outgoing edge is more interesting. To compute the associated constraint, we have to construct the associated subgraph first. The subgraph consists of all edges and all nodes in all path starting either at a constant input or a labeled edge. We get  
\begingroup
    \fontsize{8pt}{10pt}\selectfont
\[
\xymatrix{
 & & & &
\cdot \ar^{W_1}[ddr] &  
8 \ar[dr]& &
\cdot \ar_{W_2}[dl] &  &
\\
& &  
& & & &
 \cdot \ar[dl] & & 
 \\
 & 
  & & & &
 \cdot \ar_{W_3}[dll] & & &
 \\
 & & & 
 + & & & 
 \\
}
\]
\endgroup
The left input to is given by the labeled edge $W_1$. However the right input is not a labeled edge. To comput the right factor in the quadratic constraint, we therefore have to compute the output of the subgraph associated to the right edge, which is $8\cdot W_2$. Combining this we get
$$
W_1\cdot 8\cdot W_2 = W_3
$$ 
To compute the $4$th constraint, observe the associated gate is an addition gate.  To compute the associated constraint, we have to construct the associated subgraph. The subgraph consists of all edges and all nodes in all path starting either at a constant input or a labeled edge. We get
\begingroup
    \fontsize{8pt}{10pt}\selectfont
\[
\xymatrix{
 & 10 \ar[dr]& & &
\cdot \ar_{W_1}[dll]&  
& &
\cdot \ar^{W_2}[ddr] &  &
12 \ar[ddl] \\
1 \ar[dr] & &  
 \cdot \ar[dl] & & & &
 & & 
 \\
 & 
 + \ar[drr] & & & &
 \cdot \ar_{W_3}[dll] & & &
 \cdot \ar[ddlll]\\
 & & & 
 + \ar[drr]& & & 
 \\
 & & & & & 
 + \ar_{I_3}[d]\\
 & & & & & 
 f_{tiny-jj}(x,y)
}
\]
\endgroup
Since the gate is an addition gate, the right factor in the quadratic constraint is always $1$ and the left factor is computed by symbolically executing all inputs to all gates in subcircuit. We get
$$
(1 + 10\cdot W_1 + W_3 + W_2\cdot 12)\cdot 1 = I_3
$$
Since there are no more gates with labeled outgoing adges, we are done deriving the constraints. Combining all constraints together, we get the associated R1CS as
\begin{align*}
 I_1 \cdot I_1 &= W_1\\
 I_2 \cdot I_2 &= W_2\\
 8\cdot W_1\cdot W_2 &= W_3\\
 (1 + 10\cdot W_1 + W_3 + 12\cdot W_2)\cdot 1 &= I_3
\end{align*}
which is equivalent to the R1CS we derived in example XXX. The languages $L_{3.fac\_zk}$ and $L_{3.fac\_circ}$ are therefore equivalent and both the circuit as well as the R1CS are just two different ways to express the same thing.
\end{example}
\section{Gadgets} It is possible to verify the the correct execution of arbitrary bounded computation in terms of rank-$1$ contraints systems and it is possible to encode such computations in terms of algebraic circuits. However writing actual computer programs as circuits and the associated verification in terms of rank-1 constraint systems is as least as troublesome as writing any other low level language like assembler code. 

From a practical point of view it is therefore necessary to have some kind of compiler framework at hand, capable to transform some high level language like C or Rust into arithmetic circuits or rank-1 constraint systems.

As we have seen in XXX as well as XXX and XXX, both arithmetic circuits and rank-1 constraint systems have a modularity property by which it is possible to synthezise  complex circuits and constraint systems from simpler ones. A basic approach taken by circuit/R1CS compilers like (LIST) is therefore to provide a library of atomic and simple circuits and then define a way to combine them into arbitrary complex systems. 

In this section we will therefore provide an introduction to atomic types like booleans and uInts and define the fundamental control flow primitives like the ternery operator or the bounded loop. We will also look at basic functionality primitives like elliptic curve cryptography and cryptographic hash functions. Primitives like those are often called \textbf{gadgets} in the literature.
 
\subsection{Atomic Types} Since both algebraic circuits and their associated rank-1 constraint systems are defined over finite fields, the natural underlying informational units are elements from those fields. In a sense field elements $x\in \F$ are for algebraic circuits what bits are for NORMAL computers. However most computer programs are optimized for machine words, which are arrays of bits. To compile computer programs into R1CS it is therefore often necessary to simulate atomic types like booleans and uInts inside algebraic circuits.
\subsubsection{The Basefield type} The most basic type of an algebraic circuit or its associated R1CS is the type $\F$ of the field that underlays the circuit. In a sense elements $x\in\F$ are for algebraic circuits what binary machine words are for computers. They are the fundamental computational units that everything else needs to be expressed in. 

As this type is the basis for everything else, no circuit or constraint is needed to represent this type. The properties of this type are inherited from $\F$, that is we a notion of \textit{addition}, \textit{subtraction}, \textit{multiplication} and \textit{division}. 

\subsubsection{The Boolean Type} 
% implementations can be found here: https://github.com/filecoin-project/zexe/tree/master/snark-gadgets/src/bits

It is often necessary to assume that a statement contains expressions of boolean variables. However by definition the alphabet of a statement is a finite field, which is usually the scalar field of a large prime order cyclic group. So developers need a way to simulate boolean algebra inside finite fields.

The most common way to do this in algebraic circuits and their associated rank-1 constraint systems, is to interpret the additive and multiplicate neutral element $\{0,1\}\subset \F$ as boolean values, such that $0$ represents $false$ and $1$ represents $true$. Boolean functions like $and$, $or$, $xor$ and so on are the expressable in terms of computations inside $\F$. 

Representing booleans this way is convinient because the elements $0$ and $1$ are defined in any field. The representation is therefore independent of the actual field in consideration. 
\paragraph{The Boolean Constraint System}
If boolean variables appear as part of a statement, a constraint is required to actually enforces the variable to be either $1$ or $0$. In fact many of the following constraints for boolean functions, are only correct under the assumption that their input variables are boolean constraint. Not constraining boolean variables is a common issue in circuit design.

In order to constrain an arbitrary field element $x\in \F$ to be $1$ or $0$, the key observatio is that the equation $x \cdot (1-x) =0$ has only two solutions $0$ and $1$. To enfore a field element to be boolean constraint the following R1CS is therefore enough
\begin{equation}
W_1 \cdot (1-W_1) =0
\end{equation}
\paragraph{The AND operator constraint system} Given three field elements $x,y,z\in\F$ that are constrained to represent boolean variables, we want to find a circuit that computes the AND operator and an associated R1CS, such that $x$, $y$, $z$ satisfies the constraint system if and only if $x\; AND \; y =z$. 

Assuming that $x$, $y$ and $z$ are boolean constraint, that is their values are constraint to be $0$ or $1$, only the equation $x\cdot y = z$ is satisfied in $\F$ if and only if the equation $x\text{ AND }y = z$ is satisfied in boolean algebra. The logical operator AND is therefore implementable in $\F$ as multiplication. This implies that the following circuit computes the AND operator in $\F$, assuming all inputs are restricted to be $0$ or $1$:
\[
\xymatrix{x\ar^{W_1}[dr] &  & y\ar_{W_2}[dl]\\
 & \cdot\ar_{W_3}[d]\\
  & x \text{ AND } y
}
\]
The associated rank-1 constraint system can be deduced from the general process XXX and consists of the following constraint
\begin{equation}
 W_1 \cdot W_2 = W_3
\end{equation}
\paragraph{NOT constraint}
Given two field elements $x,y\in\F$ that represent boolean variables, we want to find a R1CS, such that $w=(1,x,y)$ satisfies the constraint system if and only if $x=\lnot y$. 

So again we have to constrain $x$ and $y$ to be boolean as explained in XXX. The next think is we need to find a R1CS that enforces the $NOT$ logic. We can simply choose $(1-x) =y$, since (for boolean constraint values) this enforces that $y$ is always the boolean opposite of $x$. 

Now that we have found a correct equation for a boolean constrain, we have to translate it into the associated R1CS format, which is given by 
$$
\begin{pmatrix}1 & -1 & 0 \end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \end{pmatrix}\odot
\begin{pmatrix}1 & 0 & 0 \end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \end{pmatrix} =
\begin{pmatrix}0 & 0 & 1 \end{pmatrix}\begin{pmatrix} 1 \\ x \\ y \end{pmatrix}
$$
So actually we wrote the linear equation $1-x=y$ like $(1-x)\cdot 1 = y$ and translated that into the matrix equation.

Combining this R1CS with the required fthree boolean constraints for $x$, $y$ and $z$ we get
$$
\begin{pmatrix}
1 & -1 & 0 \\
\hline
0 & 1 & 0 \\
0 & 0 & 1
\end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \end{pmatrix}\odot
\begin{pmatrix}
1 & 0 & 0 \\
\hline
1 & -1 & 0 \\
1 & 0  & -1 
\end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \end{pmatrix} =
\begin{pmatrix}
0 & 0 & 1 \\
\hline
0 & 0 & 0 \\
0 & 0 & 0 \\
\end{pmatrix}\begin{pmatrix} 1 \\ x \\ y \end{pmatrix}
$$
So from the way this R1CS is constructed, we know that whatever the underlying field $\F$ is, the only solutions to this equations are
$$
\{(0,1), (1,0)\}
$$
which is the set of all $(x,y)\in\{0,1\}^2$ such that $x=\lnot y$.

EXERCISE: DO OR; XOR; NAND

More complicated logical constraints can then be optained by combining all sub-R1CS together. For example if the task is to enforce $(in_1\; AND \lnot in_2 ) AND in_3 = out_1$ we first apply the FLATTENING technique from XXX, which gives is
$$
\begin{array}{lcr}
\lnot in_2 &=& mid_1\\
in_1\; AND \; mid_1 &=& mid_2\\
mid_2 \; AND \; in_3 &=& out_1
\end{array}
$$
So we have the statement $w=(1,in_1,in_2,in_3, mid_1, mid_2,out_1)$, $6$ boolean constraints for the variables, $2$ constraints for the $2$ $AND$ operations and $1$ constraint for the $NOT$ operation.

\subsubsection{Binary representations}
In circuit computations its is often necessary to use the binary representation of a prime field element. Binary representations of prime field elements work execactly like binary representations of ordinary unsigned integers. Only the algebraic operations are different. To compute the binary representation of some number $x\in \F_p$ we need to know the number of bits in the binary representation of $p$ first. We write this as $m= |p_{bin}|$. 

Then a bitstring $(b_0,\ldots,b_m)\in \{0,1\}^m$ is the binary representation of the field element $x$, if and only if
$$
x = b_0\cdot 2^0 + b_1\cdot 2^1 + \ldots + b_m\cdot 2^m
$$ 
Note that, since $p$ is a prime number that has a leading bit $1$ at position $m$. Moreover every prime number $p>2$ is odd and hence has least significat bit set to $1$. Hence all numbers $2^j$ for $0\leq j \leq m$ are elements of $\F_p$ and the equation is well defined. We can therefore enforce this equation as a R1CS, by flattening the equation: 
$$
\begin{array}{lcl}
b_0 \cdot 1 &=& mid_0\\
b_1 \cdot 2 &=& mid_1\\
\cdots & = & \cdots \\
b_m \cdot 2^m &=& mid_m\\
(mid_0 + mid_1 + \ldots + mid_m)\cdot 1 &=& x
\end{array}
$$
So we have the statement $w = (1, x, b_0,\ldots,b_m, mid_0,\ldots,mid_m)$ and we need $(m+1)$ constraints to enforce the binary representation in addition to the $m$ constraints that enforce booleanness.

At this point we see, that writing more complex R1CS becomes clumbsy and in actual implementations people therefore use languages to makes the constraint system more readable. In this example we could write for example something like this:

%\begin{algorithmic}
%\Require $m$ Bitlength of modulus
%\Statement $w \gets [x,b[m],mid[m]]$
%\State $tmp \gets 0$
%\For{$j\gets 1,\ldots, m$}
%	\State \textbf{Constrain:} $b[j]\cdot (1-b[j]) == 0$
%	\State \textbf{Constrain:} $b[j] \cdot 2^j == mid[j]$
%	\State $tmp = tmp + mid[j]$
%\EndFor
%\State \textbf{Constrain:} $tmp \cdot 1 == x$
%\end{algorithmic}

keeping in mind that this is a meta level algorith to \textbf{generate} the R1CS, not the R1CS itself, as constructs like for loops have not direct meening on the level of the R1CS itself.


\begin{example} Considering the prime field $\F_{13}$, we want to enforce the binary representation of $7\in \F_{13}$. To find the number of bits that we need to consider in our R1Cs, we start with the binary representation of $13$, which is $(1,0,1,1)$ since 
$13= 1\cdot 2^0 + 0\cdot 2^1 + 1\cdot 2^2 + 1\cdot 2^3$. So $m=4$ and we have to enforce a $4$-bit representation for $7$, which is $(1,1,1,0)$, since $7= 1\cdot 2^0 + 1\cdot 2^1 + 1\cdot 2^2 + 0\cdot 2^3$.

A valid statement is then given by $w=(1,7,1,1,1,0,1,2,4,0)$ and indeed we satify the $9$ required constraints 
$$
\begin{array}{lclr}
1\cdot (1-1) &=& 0 & \text{// boolean constraints}\\
1\cdot (1-1) &=& 0 &\\
1\cdot (1-1) &=& 0 &\\
0\cdot (1-0) &=& 0  &\\
\\
1 \cdot 1 &=& 1\\
1 \cdot 2 &=& 2\\
1 \cdot 4 &=& 4\\
0 \cdot 8 &=& 0\\
(1 + 2 + 4 + 0)\cdot 1 &=& 7
\end{array}
$$
\end{example}

\subsubsection{Conditional (ternary) operator}
It is often required to implement the terniary conditional operator $?:$ as a R1CS. In general this operator takes three arguments, a boolean value $b$ and two expressions $if\_true$ and $if\_false$, usually written as $b\;?\; c\; :\; d$ and executes $c$ and $d$ according to the value of $b$.

If we assume all three arguments to be values from a finite field, such that $b$ is boolean constraint (XXX), we can enforce a field element $x$ to be the result of the conditional operator as 
$$
x = b\cdot c + (1-b)\cdot d
$$
Flattening the code gives
$$
\begin{array}{lcl}
b \cdot c &=& mid_0\\
(1-b) \cdot d &=& mid_1\\
(mid_0 + mid_1)\cdot 1 &=& x
\end{array}
$$
So we have the statement $w = (1, x, b,c,d, mid_0,mid_1)$ and we need $3$ constraints to enforce the conditional operator in addition to $1$ constraint that enforces booleanness of $b$.


NOTE: THERE WAS THIS PODCAST WITH ANNA AND THE GUY JAN TALKE TO WHERE HE SAID; CONDITIONALS CAN BE IMPLEMENTED SUCH THAT NOT BOTH BRANCHES ARE EXCUTED: LOOK THAT UP

\subsubsection{Range Proofs}
$x>5$...

\subsubsection{UintN}
STUFF ABOUT HOW UINTN COMPUTATIONS ARE NOT STANDARDIZED AND THAT THERE ARE IMPLEMENTATIONS OTHER THEN MOD-N.... WE FIX ON MOD-N. WHAT DO ZEXE CIRCOM ECT FIX ON?

As we know circuits are not defined over integers but over finite fields instead. We therefore have no notation of integers in circuits. However on computers we also not use integers natively but Uint's instead.

As we know a UintN type is a representation of integers in the range of $0 \ldots 2^N$ with the exception that algebraic operations like addition and multiplication deviate from actual integers, whenever the result exceeds the largest representable number $2^N-1$. 

In circuit design it is therefore important to distinguish between various things tht might look like integers, but are actually not. For example Haskells type NAT is an actual implementation of natural numbers. In particular this means ....

\begin{example}[Uint8]
What is $0xFFF0 + 0xFFF0$ and so on...
\end{example}

\paragraph{Bit constraints}
In prime fields, addition and multiplication behaves exactly like addition and multiplication with integers as long as the result does not exceed the modulus. 

This makes the representation of UintNs in a prime field $\F_{p}$ potentially ambigious, as there are two possible representations, whenever $2^N-1 < p$. In that case any element of $UintN$ could be interpreted as an element of $\F_{p}$. This however is dangerous as the algebraic laws like addition and multiplication behave very different in general.  

It is therefore common to represent UintN types in circuits as binary constraints strings of field elements of length $N$.

\begin{example}
Consider the Uint4 type over the prime field $\F_{17}$. Since $2^4=16$, Uint4 can represent the numbers $0,\ldots, 15$ and it would be possible to interpret them as elements in $\F_{17}$. However addition 
\end{example} 



\subsubsection{Twisted Edwards curves}
Sometimes it required to do elliptic curve cryptography "inside of a circuit". This means that we have to implement the algebraic operations (addition, scalar multiplication) of an elliptic curve as a R1CS. To do this efficiently the curve that we want to implement must be defined over the same base field as the field that is used in the R1CS. 

% implmentations https://github.com/iden3/circomlib/blob/master/circuits/babyjub.circom

\begin{example}
So for example when we consider an R1CS over the field $\F_{13}$ as we did in example XXX, then we need a curve that is also defined over $\F_{13}$. Moreover it is advantegous to use a (twisted) Edwards curve inside a circuit, as the addition law contains no branching (See XXX). As we have seen in XXX our Baby-Jubjub curve is an Edwards curve defined over $\F_{13}$. So it is well suited for elliptic curve cryptography in our pend and paper examples
\end{example}

\paragraph{Twisted Edwards curves constraints} As we have seen in XXX, an Edwards curve over a finite field $F$ is the set of all pairs of points $(x,y)\in \F\times \F$, such that $x$ and $y$ satisfy the equation $a\cdot x^2+y^2= 1+d\cdot x^2y^2$. 

We can interpret this equation as a constraint on $x$ and $y$ and rewrite it as a R1CS by applying the flattenin technique from XXX.
$$
\begin{array}{lcr}
x \cdot x &=& x\_sq\\
y \cdot y &=& y\_sq\\
x\_sq \cdot y\_sq &=& xy\_sq\\
(a\cdot x\_sq+y\_sq)\cdot 1 &=& 1+d\cdot xy\_sq
\end{array}
$$
So we have the statement $w=(1,x,y,x\_sq, y\_sq, xy\_sq)$ and we need 4 constraints to enforce that $x$ and $y$ are points on the Edwards curve $x^2+y^2= 1+d\cdot x^2y^2$. Writing the constraint system in matrix form, we get:
\begingroup
    \fontsize{9pt}{9pt}\selectfont
$$
\begin{pmatrix}
0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & a & 1 & 0 
\end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \\ x\_sq \\ y\_sq \\ xy\_sq \end{pmatrix}\odot
\begin{pmatrix}
0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
1 & 0 & 0 & 0 & 0 & 0 
\end{pmatrix}  \begin{pmatrix} 1 \\ x \\ y \\ x\_sq \\ y\_sq \\ xy\_sq \end{pmatrix} =
\begin{pmatrix}
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 & d 
\end{pmatrix} \begin{pmatrix} 1 \\ x \\ y \\ x\_sq \\ y\_sq \\ xy\_sq \end{pmatrix}
$$
\endgroup
EXERCISE: WRITE THE R1CS FOR WEIERSTRASS CURVE POINTS 
\begin{example}[Baby-JubJub]
Considering our pen and paper Baby JubJub curve over from XXX, we know that the curve is defined over $\F_{13}$ and that $(11,9)$ is a curve point, while $(2,3)$ is not a curve point. 

Starting with $(11,9)$, we can compute the statement $w=(1,11,9,4,3,12)$. Substituting this into the constraints we get
$$
\begin{array}{lcr}
11 \cdot 11 &=& 4\\
9 \cdot 9 &=& 3\\
4 \cdot 3 &=& 12\\
(1\cdot 4+3)\cdot 1 &=& 1+7\cdot 12
\end{array}
$$
which is true in $\F_{13}$. So our statement is indeed a valid assignment to the twisted Edwards curve constraining system.

Now considering the non valid point $(2,3)$, we can still come up with some kind of statement $w$ that will satisfy some of the constraints. But fixing $x=2$ and $y=3$, we can never satisfy all constraints. For example $w=(1,2,3,4,9,10)$ will satisfy the first three constraints, but the last constrain can not be satisfied. Or $w=(1,2,3,4,3,12)$ will satisfy the first and the last constrain, but not the others.
\end{example}
\paragraph{Twisted Edwards curves addition} As we have seen in XXX one the major advantages of working with (twisted) Edwards curves is the existence of an addition law, that contains no branching and is valid for all curve points. Moreover the neutral element is not "at infinity" but the actual curve poin $(0,1)$.

As we know from XXX, give two points $(x_1,y_1)$ and $(x_2,y_2)$ on a twisted Edwards curve their sum is given by
$$
(x_3,y_3) = \left(\frac{x_1y_2+y_1x_2}{1+d\cdot x_1x_2y_1y_2}, \frac{y_1y_2-a\cdot x_1x_2}{1-d\cdot x_1x_2y_1y_2}\right)
$$
% https://z.cash/technology/jubjub/
We can realize this equation as a R1CS as follows: First not that we can rewrite the addition law as
$$
\begin{array}{lcl}
x_1\cdot x_2 &=& x_{12}\\
y_1\cdot y_2 &=& y_{12}\\
x_1\cdot y_2 &=& xy_{12}\\
y_1\cdot x_2 &=& yx_{12}\\
x_{12}\cdot y_{12} &=& xy_{1212}\\
x_3\cdot (1+d\cdot xy_{1212}) &=& xy_{12}+yx_{12}\\
y_3\cdot (1-d\cdot xy_{1212}) &=& y_{12}-a\cdot x_{12}
\end{array}
$$
So we have the statement $w=(1,x_1,y_1,x_2,y_2,x_3,y_3,x_{12},y_{12},xy_{12},yx_{12},xy_{1212})$ and we need 7 constraints to enforce that $(x_1,y_1)+(x_2,y_2)=(x_3,y_3)$ 
\begin{example}[Baby-JubJub]
Considering our pen and paper Baby JubJub curve over from XXX. We recall from XXX that $(11,9)$ is a generator for the large prime order subgroup. We therefor already know from XXX that
$(11,9) + (7,8) = (11,9) + [3](11,9) = [4](11,9) = (2,9)$. So we compute a valid statement as 
$w=(1,11,9,7,8,2,9,12,7,10,11,6)$. Indeed
$$
\begin{array}{lcl}
11\cdot 7 &=& 12\\
9\cdot 8 &=& 7\\
11\cdot 8 &=& 10\\
9\cdot 7 &=& 11\\
10\cdot 11 &=& 6\\
2\cdot (1+7\cdot 6) &=& 10 + 11\\
9\cdot (1-7\cdot 6) &=& 7 -1\cdot 12
\end{array}
$$
\end{example}
There are optimizations for this using only 6 constraints, available:
% https://github.com/filecoin-project/zexe/blob/master/snark-gadgets/src/groups/curves/twisted_edwards/mod.rs#L129

\paragraph{Twisted Edwards curves inversion} Similar to elliptic curves in Weierstrass form, inversion is cheap on Edwards curve as the negative of a curve point $-(x,y)$ is given by $(-x,y)$. So a curve point $(x_2,y_2)$ is the additive inverse of another curve point $(x_1,y_1)$ precisely if the equation $(x_1,y_1) = (-x_2,y_2)$ holds. We can write this as
$$
\begin{array}{lcl}
x_1 \cdot 1 &=& -x_2 \\
y_1 \cdot 1 &=& y_2
\end{array}
$$
We therefor have a statement of the form $w=(1,x_1,y_1,x_2,y_2)$ and can write the constraints into a matrix equation as
$$
\begin{pmatrix}
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0
\end{pmatrix} \begin{pmatrix} 1 \\ x_1 \\ y_1 \\ x_2 \\ y_2 \end{pmatrix}\odot
\begin{pmatrix}
1 & 0 & 0 & 0 & 0\\
1 & 0 & 0 & 0 & 0
\end{pmatrix} \begin{pmatrix} 1 \\ x_1 \\ y_1 \\ x_2 \\ y_2 \end{pmatrix} =
\begin{pmatrix}
0 & 0 & 0 & -1 & 0\\
0 & 0 & 0 & 0 & 1
\end{pmatrix} \begin{pmatrix} 1 \\ x_1 \\ y_1 \\ x_2 \\ y_2 \end{pmatrix}
$$

In addition we need the following constraints:
$$
\begin{array}{lcl}
x_1 \cdot 1 &=& -x_2 \\
y_1 \cdot 1 &=& y_2
\end{array}
$$

\paragraph{Twisted Edwards curves scalar multiplication} 
% original circuit is here https://iden3-docs.readthedocs.io/en/latest/_downloads/33717d75ab84e11313cc0d8a090b636f/Baby-Jubjub.pdf

Although there are highly optimzed R1CS implementations for scal multiplication on elliptic curves, the basic idea is somewhat simple: Given an elliptic curve $E/\F_r$, a scalar $x\in \F_r$ with binary representation $(b_0,\ldots,b_m)$ and a curve point $P\in E/\F_r$, the scalar multiplication $[x]P$ can be written as
$$
[x]P = [b_0]P + [b_1]([2]P) + [b_2]([4]P) + \ldots + [b_m]([2^m] P)
$$
and since $b_j$ is either $0$ or $1$, $[b_j](kP)$ is either the neutral element of the curve or $[2^j]P$. However $[2^j]P$ can be computed inductively by curve point doubling, since $[2^j]P= [2]([2^{j-1}]P)$.

So scalar multiplication can be reduced to a loop of length $m$, where the original curve point is repeadedly douled and added to the result, whenever the appropriate bit in the scalar is equal to one.

So to enforce that a curve point $(x_2,y_2)$ is the scalar product $[k](x_1,y_1)$ of a scalar $x\in F_r$ and a curve point $(x_1,y_1)$, we need an R1CS the defines point doubling on the curve (XXX) and an R1CS that enforces the binary representation of $x$ (XXX). 

In case of twisted Edwards curve, we can use ordinary addition for doubling, as the constraints works for both cases (doublin is addition, where both arguments are equal). Moreover $[b](x,y)=(b\cdot x, b\cdot y)$ for boolean $b$. Hence flattening equation XXX gives
$$
\begin{array}{lclr}
b_0\cdot x_1 &=& x_{0,1} & // [b_0]P\\
b_0\cdot y_1 &=& y_{0,1}\\

\end{array}
$$
In addition we need to constrain $(b_0,\ldots, b_N)$ to be the binary representation of $x$ and we need to constrain each $b_j$ to be boolean.

As we can see a R1CS for scalar multiplication utilizes many R1CS that we have introduced before. For efficiency and readability it is therefore useful to apply the concept of a gadget (XXX). A pseudocode method to derive the associated R1CS could look like this:

%\begin{algorithmic}
%\Require $m$ Bitlength of modulus
%\Statement $w \gets [x,b[m],mid[m]]$
%\State $tmp \gets 0$
%\For{$j\gets 1,\ldots, m$}
%	\State \textbf{Constrain:} $b[j]\cdot (1-b[j]) == 0$
%	\State \textbf{Constrain:} $b[j] \cdot 2^j == mid[j]$
%	\State $tmp = tmp + mid[j]$
%\EndFor
%\State \textbf{Constrain:} $tmp \cdot 1 == x$
%\end{algorithmic}

%\begin{codebox}
%\Procname{$\proc{Insertion-Sort}(A)$}
%\li \For $j \gets 2$ \To $\id{length}[A]$
%\li     \Do$\id{key} \gets A[j]$
%\li         \Comment Insert $A[j]$ into the sorted sequence $A[1 \twodots j-1]$.
%\li         $i \gets j-1$\li         \While $i > 0$ and $A[i] > \id{key}$
%\li             \Do$A[i+1] \gets A[i]$
%\li                 $i \gets i-1$\End
%\li         $A[i+1] \gets \id{key}$\End
%\end{codebox}

\paragraph{Curve Cycles} A particulary interesting case with far reaching implication is the situation when we have two curve $E_1$ and $E_2$, such that the scalar field of curve $E_1$ is the base field of curve $E_2$ and vice versa. In that case it is possible to implement the group laws of one curve in circuits defined over the scalar field of the other curve. 

\subsubsection{The RAM Model}
FROM THE PODCAST WITH ANNA R. AND THE GUY FROM JAN....


\subsubsection{Generalizations}
many circuits can be found here:
% https://github.com/iden3/circomlib

