\chapter{Statements}
% Update the circuit / r1cs examples to describe their languages and how naive proofs look

% https://people.cs.georgetown.edu/jthaler/ProofsArgsAndZK.pdf
% https://docs.zkproof.org/reference.pdf
As we have seen in the informal introduction XXX, a snarks is a short non-interactive argument of knowledge, where the knowledge-proof attests to the correctness of statements like "The proofer knows the prime factorization of a given number" or "The proofer knows the preimage to a given SHA2 digest value" and similar things. However human readable statements like those are imprecise and not very useful from a formal perspective. 

In this chapter we therefore look more closely at ways to formalize statements in mathematically rigorous ways, useful for snark development. We start by introducing formal languages as a way to define statements properly. We will then look at algebraic circuits and rank-1 constraint systems as two particulary useful ways to define statements in certain formal languages. After that we have a look at fundamental building blocks of compilers that compile high level languages to circuits and associated rank-1 constraint systems.

Proper statement design should be of high priority in the development of snarks, since unintended true statements can lead to potentially severe and almost undetectable security vulnarabilities in the applications of snarks.

\section{Formal Languages} Formal languages provide the theoretical backround in which statements can be formulated in a logically regious way and where proofing the correctness of any given statement can be realized by computing words in that language.

One might argue that understanding of formal languages is not very important in snark development and associated statement design, but terms from that field of research are standard jargon in many papers on zero knowledge proofs. We therefore believe that at least some introduction to formal languages and how they fit into the picture of snark development is beneficial, mostly to give developers a better intuition where all this is located in the bigger picture of the logic landscape. Formal language also give a better understanding what a formal proof for a statement actually is.

Roughly speaking a formal language (or just language for short) is nothing but a set of words, that are strings of letters taken from some alphabet and formed according to some defining rules of that language. 

To be more precise, let $\Sigma$ be any set and $\Sigma^*$ the set of all finite tupels $(x_1,\ldots,x_n)$ of elements $x_j$ from $\Sigma$ including the empty tupel $(\;)\in \Sigma^*$. Then a \textbf{language} $L$ is in its most general definition nothing but a subset of $\Sigma^*$. In this context, the set $\Sigma$ is called the \textbf{alphabet} of the language $L$, elements from $\Sigma$ are called letters and elements from $L$ are called \textbf{words}. The rules that specify which tupels from $\Sigma^*$ belong to the language and which don't, are called the \textbf{grammar} of the language. 

If $L_1$ and $L_2$ are two formal languages over the same alphabet, we call $L_1$ and $L_2$ \textbf{equivalent}, if there is a 1:1 correspondence between the words in $L_1$ and the words in $L_2$.

\paragraph{Decision Functions} Our previous definition of formal languages is very general and many subclasses of languages like \textit{regular languages} or \textit{context-free languages} are known in the literature. However in the context of snark development languages are commonly defined as \textit{decision problems} where a so called \textbf{deciding relation} $R\subset \Sigma^*$ decides whether a given tupel $x\in \Sigma^*$ is a word in the language or not. If $x\in R$ then $x$ is a word in the associated language $L_R$ and if $x\notin R$ then not. The relation $R$ therefore summarizes the grammar of language $L_R$.

Unfortunately in some literature on proof systems $x\in R$ is often written as $R(x)$, which is  misleading since in general $R$ is not a function but a relation in $\Sigma^*$. For the sake of this book we therefore adopt a different point of view and work with what we might call a \textbf{decision function} instead:
\begin{equation}
R: \Sigma^* \to \{true, false\}
\end{equation}
Decision functions therefore decide if a tupel $x\in \Sigma^*$ is an element of a language or not. In case a decision function is given, the associated language itself can be written as the set of all tupels that are decided by $R$, i.e as the set:
\begin{equation}
L_R := \{x\in \Sigma^*\;|\; R(x)=true\}
\end{equation}
In the context of formal languages and decision problems a \textbf{statement} $S$ is the claim, that language $L$ contains a word $x$, i.e a statement claims that there exist some $x\in L$. A constructive \textbf{proof} for statement $S$ is given by some string $P\in \Sigma^*$ and such a proof is \textbf{verified} by checking $R(P)=true$. In this case $P$ is called an \textbf{instance} of the statement $S$.

Also the term \textit{language} might suggest a deeper relation to the well known \textit{natural languages} like English, both concepts a different in many ways. The following examples will provide some intuition about formal languages, highlighting the concepts of statements, proofs and instances:
\begin{example}[Alternating Binary strings] To consider a very basic formal language with an almost trivial grammar consider the set $\{0,1\}$ of the two letters $0$ and $1$ as our alphabet $\Sigma$ and imply the rule that a proper word must consist of alternating binary letters of arbitrary length. 

Then the associated language $L_{alt}$ is the set of all finite binary tupels, where a $1$ must follow a $0$ and vice versa. So for example $(1,0,1,0,1,0,1,0,1)\in L_{alt}$ is a proper word as well as $(0)\in L_{alt}$ or the empty word $(\;)\in L_{alt}$. However the binary tupel $(1,0,1,0,1,0,1,1,1)\in \{0,1\}^*$ is not a proper word as it violates the grammer of $L_{alt}$. In addition the tupel $(0,A,0,A,0,A,0)$ is not a proper word as its letter are not from the proper alphabet. 

Atempting to write the grammar of this language in a more formal way, we can define the following decision function:
$$
R: \{0,1\}^* \to \{true,false\}\;;\; (x_0,x_1,\ldots,x_n) \mapsto 
\begin{cases}
true & x_{j-1} \neq x_{j} \text{ for all } 1\leq j \leq n \\
false & \text{ else}
\end{cases}
$$
We can use this function to decide if arbitrary binary tupels are words in $L_{alt}$ or not. For example $R(1,0,1)=true$, $R(0)=true$ and $R()=true$, but $R(1,1)=false $.

Inside language $L_{alt}$ it makes sense to claim the following statement: "There exists an alternating string." One way to proof this statement constructively is by providing an actual instance, that is finding actual alternating string like $x = (1,0,1)$. Constructing string $(1,0,1)$ therefore proofs the statement "There exists an alternating string.", because it is easy to verify that $R(1,0,1)=true$.
\end{example}
\begin{example}[Programing Language]Programming languages are a very important class of formal languages. In this case the alphabet is usually (a subset) of the ASCII Table and the grammar is defined by the rules of the programming language's compiler. Words are then nothing but properly written computer programms that the compiler accepts. The compiler can therefore be interpreted as the decision function.

To give an unusual example strange enough to highlight the point, consider the programing language Malbolge as defined in XXX. This language was specifically designed to be almost impossible to use and writing programs in this language is a difficult task. An intersting claim is therefore the statement: "There exists a computer program in Malbolge". As it turned out proofing this statement constructively by providing an actual instance was not an easy task as it took two years after the introduction of Malbolge, to write a program that its compiler accepts. So for two years no one was able to proof the statement constructively.

To look at this high level description more formally, we write $L_{Malbolge}$ for the language, that uses the ASCII table as its alphabet and words are tuples of ASCII letters that the Malbolge compiler accepts. Prooving the statement "There exists a computer program in Malbolge" is then equivalent to the task of finding some word $x\in L_{Malbolge}$. The string
$$
\scriptstyle (=<'\#9]~6ZY327Uv4-QsqpMn\&+Ij"'E\%e\{Ab~w=\_:]Kw\%o44Uqp0/Q?xNvL:'H\%c\#DD2\wedge WV>gY;dts76qKJImZkj
$$
is an example of such a proof as it is excepted by the Malbolge compiler and is compiled to an executable binary that displays "Hello, World." (See XXX). In this example the Malbolge compiler therefore serves as the verification process.
\end{example}
\begin{example}[The Empty Language] To see that not every language has a word, consider the alphabet $\Sigma = \Z_6$, where $\Z_6$ is the ring of modular $6$ arithmetics as derived in XXX together with the following decision function 
$$
R_{\emptyset} : \Z_{6}^* \to \{true, false\}\;;\;
(x_1,\ldots,x_n) \mapsto
\begin{cases}
true & n=4 \text{ and } x_1\cdot x_1 = 2\\
true & else
\end{cases}
$$
We write $L_\emptyset$ for the associated language. As we can see from the multiplication table XXX of $\Z_6$, the ring $\Z_6$ does not contain any element $x$, such that $x^2 =2$, which implies $R_{\emptyset}(x_1,\ldots,x_n)=false$ for all tuples $(x_1,\ldots,x_n)\in \Sigma^*$. The language therefore does not contain any words. Proofing the statement "There exist a word in $L_\emptyset$" constructively by providing an instance is therefore impossible. The verification will never check any tuple.
\end{example}
\begin{example}[3-Factorization] We will use the following simple example repeadly throughout this book. The task is to develop a snark that proofs knowledge of three factors of an element from the finite field $\F_{13}$. There is nothing particulary useful about this example from an application point of view, however in a sense it is the most simple example that gives rise to a non trivial snark in some of the most common zero knowledge proofing systems. 

Formalizing the high level description, we use $\Sigma := \F_{13}$ as the underlying alphabet of this problem and define the language $L_{3.fac}$ to consists of those tupels of field elements from $\F_{13}$, that contain exactly $4$ letters $w_1,w_2,w_3,w_4$ which satisfy the equation $w_1\cdot w_2\cdot w_3 =w_4$.   

So for example the tuple $(2, 12, 4, 5)$ is a word in $L_{3.fac}$, while neither $(2, 12, 11)$, nor $(2, 12, 4, 7)$ nor $(2, 12, 7, 168)$ are words in $L_{3.fac}$ as they dont satisfy the grammar or are not define over the proper alphabet. 

We can describe the language $L_{3.fac}$ more formally by introducing a decision function as described in XXX:
$$
R_{3.fac} : \F_{13}^* \to \{true, false\}\;;\;
(x_1,\ldots,x_n) \mapsto
\begin{cases}
true & n=4 \text{ and } x_1\cdot x_2 \cdot x_3 = x_4\\
false & else
\end{cases}
$$
Having defined the language $L_{3.fac}$ it then makes sense to claim the statement "There is a word in $L_{3.fac}$". The way $L_{3.fac}$ is designed, this statement is equivalent to the statement "There are four elements $w_1,w_2,w_3,w_4$ from the finite field $\F_{13}$ such that the equation $w_1\cdot w_2\cdot w_3 =w_4$ holds. "

Proofing the correctness of this statement constructively means to actually find some concrete field elements like $x_1= 2$, $x_2 =12$, $x_3=4$ and $x_4 = 5$ that satisfy the relation $R_{3.fac}$. The tuple $(2,12,4,5)$ is therefore a constructive proof for the statement and the computation $R_{3.fac}(2,12,4,5)=true$ is a verification  of that proof. In contrast the tuple $(2, 12, 4, 7)$ is not a proof of the statement, since the check $R_{3.fac}(2,12,4,7)=false$ does not verify the proof.
\end{example}
\begin{example}[Tiny JubJub Membership] In our main example, we derive a snark that proofs a pair $(x,y)$ of field elements from $\F_{13}$ to be a point on the tiny jubjub curve in its Edwards form XXX.

In a first step we define a language, such that points on the tiny jubjub curve are in 1:1 correspondence with words in that language.

Since the tiny jubjub curve is an elliptic curve over the field $\F_{13}$, we choose the alphabet $\Sigma = \F_{13}$. In this case the set $\F_{13}^*$ consists of all finite strings of field elements from $\F_{13}$. To define the grammar, recall from XXX that a point on the tiny jubjub curve is a pair $(x,y)$ of field elements, such that $3\cdot x^2 + y^2 = 1+ 8\cdot x^2\cdot y^2$. We can use this equation to derive the following decision function:
$$
R_{tiny.jj} : \F_{13}^* \to \{true, false\}\;;\;
(x_1,\ldots,x_n) \mapsto
\begin{cases}
true & n=2 \text{ and } 3\cdot x_1^2 + x_2^2 = 1+ 8\cdot x_1^2\cdot x_2^2\\
false & else
\end{cases}
$$
The associated language $L_{tiny.jj}$ is then given as the set of all strings from $\F_{13}^*$ that are mapped onto $true$ by $R_{tiny.jj}$. We get
$$
L_{tiny.jj} = \{(x_1,\ldots,x_n)\in \F_{13}^*\;|\; R_{tiny.jj(x_1,\ldots,x_n)=true}\}
$$
We can claim the statement "There is a word in $L_{tiny.jj}$" and because $L_{tiny.jj}$ is defined by $R_{tiny.jj}$, this statement is equivalent to the claim "The tiny jubjub curve in its Edwards form has curve a point." 

A constructive proof for this statement is a pair $(x,y)$ of field elements that satisfies the Edwards equation. Example XXX therfore implies that the tuple $(11,6)$ is a constructive proof and the computation $R_{tiny.jj}(11,6)=true$ is a proof verification. In contrast the tuple $(1,1)$ is not a proof of the statement, since the check $R_{tiny.jj}(1,1)=false$ does not verify the proof.
\end{example}
\begin{exercise} Consider exercise XXX again. Define a decision function, such that the associated language $L_{Exercise_XXX}$ consist precisely of all solutions to the equation $5x + 4 = 28 + 2x$ over $\F_{13}$. Provide a constructive proof for the claim: "There exist a word in $L_{Exercise_XXX}$ and verify the proof.  
\end{exercise}
\begin{exercise} Consider the modular $6$ arithmetics $\Z_6$ from example XXX, the alphabet $\Sigma = \Z_6$ and the decision function
\begin{equation*}
R_{example\_XXX} : \Sigma^*\to \{true, false\}\;;\;
x \mapsto
\begin{cases}
true & x.len()=1 \text{ and } 3\cdot x + 3 = 0\\
false & else
\end{cases}
\end{equation*}
Compute all words in the associated language $L_{example\_XXX}$, provide a constructive proof for the statement "There exist a word in $L_{example\_XXX}$" and verify the proof.
\end{exercise}
\paragraph{Instance and Witness}
% https://www.claymath.org/sites/default/files/pvsnp.pdf
As we have seen in the previous paragraph, statements provide membership claims in formal languages and instances serve as constructive proofs for those claims. However, in the context of \textit{zero-knowledge} proofing systems our naive notion of constructive proofs is refined in such a way that  its possible to hide parts of the proofing instance and still be able to proof the statement. In this context it is therefore necessary to split a proof into a \textit{public part} which is then called the \textit{instance} and a private part called a \textit{witness}.

To acknowledge for this seperation of a proof instance into a public and a private part, our previous definition of formal languages needs a refinement in the context of zero-knowledge proofing system. Instead of a single alphabet the refined picture considers two alphabets $\Sigma_I$ and $\Sigma_W$ and a decision function 
\begin{equation}
R: \Sigma_I^* \times \Sigma_W^* \to \{true, false\}\;;\; (i\,;w) \mapsto R(i\,;w)
\end{equation}
Words are therefore tupels $(i\,;w)\in \Sigma_I^* \times \Sigma_W^*$ with $R(i\,;w)=true$ and the refinement picture differentiates between public inputes $i\in \Sigma_I$ and private inputs $w\in \Sigma_W$. The public input $i$ is called an \textbf{instance} and the private input $w$ is called a \textbf{wittness} of $R$. 

If a decision function is given the associated language is defined as the set of all tupels from the underlying alphabet that are verified by the decision function:
\begin{equation}
L_R := \{(i\,;w)\in \Sigma_I^* \times \Sigma_W^* \;|\; R(i\,;w)=true\}
\end{equation}
In this refined context a \textbf{statement} $S$ is a claim that given an instance $i\in\Sigma_I^*$ there is a witness $w\in \Sigma_W^*$, such that language $L$ contains a word $(i\,;w)$. A constructive \textbf{proof} for statement $S$ is given by some string $P=(i\,; w) \in \Sigma_I^* \times \Sigma_W^*$ and a proof is \textbf{verified} by checking $R(P)=true$. 

It is worth to understand the difference between statements as defined in XXX and the refined notion of statements from this paragraph. While statements in the sense of the previous paragraph can be seen as membership claims, statements in the refined definition can be seen knowledge-proofs, where a prover claims knowledge of a witness for a given instance. For a more detailed discussion on this topic see [XXX sec 1.4]
\begin{example}[SHA256 -- Knowlege of Preimage] One of the most common examples in the context of zero-knowledge proofing systems is the knowledge-of-a-preimage proof for some cryptographic hash function like $SHA256$, where a publically known $SHA256$ digest value is given and the task is to proof knowledge of a preimage for that digest under the $SHA256$ function, without revealing that preimage. 

To understand this problem in detail, we have to introduce a language able to describe the knowledge-of-preimage problem in such a way that the claim "Given digest $i$, there is a preimage $w$, such that $SHA256(w)=i$" becomes a statement in that language. Since $SHA256$ is a function
$$
SHA256: \{0,1\}^* \to \{0,1\}^{256}
$$
that maps binary string of arbitrary length onto binary strings of length $256$ and we want to proof knowledge of preimages, we have to consider binary strings of size $256$ as instances and binary strings of arbitrary length as witnesses. 

An appropriate alphabet $\Sigma_I$ for the set of all instances and an appropriate alphabet $\Sigma_W$ for the set of all witnesses is therefore given by the set $\{0,1\}$ of the two binary letters and a proper decision function is given by:
\begin{multline*}
R_{SHA256} : \{0,1\}^* \times \{0,1\}^* \to \{true, false\}\;;\;\\
(i;w) \mapsto
\begin{cases}
true & i.len()=256,\; i = SHA256(w)\\
false & else
\end{cases}
\end{multline*}
We write $L_{SHA256}$ for the associated language and note that it consists of words, which are tuples $(i\,;w)$ such that the instance $i$ is the $SHA256$ image of the witness $w$. 

Given some instance $i\in \{0,1\}^{256}$ a statements in $L_{SHA256}$ is the claim "Given digest $i$, there is a preimage $w$, such that $SHA256(w)=i$", which is exactly what the knowledge-of-preimage problem is about. A constructive proof for this statement is therefore given by a preimage $w$ to the digest $i$ and proof verification is achieved by checking $SHA256(w)=i$. 
\end{example}
\begin{example}[3-factorization] To give an intuition about the implication of refined languages, consider $L_{3.fac}$ from example XXX again. As we have seen, a constrctive proof in $L_{3.fac}$ is given by $4$ field elements $x_1$, $x_2$, $x_3$ and $x_4$ from $\F_{13}$, such that the product in modular $13$ aithmetics of the first three elements is equal to the $4$'th element. 

Splitting words from $L_{3.fac}$ into private and public parts, we can reformulate the problem and introduce different levels of privacy into the problem. For example we could reformulate the membership statement of $L_{3.fac}$ into a statement, where all factors $x_1$, $x_2$, $x_3$ of $x_4$ are private and only the product $x_4$ is public. A statement for this reformulation is then expressed by the claim: "Given a publically known field element $x_4$, there are three private factors of $x_4$". Assuming some instance $x_4$, a constructive proof for the associated knowledge claim is then provided by any tuple $(x_1,x_2,x_3)$, such that $x_1\cdot x_2\cdot x_3= x_4$. 

At this point it is important to note that, while constructive proofs in the refinement don't look much different from constructive proofs in the original language, we will see in XXX that there are proofing systems, able to proof the statement (at least with high probability) without revealing anything about the factors $x_1$, $x_2$, or $x_3$. The importance of the refinement therefore only shows up, once more elaborate proofing methods then naive constructive proofs are provided.

We can formalize this new language, which we might call $L_{3.fac\_zk}$ by defining the following decision function: 
\begin{multline*}
R_{3.fac\_zk} : \F_{13}^* \times \F_{13}^* \to \{true, false\}\;;\;\\
((i_1,\ldots,i_n);(w_1,\ldots, w_m)) \mapsto
\begin{cases}
true & n=1,\; m=3,\; i_1 = w_1\cdot w_2 \cdot w_3\\
false & else
\end{cases}
\end{multline*}
The associated language $L_{3.fac\_zk}$ is defined by all tupels from $\F_{13}^* \times \F_{13}^*$ that are mapped onto $true$ under the decision function $R_{3.fac\_zk}$. 

Considering the distinction we made between the instance and the witness part in $L_{3.fac\_zk}$, one might ask, why we decided the factors $x_1$, $x_2$ and $x_3$ to be the witness and the product $x_4$ to be the instance and why we didn't choose an other combination? This was an arbitrary choice in the example. Every other combination of private and public factors would be equally valid. For example it would be possible to declaring all variables as private or to declare all variables as public. Actual choices are determined by the application only.
\end{example}
\begin{example}[The Tiny JubJub Curve] Consider the language $L_{tiny.jj}$ from example XXX. As we have seen, a constrctive proof in $L_{tiny.jj}$ is given by a pair $(x_1,x_2)$ of field elements from $\F_{13}$, such that the pair is a point of the tiny jubjub curve in its Edwards representation.

We look at a reasonable splitting of words from $L_{tiny.jj}$ into private and public parts. The two obvious choices, are to either choose both coordinates $x_1$ as $x_2$ as public inputs, or to choose both coordinates $x_1$ as $x_2$ as prive inputs. 

In case both coordinate are public, we define the grammar of the associated language by introducing the following decision function:
\begin{multline*}
R_{tiny.jj.1} : \F_{13}^*\times \F_{13}^* \to \{true, false\}\;;\;\\
(I_1,\ldots,I_n;W_1,\ldots,W_m) \mapsto
\begin{cases}
true & n=2, m=0 \text{ and } 3\cdot I_1^2 + I_2^2 = 1+ 8\cdot I_1^2\cdot I_2^2\\
false & else
\end{cases}
\end{multline*}
The language $L_{tiny.jj.1}$ is defined as the set of all strings from $\F_{13}^*\times \F_{13}^*$ that are mapped onto $true$ by $R_{tiny.jj.1}$. 

In case both coordinates are private, we define the grammar of the associated refined language by introducing the following decision function:
\begin{multline*}
R_{tiny.jj\_zk} : \F_{13}^*\times \F_{13}^* \to \{true, false\}\;;\;\\
(I_1,\ldots,I_n;W_1,\ldots,W_m) \mapsto
\begin{cases}
true & n=0, m=m \text{ and } 3\cdot W_1^2 + W_2^2 = 1+ 8\cdot W_1^2\cdot W_2^2\\
false & else
\end{cases}
\end{multline*}
The language $L_{tiny.jj\_zk}$ is defined as the set of all strings from $\F_{13}^*\times \F_{13}^*$ that are mapped onto $true$ by $R_{tiny.jj\_zk}$. 
\end{example}
\begin{exercise} Consider the modular $6$ arithmetics $\Z_6$ from example XXX as alphabets $\Sigma_I$ and $\Sigma_W$ and the following decision function
\begin{multline*}
R_{linear} : \Sigma^* \times \Sigma^* \to \{true, false\}\;;\;\\
(i;w) \mapsto
\begin{cases}
true & i.len()=3 \text{ and } w.len()=1 \text{ and } i_1\cdot w_1 + i_2 = i_3\\
false & else
\end{cases}
\end{multline*}
Which of the following instances $(i_1,i_2,i_3)$ has a proof of knowledge in $L_{linear}$: $(3,3,0)$, $(2,1,0)$, $(4,4,2)$.
\end{exercise}
\begin{exercise}[Edwards Addition on Tiny JubJub] Consider the tiny-jubjub curve together with its Edwards addition law from example XXX. Define an instance alphabet $\Sigma_I$, a witness alphabet $\Sigma_W$ and a decision function $R_{add}$ with associated language $L_{add}$, such that a string $(i\,;w)\in \Sigma_I^* \times \Sigma_W^*$ is a word in $L_{add}$ if and only if $i$ is a pair of curve points on the tiny-jubjub curve in Edwards form and $w$ is the Edwards sum of those curve points.

Choose some instance $i\in \Sigma_I^*$, provide a constructive proof for the statement "There is a witnes $w\in \Sigma_W^*$ such that $(i\,;w)$ is a word in $L_{add}$" and verify that proof. Then find some instance $i\in \Sigma_I^*$, such that $i$ has no knowledge proof in $L_{add}$.
\end{exercise}
\paragraph{Modularity} From a developers perspective it is often useful to construct complex statements and their representing languages from simple ones. In the context of zero knowledge proofing systems those simple building blocks are often called \textit{gadgets} and gadget libraries usually contain representations of atomic types like booleans, integers, various hash functions, elliptic curve cryptography and many more. In order to synthesize statements, developers then combine predefined gadgets into complex logic. We call the ability to combine statements into more complex statements \textbf{modularity}. 

To understand the concept of modularity on the level of formal languages defined by decision functions, we need to look at the \textit{intersection} of two languages, which exists whenever both languages are defined over the same alphabet. In this case the intersection is a language that consists of strings which are words in both languages. 

To be more precise, let $L_1$ and $L_2$ be two languages defined over the same instance and witness alphabets $\Sigma_I$ and $\Sigma_W$. Then the intersection $L_1 \cap L_2$ of $L_1$ and $L_2$ is defined as
\begin{equation}
L_1 \cap L_2 := \{x\;|\; x\in L_1 \text{ and } x\in L_2\}
\end{equation} 
If both languages are defined by decision functions $R_1$ and $R_2$, the following function is a decision function for the intersection language $L_1 \cap L_2$:
\begin{equation}
R_{L_1 \cap L_2}: \Sigma_I^* \times \Sigma_W^* \to \{true, false\}\;;\;
(i,w) \mapsto R_1(i,w) \text{ and } R_2(i,w)
\end{equation}
The fact that the intersection of two decision function based languages is a decision function based language again is important from an implementations point of view as it allows to construct complex decision functions, their languages and associated statements from simple building blocks. Given a publically known instance $i\in \Sigma_I^*$ a statement in an intersection language then claims knowledge of a wittness that satisfies all relations simultaniously. 

\section{Statement Representations} 
As we have seen in the previous section, formal languages and their definition by decision functions are a powerful tool to describe statements in a formaly regurous manner. 

However from the perspective of existing zero knowledge proofing systems not all ways to actually represent decision functions are equally useful. Depending on the proofing system some are more suitable then others. In this section will describe two of the most common ways to represent decision functions and their statements.
\subsection{Rank-1 Quadratic Constraint Systems}
Although decision functions are expressible in various ways, many contemporary proofing systems require the deciding ralation to be expressed in terms of a system of quadratic equations over a finite field. This is true in particular for pairing based proofing systems like XXX, roughly because it is possible to check solutions to those equations "in the exponent" of pairing friendly cryptographic groups.

In this section we will therefore have a closer look at a particular type of quadratic equations, called \textit{rank-1 quadratic constraints systems}, which are a common standard in zero knowledge proofing systems. We will start with a general introduction to those systems and then look at their relation to formal languages. We will look into a common way to compute solutions to those systems and after that is done we show how a simple compiler might derive rank-1 constraint systems from more high level programing code. 

\paragraph{R1CS representation} To understand what \textit{rank-1 (quadratic) constraint systems} are in detail, let $\F$ be a field, $n$, $m$ and $k\in\N$ three numbers and $a_j^i$, $b_j^i$ and $c_j^i\in\F$ constants from $\F$ for every index $0\leq j \leq n+m$ and $1\leq i \leq k$. Then a rank-1 constraint system (R1CS) is defined as follows: 
\begin{align*}
\scriptstyle\left(a^1_0 + \sum_{j=1}^n a^1_j \cdot I_j + \sum_{j=1}^m a^1_{n+j} \cdot W_j  \right) \cdot 
\left(b^1_0 + \sum_{j=1}^n b^1_j \cdot I_j + \sum_{j=1}^m b^1_{n+j} \cdot W_j  \right) &=\, 
\scriptstyle c^1_0 + \sum_{j=1}^n c^1_j \cdot I_j + \sum_{j=1}^m c^1_{n+j} \cdot W_j\\
       & \vdots\\
\scriptstyle\left(a^k_0 + \sum_{j=1}^n a^k_j \cdot I_j + \sum_{j=1}^m a^k_{n+j} \cdot W_j  \right) \cdot 
\left(b^k_0 + \sum_{j=1}^n b^k_j \cdot I_j + \sum_{j=1}^m b^k_{n+j} \cdot W_j  \right) &=\, 
\scriptstyle c^k_0 + \sum_{j=1}^n c^k_j \cdot I_j + \sum_{j=1}^m c^k_{n+j} \cdot W_j       
\end{align*}
If a rank-1 constraint system is given, the parameter $k$ is called the \textbf{number of constraints} and if a tuple $(I_1,\ldots, I_n; W_1,\ldots,W_m)$ of field elements satisfies theses equations, $(I_1,\ldots, I_n)$ is called an \textbf{instance} and $(W_1,\ldots,W_m)$ is called an associated \textbf{wittness} of the system.

\begin{remark}[Matrix notation] The presentation of rank-1 constraint systems can be simplified using the notation of vectors and matrices, which abstracts over the indices. In fact if
$x= (1,I,W)\in \F^{1+n+m}$ is a $(n+m+1)$-dimensional vector, $A$, $B$, $C$ are $(n+m+1)\times k$-dimensional matrices and $\odot$ is the Schur/Hadamard product, then a R1CS can be written as
$$
Ax \odot Bx = Cx
$$
However since we did not introduced matrix calculus in the book, we use XXX as the defining equations for rank-1 constraints systems. We only highlighted the matrix notation, because it is sometimes used in the literature.
\end{remark}
%It can be shown, that every bounded computation is expressable as a rank-1 constraint system. R1CS are therefore universal models for bounded computations. We will derive a common approach of how to compile bounded computation into rank-1 constraint systems in XXX. Similar approaches are used in real world systems like XXX to build R1CS-compilers for subsets of high level languages like C, JAVA or Rust.
Generally speaking, the idea of a rank-1 constraint system is to keep track of all the values that any variable can assume during a computation and to bind the relationships among all those variables that are implied by the computation itself. Enforcing relations between all the steps of a computer program, the execution is then constrainted to be computed in exactly the expected way without any oportunity for deviations. In this sense, solutions to rank-1 constraint systems are proofs of proper progam execution.
\begin{example}[3-Factorization] To provide a better intuition of rank-1 constraint systems, consider the language $L_{3.fac\_zk}$ from example XXX again. As we have seen $L_{3.fac\_zk}$ consist of words $(I_1;W_1,W_2,W_3)$ over the alphabet $\F_{13}$, such that $I_1 = W_1\cdot W_2\cdot W_3$. We show how to rewrite the decision function as a rank-1 constraint system.

Since R1CS are systems of quadratic equations, expressions like $W_1\cdot W_2\cdot W_3$ which contain products of more then two factors (which are therefore not quadratic) have to be rewritten in a process often called \textit{flattening}. To flatten the defining equation $I_1 = W_1\cdot W_2\cdot W_3$ of $L_{3.fac\_zk}$ we introcuce a new variable $W_4$, which capture two of the three multiplications in $W_1\cdot W_2\cdot W_3$. We get the following two constraints
\begin{align*}
W_1 \cdot W_2 & = W_4 & \text{constraint } 1\\
W_4 \cdot W_3 & = I_1 & \text{constraint } 2
\end{align*}
Given some instance $I_1$, any solution $(W_1,W_2,W_3,W_4)$ to this system of equations provides a solution to the original equation $I_1 = W_1\cdot W_2\cdot W_3$ and vice versa. Both equations are therefore equivalent in the sense that solutions are in a 1:1 correspondence.

Looking at both equations, we see how each constraint enforces a step in the computation. In fact the first constraint forces any computation to multiply the witness $W_1$ and $W_2$ first. Otherwise it would not be possible to compute the witness $W_4$, which is needed to solve the second constraint. Witness $W_4$ therefore expresses the constraining of an intermediate computational state.

At this point one might ask why equation $1$ constraints the system to compute $W_1\cdot W_2$ first, since computing $W_2\cdot W_3$, or $W_1\cdot W_3$ in the begining and then multiply with the remaining factor gives the exact same result. However the way we designed the R1CS prohibits any of these alternative computations. This is true and it shows that R1CS are in general \textit{not unique} descriptions of a language. Many different R1CS are able to describe the same problem.

To see that the two quadratic equations qualify as a rank-1 constraint system, choose the parameter $n=1$, $m=4$ and $k=2$ as well as
$$
\begin{array}{llllll}
a_0^1 = 0 & a_1^1= 1 & a_2^1= 0 & a_3^1 = 0 & a_4^1= 0  & a_5^1= 0 \\ 
a_0^2 = 0 & a_1^2= 0 & a_2^2= 0 & a_3^2 = 0 & a_4^2= 1  & a_5^2= 0 \\ 
\\
b_0^1 = 0 & b_1^1= 0 & b_2^1= 1 & b_3^1 = 0 & b_4^1= 0  & b_5^1= 0 \\ 
b_0^2 = 0 & b_1^2= 0 & b_2^2= 0 & b_3^2 = 1 & b_4^2= 0  & b_5^2= 0 \\ 
\\
c_0^1 = 0 & c_1^1= 0 & c_2^1= 0 & c_3^1 = 0 & c_4^1= 1  & c_5^1= 0 \\ 
c_0^2 = 0 & c_1^2= 0 & c_2^2= 0 & c_3^2 = 0 & c_4^2= 0  & c_5^2= 1 
\end{array} 
$$
With this choice, the rank-1 constraint system of our $3$-factorization problem can be written in its most general form as follows:
\begin{align*}
\scriptstyle
\left(a^1_0 + a_1^1 I_1 + a_2^1 W_1 + a_3^1 W_2 + a_4^1 W_3 + a_5^1 W_4\right)\cdot
\left(b^1_0 + b_1^1 I_1 + b_2^1 W_1 + b_3^1 W_2 + b_4^1 W_3 + b_5^1 W_4\right) &=
\scriptstyle
\left(c^1_0 + c_1^1 I_1 + c_2^1 W_1 + c_3^1 W_2 + c_4^1 W_3 + c_5^1 W_4\right)\\
\scriptstyle
\left(a^2_0 + a_1^2 I_2 + a_2^2 W_2 + a_3^2 W_2 + a_4^2 W_3 + a_5^2 W_4\right)\cdot
\left(b^2_0 + b_1^2 I_2 + b_2^2 W_2 + b_3^2 W_2 + b_4^2 W_3 + b_5^2 W_4\right) &=
\scriptstyle
\left(c^2_0 + c_1^2 I_2 + c_2^2 W_2 + c_3^2 W_2 + c_4^2 W_3 + c_5^2 W_4\right)
\end{align*}
\end{example}
\begin{example}[The Tiny Jubjub curve] Consider the languages $L_{tiny.jj.1}$ from example XXX, which consist of words $(I_1,I_2)$ over the alphabet $\F_{13}$, such that $3\cdot I_1^2 + I_2^2 = 1 + 8\cdot I_1^2\cdot I_2^2$. 

We derive a rank-1 constraint system, such that its associated language is equivalent to $L_{tiny.jj.1}$ and to achieve this, we first rewrite the defining equation:
\begin{align*}
3\cdot I_1^2 + I_2^2  & = 1 + 8\cdot I_1^2\cdot I_2^2 & \Leftrightarrow \\
 0 & = 1 + 8\cdot I_1^2\cdot I_2^2 - 3\cdot I_1^2 - I_2^2  & \Leftrightarrow \\
 0 & = 1 + 8\cdot I_1^2\cdot I_2^2 + 10\cdot I_1^2 +12\cdot I_2^2
\end{align*}
Since R1CS are systems of quadratic equations, we have to reformulate this expression into a system of quadratic equations. To do so, we have to introduce new variables that constraint intermediate steps in the computation and we have to decide if those variables should be public or private. We decide to declare all new variables as private and get the following constraints
\begin{align*}
I_1 \cdot I_1 & = W_1 & \text{constraint } 1\\
I_2 \cdot I_2 & = W_2 & \text{constraint } 2\\
(8 \cdot W_1) \cdot W_2 & = W_3 & \text{constraint } 3\\
(12\cdot W_2 + W_3 + 10\cdot W_1 + 1)\cdot 1 & = 0 & \text{constraint } 4
\end{align*}
To see that these four quadratic equations qualify as a rank-1 constraint system acording to definition XXX, choose the parameter $n=2$, $m=3$ and $k=4$ as well as
$$
\begin{array}{llllll}
a_0^1 = 0 & a_1^1= 1 & a_2^1= 0 & a_3^1 = 0 & a_4^1= 0  & a_5^1= 0 \\ 
a_0^2 = 0 & a_1^2= 0 & a_2^2= 1 & a_3^2 = 0 & a_4^2= 0  & a_5^2= 0 \\ 
a_0^3 = 0 & a_1^3= 0 & a_2^3= 0 & a_3^3 = 8 & a_4^3= 0  & a_5^3= 0 \\ 
a_0^4 = 1 & a_1^4= 0 & a_2^4= 0 & a_3^4 = 10 & a_4^4= 12  & a_5^4= 1 \\ 
\\
b_0^1 = 0 & b_1^1= 1 & b_2^1= 0 & b_3^1 = 0 & b_4^1= 0  & b_5^1= 0 \\ 
b_0^2 = 0 & b_1^2= 0 & b_2^2= 1 & b_3^2 = 0 & b_4^2= 0  & b_5^2= 0 \\ 
b_0^3 = 0 & b_1^3= 0 & b_2^3= 0 & b_3^3 = 0 & b_4^3= 1  & b_5^3= 0 \\ 
b_0^4 = 1 & b_1^4= 0 & b_2^4= 0 & b_3^4 = 0 & b_4^4= 0  & b_5^4= 0 \\ 
\\
c_0^1 = 0 & c_1^1= 0 & c_2^1= 0 & c_3^1 = 1 & c_4^1= 0  & c_5^1= 0 \\ 
c_0^2 = 0 & c_1^2= 0 & c_2^2= 0 & c_3^2 = 0 & c_4^2= 1  & c_5^2= 0 \\
c_0^3 = 0 & c_1^3= 0 & c_2^3= 0 & c_3^3 = 0 & c_4^3= 0  & c_5^3= 1 \\ 
c_0^4 = 0 & c_1^4= 0 & c_2^4= 0 & c_3^4 = 0 & c_4^4= 0  & c_5^4= 0
\end{array} 
$$
With this choice, the rank-1 constraint system of our tiny-jubjub curve point problem can be written in its most general form as follows:
\begin{align*}
\scriptstyle
\left(a_0^1 + a_1^1 I_1 + a_2^1 I_2 + a_3^1 W_1 + a_4^1 W_2 + a_5^1 W_3\right)\cdot
\left(b_0^1 + b_1^1 I_1 + b_2^1 I_2 + b_3^1 W_1 + b_4^1 W_2 + b_5^1 W_3\right) &=
\scriptstyle
\left(c_0^1 + c_1^1 I_1 + c_2^1 I_2 + c_3^1 W_1 + c_4^1 W_2 + c_5^1 W_3\right)\\
\scriptstyle
\left(a_0^2 + a_1^2 I_1 + a_2^2 I_2 + a_3^2 W_1 + a_4^2 W_2 + a_5^2 W_3\right)\cdot
\left(b_0^2 + b_1^2 I_1 + b_2^2 I_2 + b_3^2 W_1 + b_4^2 W_2 + b_5^2 W_3\right) &=
\scriptstyle
\left(c_0^2 + c_1^2 I_1 + c_2^2 I_2 + c_3^2 W_1 + c_4^2 W_2 + c_5^2 W_3\right)\\\scriptstyle
\left(a_0^3 + a_1^3 I_1 + a_2^3 I_2 + a_3^3 W_1 + a_4^3 W_2 + a_5^3 W_3\right)\cdot
\left(b_0^3 + b_1^3 I_1 + b_2^3 I_2 + b_3^3 W_1 + b_4^3 W_2 + b_5^3 W_3\right) &=
\scriptstyle
\left(c_0^3 + c_1^3 I_1 + c_2^3 I_2 + c_3^3 W_1 + c_4^3 W_2 + c_5^3 W_3\right)\\\scriptstyle
\left(a_0^4 + a_1^4 I_1 + a_2^4 I_2 + a_3^4 W_1 + a_4^4 W_2 + a_5^4 W_3\right)\cdot
\left(b_0^4 + b_1^4 I_1 + b_2^4 I_2 + b_3^4 W_1 + b_4^4 W_2 + b_5^4 W_3\right) &=
\scriptstyle
\left(c_0^4 + c_1^4 I_1 + c_2^4 I_2 + c_3^4 W_1 + c_4^4 W_2 + c_5^4 W_3\right)\\
\end{align*}
In what follows we write $L_{jubjub}$ for the associated language that consists of solutions to the R1CS.

To see that $L_{jubjub}$ is equivalent to $L_{tiny.jj.1}$, let $(I_1,I_2; W_1, W_2, W_3)$ be a word in $L_{jubjub}$, then $(I_1,I_2)$ is a word in $L_{tiny.jj.1}$, since the defining R1CS of $L_{jubjub}$ implies that $I_1$ and $I_2$ satisfy the Edwards equation of the tiny jubjub curve. On the other hand let $(I_1,I_2)$ be a word in $L_{tiny.jj.1}$. Then $(I_1,I_2; I_1^2, I_2^2, 8\cdot I_1^2\cdot I_2^2)$ is a word in $L_{jubjub}$ and both maps are inverse to each other.
\end{example}
\begin{exercise} Consider the language $L_{tiny.jj\_zk}$ and define a rank-1 constraint relation with decision function, such that the associated language is equivalent to $L_{tiny.jj\_zk}$.
\end{exercise} 
\paragraph{R1CS Satisfiability}To understand how rank-1 constraint systems define formal languages, oberserve that every R1CS over a fields $\F$ defines a decision function over the alphabet $\Sigma_I \times \Sigma_W = \F \times \F$ in the following way:
\begin{equation}
R_{R1CS} : \F^* \times \F^* \to \{true, false\}\;;\;
(I;W) \mapsto
\begin{cases}
true & (I;W) \text{ satisfies R1CS}\\
false & else
\end{cases}
\end{equation}
Every R1CS therefore defines a formal language. The grammar of this language is encoded in the constraints, words are solutions to the equations and  a \textbf{statement} is a knowledge claim "Given instance $I$, there is a witness $W$, such that $(I;W)$ is a solution to the rank-1 constraints system". A constructive proof to this claim is therefore an assignment of a field element to every witness variable, which is verfied whenever the set of all instance and witness variables solves the R1CS. 

\begin{remark}[R1CS satisfyability] It should be noted that in our definition, every R1CS defines its own language. However in more theoretical approaches another language usually called \textit{R1CS satisfyability} is often considered, which is useful when it comes to more abstract problems like expressiviness, or computational complexity of the class of \textit{all} R1CS. From our perspective the R1CS saisfyability language is optained by union of all R1CS languages that arie in our definition. To be more precise, let the alphabet $\Sigma=\F$ be a field. Then 
$$
L_{R1CS\_SAT(\F)} = \{(i;w)\in \Sigma^*\times \Sigma^*\;|\; \text{there is a R1CS $R$ such that } R(i;w)=true  \}
$$
\end{remark}
\begin{example}[3-Factorization]Consider the language $L_{3.fac\_zk}$ from example XXX and the R1CS defined in example XXX. As we have seen in XXX solutions to the R1CS are in 1:1 correspondense with solutions to the decision function of $L_{3.fac\_zk}$. Both languages are therefore equivalent in the sense that there is a 1:1 correspondence between words in both languages.

To give an intuition of how constructive proofs in $L_{3.fac\_zk}$ look like, consider the instance $I_1= 11$. To proof the statement "There exist a witness $W$, such that $(I_1;W)$ is a word in $L_{3.fac\_zk}$" constructively a proof has to privide assignments to all witness variables $W_1$, $W_2$, $W_3$ and $W_4$. Since the alphabet is $\F_{13}$, an example assignment is given by
$W=(2,3,4,6)$ since $(I_1;W)$ satisfies the R1CS
\begin{align*}
W_1 \cdot W_2 &= W_4 & \text{\# } 2\cdot 3 = 6\\
W_4 \cdot W_3 &= I_1 & \text{\# } 6\cdot 4 = 11
\end{align*}
A proper constructive proof is therefore given by $P=(2,3,4,6)$. Of course $P$ is not the only possible proof for this statement. Since factorization is in general not unique in a field, another constructive proof is given by $P'=(3,5,12,2)$. 
\end{example}
\begin{example}[The tiny jubjub curve] Consider the language $L_{jubjub}$ from example XXX and its associated R1CS. To see how constructive proofs in $L_{jubjub}$ look like, consider the instance $(I_1,I_2)= (11,6)$. To proof the statement "There exist a witness $W$, such that $(I_1,I_2;W)$ is a word in $L_{jubjub}$" constructively a proof has to privide assignments to all witness variables $W_1$, $W_2$ and $W_3$. Since the alphabet is $\F_{13}$, an example assignment is given by
$W=(4,10,8)$ since $(I_1,I_2;W)$ satisfies the R1CS
\begin{align*}
I_1 \cdot I_1 & = W_1 & 11\cdot 11 = 4\\
I_2 \cdot I_2 & = W_2 & 6 \cdot 6 = 10 \\
(8 \cdot W_1) \cdot W_2 & = W_3 & (8\cdot 4)\cdot 10 = 8\\
(12\cdot W_2 + W_3 + 10\cdot W_1 + 1)\cdot 1 & = 0 & 12\cdot 10 + 8 + 10\cdot 4 + 1 = 0
\end{align*}
A proper constructive proof is therefore given by $P=(4,10,8)$, which shows that the instance $(11,6)$ is a point on the tiny jubjub curve. 
\end{example}
\paragraph{Modularity} As we discuss in XXX, it is often useful to construct complex statements and their representing languages from simple ones. Rank-1 constraint systems are particulary useful for this as the intersection of two R1CS over the same alphabet results in a new R1CS over that same alphabet. 

To be more precise let $S_1$ and $S_2$ be two R1CS over $\F$, then a new R1CS $S_3$ is optained by the intersection $S_3 = S_1\cap S_2$  of $S_1$ and $S_2$, where in this context intersecion means, that both, the equations of $S_1$ \textit{and} the equations of $S_2$ have to be satisfied, in order to provide a solution for the system $S_3$.

As a consequence, developers are able to construct complex R1CS from simple ones and this modularity provides the theoretical foundation for many R1CS compilers as we will see in XXX.

\subsection{Algebraic Circuits} As we have seen in the previous paragraphs, rank-1 constraint systems are quadratic equations, such that solutions are knowledge proofs for the existence of words in associated languages. From the perspective of a proofer it is therefore important to solve those equations efficiently. 

However in contrast to systems of linear equation, no general methods are known that solve systems of quadratic equations efficiently. Rank-1 constraint systems are therefore impractical from a proofers perspective and auxilliary information is needed that helps to compute solutions efficiently.

Methods which compute R1CS solutions are sometimes called \textit{witness generator functions} and to provide a common example, we introduce another class of decision functions called \textit{algebraic circuits}. As we will see, every algebraic circuit defines an associated R1CS and moreover provides an efficient way to compute solutions for that R1CS.

It can be shown that every space and time bounded computation is expressible as an algebraic circuit and transforming high level computer programs into those circuits is a process often called \textit{flattening}. 

To understand this in more detail we will introduce our model for algebraic circuits and look at the concept of circuit execution and valid assignments. After that we will show how to derive rank-1 constraint systems from circuits and how circuits are useful to compute solutions to their R1CS efficiently.
\paragraph{Algebraic circuit representation} To see what algebraic circuits are, let $\F$ be a field. An algebraic circuit is then a directed acyclic (multi)graph that computes a polynomial function over $\F$. Nodes with only outgoing edges (source nodes) represent the variables and constants of the function and nodes with only incoming edges (sink nodes) represent the outcome of the function. All other nodes have exactly two incoming edges and represent the defining field operations \textit{addition} as well as \textit{multiplication}. Graph edges represent the flow of the computation along the nodes.

To be more precise, in this book a directed acyclic multi-graph $C(\F)$ is called an \textbf{algebraic circuit} over $\F$, if the following conditions hold:
\begin{itemize}
\item The set of edges has a total order.  
\item Every source node has a label, that represents either a variable or a constant from the field $\F$.
\item Every sink node has exactly one incoming edge and a label, that represents either a variable or a constant from the field $\F$.
\item Every node that is neither a source nor a sink has exactly two incoming edges and a label from the set $\{+,*\}$ that represents either addition or multiplication in $\F$.
\item All outgoing edges from a node have the same label.
\item Outgoing edges from a node with a label that represents a variable have a label.
\item Outgoing edges from a node with a label that represents multiplication have a label, if there is at least one labeled edge in both input path.
\item All incoming edges to sink nodes have a label.
\item If an edge has two labels $S_i$ and $S_j$ it gets a new label $S_i = S_j$.
\item No other edge has a label.
\item Incoming edges to sink nodes that are labeled with a constant $c\in\F$ are labeled with the same constant. Every other edge label is taken from the set $\{W,I\}$ and indexed compatible with the order of the edge set. 
\end{itemize} 
It should be noted that the details in the definitions of algbraic circuits vary between different sources. We use this definition as it is conceptually straight forward and well suited for pen and paper computations.

To get a better intuition of our definition, let $C(\F)$ be an algebraic circuit. Source nodes are the inputs to the circuit and either represent variables or constants. In a similar way sink nodes represent termination points of the circuit and are either output variables or constants. Constant sink nodes enforce computational outputs to take on certain values.  

Nodes that are neither source nodes nor sink nodes are called \textbf{aithmetic gates}. Arithmetic gates that are decorated with the "$+$"-label are called \textbf{addition-gates} and arithmetic gates that are decorated with the "$\cdot$"-label are called \textbf{multiplication-gates}. Every arithmetic gate has exactly two inputs, represented by the two incoming edges.

Since the set of edges is ordered we can write it as $\{E_1,E_2,\ldots, E_n\}$ for some $n\in \N$ and we use those indices to index the edge labels, too. Edge labels are therefore either constants or symbols like $I_j$, $W_j$ or $S_j$, where $j$ is an index compatible with the edge order. Labels $I_j$ represent instance variables, labels $W_j$ witness variables. Labels on the outgoing edges of input variables constrain the associated variable to that edge. Every other edge defines a constraining equation in the associated R1CS. we will explain this in more detail in XXX.
\begin{notation}
In synthezising algebraic circuits assigning instance $I_j$ or witness $W_j$ labels to appropriate edges is often the final step. It is therefore convinient to not distiguishe both types of edges in previous steps. To acknowledge for that we often simply write $S_j$ for an edge label, indicating that the private/public property of the label is unspecified and might either represent an instance or a witness label. 
\end{notation}
\begin{example}[Generalized factorization snark] To give a simple example of an algebraic circuit, consider our $3$-factorization problem from example XXX again.  To express the problem in the algebraic circuit model, consider the following function 
\[
f_{3.fac}:\mathbb{F}_{13}\times\mathbb{F}_{13}\times\mathbb{F}_{13}\to\mathbb{F}_{13};(x_{1},x_{2},x_{3})\mapsto x_{1}\cdot x_{2}\cdot x_{3}
\]
Using this function, we can describe the zero-knowledge $3$-factorization problem from XXX, in the following way: Given instance $I_1\in \F_{13}$ a valid witness is a preimage of $f_{3.fac}$ at the point $I_1$, i.e. a valid witness consists of three values $W_1$, $W_2$ and $W_3$ from $\F_{13}$, such that $f_{3.fac}(W_1,W_2,W_3)=I_1$. 

To see how this function can be transformed into an algebraic circuit over $\F_{13}$, it is a common first step to introduce brackets into the function's definition and then write the operations as binary operators, in order to highlight how exactly every field operation acts on its two inputs. Due to the associativitly laws in a field, we have different choices. We choose
\begin{align*}
f_{3.fac}(x_1,x_2,x_3) & = x_1\cdot x_2 \cdot x_3  & \text{\# bracket choice} \\
                       & = (x_1\cdot x_2 ) \cdot x_3  & \text{\# operator notation} \\
                       & = MUL(MUL(x_1,x_2),x_3)
\end{align*}
Using this expression we can write an associated algebraic circuit by first constraining the variables to edge labels $W_1=x_1$, $W_2=x_2$ and $W_3=x_3$ as well as $I_1=f_{3.fac}(x_1,x_2,x_3)$, taking the distinction between private and public inputs into account. We then rewrite the operator representation of $f_{3.fac}$ into circuit nodes and get: 
\begin{center}
% https://www.graphviz.org/pdf/dotguide.pdf
\digraph[scale=0.5]{G1}{
	forcelabels=true;
	//center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n3 [xlabel="W_2  "];
	n2 -> n3 [xlabel="  W_1"];
	n3 -> n5 [label="W_4"];
	n4 -> n5 [label=" W_3"];
	n5 -> n6 [label=" I_1"];
	n1 [shape=box, label="x_2"];
	n2 [shape=box, label="x_1"];
	n3 [label="*"];
	n4 [shape=box, label="x_3"];
	n5 [label="*"];
	n6 [shape=box, label="f_(3.fac_zk)"];
}
\end{center}
%\[
%\xymatrix{x_1\ar^{W_1}[dr] &  & x_2\ar_{W_2}[dl]\\
% & \cdot \ar^{W_4}[drr] &   & & x_3\ar_{W_3}[dl]\\
%  &  &  & \cdot\ar_{I_1}[d]\\
%  &  &  & f(x_1,x_2,x_3)
%}
%\]


In this case the directed acyclic multi-graph, is a binary tree with three leaves (the source nodes) labeled by $x_1$, $x_2$ and $x_3$, one root (the single sink node) labeled by $f(x_1,x_2,x_3)$ and two internal nodes, which are labeled as multiplication gates. 

The order we used to label the edges is choosen to make the edge labeling consistent with the choice of $W_4$ as defined in example XXX. This order can for example be optained by deapth-first right-to-left-first traversal algorithm.
\end{example}
\begin{example} To give a more realistic example of an algebraic circuit look at the defining equation XXX of the tiny-jubjub curve again. A pair of field elements 
$(x,y)\in \F_{13}^2$ is a curve point, precisely if
$$
3\cdot x^2 + y^2 = 1+ 8\cdot x^2\cdot y^2
$$ 
To understand how one might transform this identity into an algebraic circuit, we first rewrite this equation by shifting all terms to the right. We get:
\begin{align*}
3\cdot x^2 + y^2 & = 1+ 8\cdot x^2\cdot y^2 & \Leftrightarrow\\
0 & = 1+ 8\cdot x^2\cdot y^2 - 3\cdot x^2 - y^2 & \Leftrightarrow\\
0 & = 1+ 8\cdot x^2\cdot y^2 + 10\cdot x^2 +12\cdot y^2
\end{align*}
Then we use this expression to define a function, such that all points of the tiny-jubjub curve are characterized as the functions preimages at $0$.
$$
f_{tiny-jj}: \F_{13}\times \F_{13}\to \F_{13}\; ; \;
(x,y)\mapsto 1+ 8\cdot x^2\cdot y^2 + 10\cdot x^2 +12\cdot y^2
$$
Every pair of points $(x,y)\in \F_{13}^2$ with $f_{tiny-jj}(x,y)=0$ is a point on the tiny-jubjub curve and there are no other curve points. The preimage $f_{tiny-jj}^{-1}(0)$ is therefore a complete description of the tiny-jubjub curve.

We can transform this function into an algebraic circuit over $\F_{13}$. We first introduce brackets into potentially ambigious expressions and then rewrite the function in terms of binary operators. We get
\begin{align*}
f_{tiny-jj}(x,y) & = 1+ 8\cdot x^2\cdot y^2 + 10\cdot x^2 +12 y^2  & \Leftrightarrow\\
 & = ((8\cdot ((x\cdot x)\cdot (y\cdot y))) + (1+ 10\cdot (x\cdot x))) + (12\cdot(y\cdot y))  & \Leftrightarrow\\
  & = \scriptstyle ADD(ADD(MUL(8,MUL(MUL(x,x),MUL(y,y))), ADD(1,MUL(10,MUL(x,x)))),MUL(12,MUL(y,y)))
\end{align*}
Since we haven't decided which part of the computation should be public and which part should be private, we use the unspecified symbol $S$ to represent edge labels. Constraining all variables to edge labels $S_1=x$, $S_2=y$ and $S_6=f_{tiny-jj}$ we get the following circuit, representing the function $f_{tiny-jj}$, by inductively replaving binary operators with their associated arithmetic gates:
\begin{center}
% https://www.graphviz.org/pdf/dotguide.pdf
\digraph[scale=0.4]{G2}{
	forcelabels=true;
	//center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n4 [xlabel="S_1" /*, color=lightgray */];
        n1 -> n4 /*[ color=lightgray ]*/
	n2 -> n5 [xlabel="S_2" /*, color=lightgray */];
	n2 -> n5 /*[ color=lightgray ]*/ ;
	n3 -> n8 /*[ color=lightgray ]*/ ;
	n4 -> n8 [taillabel="S_3", labeldistance="2" /*, color=lightgray */];
	n4 -> n10 [taillabel="S_3", labeldistance="4" /*, color=lightgray */];
	n5 -> n10 [xlabel="S_4" /*, color=lightgray */];
	n5 -> n13 [xlabel="S_4" /*, color=lightgray */];
	n6 -> n13 /*[ color=lightgray ]*/ ;
	n7 -> n11 /*[ color=lightgray ]*/ ;
	n8 -> n11 /*[ color=lightgray ]*/ ;
	n9 -> n12 /*[ color=lightgray ]*/ ;
	n10 -> n12 [taillabel="S_5", labeldistance="4" /*, color=lightgray */];
	n11 -> n14 /*[ color=lightgray ]*/ ;
	n12 -> n14 /*[ color=lightgray ]*/ ;	
	n13 -> n15 /*[ color=lightgray ]*/ ;
	n14 -> n15 /*[ color=lightgray ]*/ ;
	n15 -> n16 [label="  S_6", labeldistance="2" /*, color=lightgray */];
	n1 [shape=box, label="x" /*, color=lightgray */];
	n2 [shape=box, label="y" /*, color=lightgray */];
	n3 [shape=box, label="10" /*, color=lightgray */];
	n4 [label="*" /*, color=lightgray */];
	n5 [label="*" /*, color=lightgray */];
	n6 [shape=box, label="12" /*, color=lightgray */];
	n7 [shape=box, label="1" /*, color=lightgray */];
	n8 [label="*" /*, color=lightgray */];
	n9 [shape=box, label="8" /*, color=lightgray */];
	n10 [label="*" /*, color=lightgray */];
	n11 [label="+" /*, color=lightgray */];
	n12 [label="*" /*, color=lightgray */];	
	n13 [label="*" /*, color=lightgray */];
	n14 [label="+" /*, color=lightgray */];
	n15 [label="+" /*, color=lightgray */];
	n16 [shape=box, label="f_tiny-jj" /*, color=lightgray */];		
}
\end{center}
This circuit is not a graph, but a multigraph, since there are more then one edge between some of the nodes. 

In the designing process of circuits from functions, it should be noted that circuit representations are not unique in general. In case of function $f_{tiny-jj}$, the circuit shape is dependent on our choice of bracketing in XXX. An alernative design is for example given by the following circuit, which occures when the bracketed expression $8\cdot ( (x\cdot x) \cdot (y\cdot y) )$ is replaced by the expression $(x\cdot x) \cdot ( 8 \cdot (y\cdot y) )$. 
\begin{center}
\digraph[scale=0.4]{G2A}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n4 [label="S_1" /* comment*/ labeldistance="4" /*, color=lightgray */];
        n1 -> n4 /*[ color=lightgray ]*/
	n2 -> n6 [label="S_2" /*, color=lightgray */];
	n2 -> n6 /*[ color=lightgray ]*/ ;
	n3 -> n9 /*[ color=lightgray ]*/ ;
	n4 -> n9 [taillabel="S_3", labeldistance="2" /*, color=lightgray */];
	n4 -> n12 [taillabel="S_3", labeldistance="4" /*, color=lightgray */];
	n5 -> n10 /*[ color=lightgray ]*/ ;
	n6 -> n10 [headlabel="S_4", labeldistance="4" /*, color=lightgray */];
	n6 -> n13 [taillabel="S_4", labeldistance="4" /*, color=lightgray */];
	n7 -> n13 /*[ color=lightgray ]*/ ;
	n8 -> n11 /*[ color=lightgray ]*/ ;
	n9 -> n11 /*[ color=lightgray ]*/ ;
	n10 -> n12 /*[ color=lightgray ]*/ ; 
	n11 -> n14 /*[ color=lightgray ]*/ ;
	n12 -> n14 [xlabel="S_5  "  /*, color=lightgray */];	
	n13 -> n15 /*[ color=lightgray ]*/ ;
	n14 -> n15 /*[ color=lightgray ]*/ ;
	n15 -> n16 [label="  S_6", labeldistance="2" /*, color=lightgray */];
	n1 [shape=box, label="x" /*, color=lightgray */];
	n2 [shape=box, label="y" /*, color=lightgray */];
	n3 [shape=box, label="10" /*, color=lightgray */];
	n4 [label="*" /*, color=lightgray */];
	n5 [shape=box, label="8" /*, color=lightgray */];
	n6 [label="*" /*, color=lightgray */];
	n7 [shape=box, label="12" /*, color=lightgray */];
	n8 [shape=box, label="1" /*, color=lightgray */];
	n9 [label="*" /*, color=lightgray */];
	n10 [label="*" /*, color=lightgray */];
	n11 [label="+" /*, color=lightgray */];	
	n12 [label="*" /*, color=lightgray */];	
	n13 [label="*" /*, color=lightgray */];
	n14 [label="+" /*, color=lightgray */];
	n15 [label="+" /*, color=lightgray */];
	n16 [shape=box, label="f_tiny-jj" /*, color=lightgray */];		
}
\end{center}
Of course both circuits represent the same function, due to the associativity and commutativity laws that hold true in any field.

With a circuit that represents the function $f_{tiny-jj}$, we can now proceed to derive a circuit that constrains arbitray pairs $(x,y)$ of field elements to be points on the tiny-jubjub curve. To do so, we have to constrain the output to be zero, that is we have to constrain $S_6=0$. To indicate this in the circuit we replace the output variable by the constant $0$ and constrain the related edge label accordingly. We get 
\begin{center}
\digraph[scale=0.4]{G2AJA}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n4 [label="S_1" /* comment*/ labeldistance="4" /*, color=lightgray */];
        n1 -> n4 /*[ color=lightgray ]*/
	n2 -> n6 [label="S_2" /*, color=lightgray */];
	n2 -> n6 /*[ color=lightgray ]*/ ;
	n3 -> n9 /*[ color=lightgray ]*/ ;
	n4 -> n9 [taillabel="S_3", labeldistance="2" /*, color=lightgray */];
	n4 -> n12 [taillabel="S_3", labeldistance="4" /*, color=lightgray */];
	n5 -> n10 /*[ color=lightgray ]*/ ;
	n6 -> n10 [headlabel="S_4", labeldistance="4" /*, color=lightgray */];
	n6 -> n13 [taillabel="S_4", labeldistance="4" /*, color=lightgray */];
	n7 -> n13 /*[ color=lightgray ]*/ ;
	n8 -> n11 /*[ color=lightgray ]*/ ;
	n9 -> n11 /*[ color=lightgray ]*/ ;
	n10 -> n12 /*[ color=lightgray ]*/ ; 
	n11 -> n14 /*[ color=lightgray ]*/ ;
	n12 -> n14 [xlabel="S_5  "  /*, color=lightgray */];	
	n13 -> n15 /*[ color=lightgray ]*/ ;
	n14 -> n15 /*[ color=lightgray ]*/ ;
	n15 -> n16 [label="  S_6=0", labeldistance="2" /*, color=lightgray */];
	n1 [shape=box, label="x" /*, color=lightgray */];
	n2 [shape=box, label="y" /*, color=lightgray */];
	n3 [shape=box, label="10" /*, color=lightgray */];
	n4 [label="*" /*, color=lightgray */];
	n5 [shape=box, label="8" /*, color=lightgray */];
	n6 [label="*" /*, color=lightgray */];
	n7 [shape=box, label="12" /*, color=lightgray */];
	n8 [shape=box, label="1" /*, color=lightgray */];
	n9 [label="*" /*, color=lightgray */];
	n10 [label="*" /*, color=lightgray */];
	n11 [label="+" /*, color=lightgray */];	
	n12 [label="*" /*, color=lightgray */];	
	n13 [label="*" /*, color=lightgray */];
	n14 [label="+" /*, color=lightgray */];
	n15 [label="+" /*, color=lightgray */];
	n16 [shape=box, label="0" /*, color=lightgray */];		
}
\end{center}
The previous circuit enforces input values assigned to the labels $S_1$ and $S_2$ to be points on the tiny jubjub curve. However it does not specify which labels are considered public and which are considered private. The following circuit defines the inputs to be public, while all other labels are private:
\begin{center}
\digraph[scale=0.4]{G2AJA2}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n4 [label="I_1" /* comment*/ labeldistance="4" /*, color=lightgray */];
        n1 -> n4 /*[ color=lightgray ]*/
	n2 -> n6 [label="I_2" /*, color=lightgray */];
	n2 -> n6 /*[ color=lightgray ]*/ ;
	n3 -> n9 /*[ color=lightgray ]*/ ;
	n4 -> n9 [taillabel="W_1", labeldistance="2" /*, color=lightgray */];
	n4 -> n12 [taillabel="W_1", labeldistance="4" /*, color=lightgray */];
	n5 -> n10 /*[ color=lightgray ]*/ ;
	n6 -> n10 [headlabel="W_2", labeldistance="4" /*, color=lightgray */];
	n6 -> n13 [taillabel="W_2", labeldistance="4" /*, color=lightgray */];
	n7 -> n13 /*[ color=lightgray ]*/ ;
	n8 -> n11 /*[ color=lightgray ]*/ ;
	n9 -> n11 /*[ color=lightgray ]*/ ;
	n10 -> n12 /*[ color=lightgray ]*/ ; 
	n11 -> n14 /*[ color=lightgray ]*/ ;
	n12 -> n14 [xlabel="W_3  "  /*, color=lightgray */];	
	n13 -> n15 /*[ color=lightgray ]*/ ;
	n14 -> n15 /*[ color=lightgray ]*/ ;
	n15 -> n16 [label="  0", labeldistance="2" /*, color=lightgray */];
	n1 [shape=box, label="x" /*, color=lightgray */];
	n2 [shape=box, label="y" /*, color=lightgray */];
	n3 [shape=box, label="10" /*, color=lightgray */];
	n4 [label="*" /*, color=lightgray */];
	n5 [shape=box, label="8" /*, color=lightgray */];
	n6 [label="*" /*, color=lightgray */];
	n7 [shape=box, label="12" /*, color=lightgray */];
	n8 [shape=box, label="1" /*, color=lightgray */];
	n9 [label="*" /*, color=lightgray */];
	n10 [label="*" /*, color=lightgray */];
	n11 [label="+" /*, color=lightgray */];	
	n12 [label="*" /*, color=lightgray */];	
	n13 [label="*" /*, color=lightgray */];
	n14 [label="+" /*, color=lightgray */];
	n15 [label="+" /*, color=lightgray */];
	n16 [shape=box, label="0" /*, color=lightgray */];		
}
\end{center} 
\end{example}
It can be shown that every space and time bounded computation can be transformed into an algebraic circuit. We call any process that transforms a bounded computation into a circuit \textbf{flattening}. 
\paragraph{Circuit Execution} Algebraic circuits are directed, acyclic multi-graphs, where nodes represent variables, constants, or addition and multiplication gates. In particular every algebraic circuit with $n$ input nodes decorated with variable symbols and $m$ output nodes decorated with variables, can be seen a function that transforms an input tuple $(x_1,\ldots, x_n)$ from $\F^n$ into an output tupel $(f_1,\ldots,f_m)$ from $\F^m$. The transformation is done by sending values associated to nodes along their outgoing edges to other nodes. If those nodes are gates, then the values are transformed according to the gates label and the process is repeated along all edges until a sink node is reached. We call this computation \textbf{circuit execution}.

Executing a circuit, it is possible to not only compute the output values of the circuit but to derive field elements for all edges and in particulr for all edge labels in the circuit. The result is a tupel $(S_1,S_2,\ldots, S_n)$ of field elements associated to all labeled edges, which we call a \textbf{valid asignment} to the circuit. In contrast any assignment $(S'_1,S'_2,\ldots, S'_n)$ of field elements to edge labels, that can not arise from circuit execution is called an \textbf{invalid assignment}.

Valid asignments can be interpreted as \textit{proofs for proper circuit execution} because they keep record of the computational result as well as intermediate computational steps. 
\begin{example}[3-factorization] Consider the $3$-factorization problem from example XXX and its representation as an algebraic circuit from XXX. We know that the set of edge labels is given by $S:=\{I_{1};W_{1},W_{2},W_{3}, W_{4}\}$. 

To understand how this circuit is executed, consider the variables $x_1=2$, $x_2=3$ as well as $x_3=4$. Following all edges in the graph, we get the assignments $W_1=2$, $W_2=3$ and $W_3=4$. Then the assigments of $W_1$ and $W_2$ enter a multiplication gate and the output of the gate is $2\cdot 3 = 6$, which we assign to $W_4$, i.e. $W_4=6$. The values $W_4$ and $W_3$ then enter the second multiplication gate and the output of the gate is $6\cdot 4 = 11$, which we assign to $I_1$, i.e. $I_1=11$. 

A valid assignment to the 3-factorization circuit $C_{3.fac}(\F_{13})$ is therefore given by the set $S_{valid}:=\{11;2,3,4,6\}$. We can picture this assignment in the circuit as follows:
\begin{center}
% https://www.graphviz.org/pdf/dotguide.pdf
\digraph[scale=0.5]{G3}{
	forcelabels=true;
	//center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n3 [xlabel="W_2=3  "];
	n2 -> n3 [xlabel="W_1=2 "];
	n3 -> n5 [label="W_4=6"];
	n4 -> n5 [label=" W_3=4"];
	n5 -> n6 [label=" I_1=11"];
	n1 [shape=box, label="x_2"];
	n2 [shape=box, label="x_1"];
	n3 [label="*"];
	n4 [shape=box, label="x_3"];
	n5 [label="*"];
	n6 [shape=box, label="f_(3.fac_zk)"];
}
\end{center}
To see how an invalid assignment looks like, consider the assignment $S_{err}:=\{8;2,3,4,7\}$. In this assignment the input values are the same as in the previous case. The associated circuit is:
\begin{center}
% https://www.graphviz.org/pdf/dotguide.pdf
\digraph[scale=0.5]{G4}{
	forcelabels=true;
	//center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n3 [xlabel="W_2=3  "];
	n2 -> n3 [xlabel="W_1=2 "];
	n3 -> n5 [label="W_4=7"];
	n4 -> n5 [label=" W_3=4"];
	n5 -> n6 [label=" I_1=8"];
	n1 [shape=box, label="x_2"];
	n2 [shape=box, label="x_1"];
	n3 [label="*"];
	n4 [shape=box, label="x_3"];
	n5 [label="*"];
	n6 [shape=box, label="f_(3.fac_zk)"];
}
\end{center}
This assignment is invalid as the assignments of $I_1$ and $W_4$ can not be obtained by executing the circuit.
\end{example}
\begin{example} To compute a more realistic algebraic circuit execution, consider the defining circuit $C_{tiny-jj}(\F_{13})$ from example XXX again. We already know from the way this circuit is constructed that any valid assignment with $S_1=x$, $S_2=y$ and $S_6=0$ will ensure that the pair $(x,y)$ is a point on the tiny jubjub curve XXX in its Edwards representation. 

From example XXX we know that the pair $(11,6)$ is a proper point on the tiny-jubjub curve and we use this point as input to a circuit execution. We get:
\begin{center}
% https://www.graphviz.org/pdf/dotguide.pdf
\digraph[scale=0.4]{G2C}{
	forcelabels=true;
	//center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n4 [label="S_1=11" labeldistance="4" /*, color=lightgray */];
        n1 -> n4 /*[ color=lightgray ]*/
	n2 -> n5 [label="S_2=6" /*, color=lightgray */];
	n2 -> n5 /*[ color=lightgray ]*/ ;
	n3 -> n8 /*[ color=lightgray ]*/ ;
	n4 -> n8 [taillabel="S_3=4  " /*, color=lightgray */];
	n4 -> n10 [taillabel="S_3=4", labeldistance="4" /*, color=lightgray */];
	n5 -> n10 [xlabel="S_4=10 " /*, color=lightgray */];
	n5 -> n13 [headlabel="S_4=10", labeldistance="4" /*, color=lightgray */];
	n6 -> n13 /*[ color=lightgray ]*/ ;
	n7 -> n11 /*[ color=lightgray ]*/ ;
	n8 -> n11 [headlabel="[10*4=1]    "];
	n9 -> n12 /*[ color=lightgray ]*/ ;
	n10 -> n12 [taillabel="S_5=1  " /*, color=lightgray */];
	n11 -> n14 [headlabel="[1+1=2]    "];
	n12 -> n14 [label="  [8*1=8]"];	
	n13 -> n15 [headlabel="    [10*12=3]"];
	n14 -> n15 [taillabel="   [2+8=10]"];
	n15 -> n16 [label="  S_6=0", labeldistance="2" /*, color=lightgray */];
	n1 [shape=box, label="x" /*, color=lightgray */];
	n2 [shape=box, label="y" /*, color=lightgray */];
	n3 [shape=box, label="10" /*, color=lightgray */];
	n4 [label="*" /*, color=lightgray */];
	n5 [label="*" /*, color=lightgray */];
	n6 [shape=box, label="12" /*, color=lightgray */];
	n7 [shape=box, label="1" /*, color=lightgray */];
	n8 [label="*" /*, color=lightgray */];
	n9 [shape=box, label="8" /*, color=lightgray */];
	n10 [label="*" /*, color=lightgray */];
	n11 [label="+" /*, color=lightgray */];
	n12 [label="*" /*, color=lightgray */];	
	n13 [label="*" /*, color=lightgray */];
	n14 [label="+" /*, color=lightgray */];
	n15 [label="+" /*, color=lightgray */];
	n16 [shape=box, label="0" /*, color=lightgray */];		
}
\end{center}
Executing the circuit we indeed compute $S_6=0$ as expected, which proofs that $(11,6)$ is a point on the tiny-jubjub curve in its Edwards representation. A valid assignment of $C_{tiny-jj}(\F_{13})$ is therefore given by 
$$
S_{tiny-jj} = \{S_1, S_2, S_3, S_4, S_5, S_6\} = \{11, 6, 4, 10, 1, 0\}
$$
\end{example}
\paragraph{Circuit Satisfyability} To understand how algebraic circuits give rise to formal languages, oberserve that every algebraic circuit $C(\F)$ over a fields $\F$ defines a decision function over the alphabet $\Sigma_I \times \Sigma_W = \F \times \F$ in the following way:
\begin{equation}
R_{C(\F)} : \F^* \times \F^* \to \{true, false\}\;;\;
(I;W) \mapsto
\begin{cases}
true & (I;W) \text{is valid assignment to } C(\F)\\
false & else
\end{cases}
\end{equation}
Every algebraic circuit therefore defines a formal language. The grammar of this language is encoded in the shape of the circuit, words are assigments to edge label that a derived from circuit execution and \textbf{statements} are knowledge claims "Given instance $I$, there is a witness $W$, such that $(I;W)$ is a valid assignment to the circuit". A constructive proof to this claim is therefore an assignment of a field element to every witness variable, which is verfied by executing the circuit to see if the assignment of the execution meets the assignment of the proof. 

In the context of zero knowledge proofing systems, executing circuits is also often called \textbf{witness generation}, since in apllications the instance part is usually public, while its the task of a proofer to compute the witness part.

\begin{remark}[Circuit satisfyability] It should be noted that in our definition, every circuit defines its own language. However in more theoretical approaches another language usually called \textit{circuit satisfyability} is often considered, which is useful when it comes to more abstract problems like expressiviness, or computational complexity of the class of \textit{all} algebraic circuit over a given field. From our perspective the circuit saisfyability language is optained by union of all circuit languages that arie in our definition. To be more precise, let the alphabet $\Sigma=\F$ be a field. Then 
$$
L_{CIRCUIT\_SAT(\F)} = \{(i;w)\in \Sigma^*\times \Sigma^*\;|\; \text{there is a circuit } C(\F) \text{ such that } (i;w) \text{ is valid assignment}\}
$$
\end{remark}
\begin{example}[3-Factorization]Consider the circuit $C_{3.fac}$ from example XXX again. We call the associated language $L_{3.fac\_circ}$.

To understand how a constructive proof of a statement in $L_{3.fac\_circ}$ looks like, consider the instance $I_1= 11$. To provide a proof for the statement "There exist a witness $W$, such that $(I_1;W)$ is a word in $L_{3.fac\_circ}$" a proof therefore has to consists of proper values for the variables $W_1$, $W_2$, $W_3$ and $W_4$. Any proofer therefore has to find input values for $W_1$, $W_2$ and $W_3$ and then execute the circuit to compute $W_4$ under the assumption $I_1=11$. 

Example XXX implies that $(2,3,4,6)$ is a proper constructive proof and in order to verify the proof a verifier needs to execute the circuit with instance $I_1=11$ and inputs $W_1=2$, $W_2=3$ and $W_3=4$ to decide whether the proof is a valid assignment or not. 
\end{example}
\paragraph{Associated Constraint Systems} As we have seen in XXX, rank-1 constraint systems defines a way to represent statements in terms of a system of quadratic equations over finite fields, suiteable for pairing based zero-knowledge proofing systems. However those equations provide no practical way for a proofer to actually compute a solution. On the other hand algebraic circuits can be executed in order to derive valid assignments efficiently. 

In this paragraph we show how to transform any algebraic circuit into a rank-1 constraint systems, such that valid circuit assignments are in 1:1 correspondence with solutions to the associated R1CS. 

To see this, let $C(\F)$ be an algebraic circuit over a finite field $\F$, with a set of edge labels $\{S_1,S_2,\ldots, S_n\}$. Then one of the following steps is executed for every edge label $S_j$ from that set:
\begin{itemize}
\item If the edge label $S_j$ is an outgoing edge of a multiplication gate, the R1CS gets a new quadratic constraint
\begin{equation}
(\text{left input})\cdot (\text{right input}) = S_j
\end{equation}  
where $(\text{left input})$ respectively $(\text{right input})$ is the output from the symbolic execution of the subgraph that consists of the left respectively right input edge of this gate and all edges and nodes that have this edge in their path, starting with constant inputs or labeled outgoing edges of other nodes.
\item If the edge label $S_j$ is an outgoing edge of an addition gate, the R1CS gets a new quadratic constraint
\begin{equation}
(\text{left input} + \text{right input})\cdot 1 = S_j
\end{equation}  
where $(\text{left input})$ respectively $(\text{right input})$ is the output from the symbolic execution of the subgraph that consists of the left respectively right input edge of this gate and all edges and nodes that have this edge in their path, starting with constant inputs or labeled outgoing edges of other nodes.
\item No other edge label adds a constraints to the system.
\end{itemize}
The result of this method is a rank-1 constraints system and in this sense every algebraic circuit $C(\F)$ generates a R1CS $R$, which we call the \textbf{associated R1CS} of the circuit. It can be shown, that a tupel of field elements $(S_1,S_2,\ldots, S_n)$ is a valid assignment to a circuit, if and only if the same tuplel is a solution to the associated R1CS. Circuit executions therefore compute solutions to rank-1 constraints systems efficiently. 

To understand the contribution of algebraic gates to the number of constraints, note that
by definition multiplication gates have labels on their outgoing edges, if and only if
there is at least one labeled edge in both input path, or if the outgoing edge is an input to a sink node. This implies that multiplication with a constant is essentially free in the sense that it doesn't add a new constraint to the system, as long as that multiplication gate is not am input to an output node. 

Moreover addition gates have labels on their outgoing edges, if and only if they are inputs to sink nodes. This implies that addition is essentially free in the sense that it doesn't add a new constraint to the system, as long as that addition gate is not an input to an output node. 
\begin{example}[$3$-factorization] Consider our $3$-factorization problem from example XXX and the associated circuit $C_{3.fac}(\F_{13})$. Out task is to transform this circuit into an equivalent rank-1 constraint system.
\begin{center}
\digraph[scale=0.5]{G1}{
	forcelabels=true;
	//center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n3 [xlabel="W_2  "];
	n2 -> n3 [xlabel="  W_1"];
	n3 -> n5 [label="W_4"];
	n4 -> n5 [label=" W_3"];
	n5 -> n6 [label=" I_1"];
	n1 [shape=box, label="x_2"];
	n2 [shape=box, label="x_1"];
	n3 [label="*"];
	n4 [shape=box, label="x_3"];
	n5 [label="*"];
	n6 [shape=box, label="f_(3.fac_zk)"];
}
\end{center}
We start with an empty R1CS and in order to generate all constraints, we have to iterate over the set of edge labels $\{I_1;W_1,W_2,W_3,W_4\}$. 

Starting with the edge label $I_1$, we see that it is an outgoing edge of a multiplication gate and since both input edges are labeled, we  have to add the following constraint to the system:
\begin{align*}
(\text{left input})\cdot (\text{right input})  &= I_1 & \Leftrightarrow\\
W_4\cdot W_3  &= I_1
\end{align*}
Next we consider the edge label $W_1$ and since its not an outgoing edge of a multiplication or addition label, we don't add a constraint to the system. The same holds true for the labels $W_2$ and $W_3$. 

For edge label $W_4$ , we see that it is an outgoing edge of a multiplication gate and since both input edges are labeled, we have to add the following constraint to the system:
\begin{align*}
(\text{left input})\cdot (\text{right input})  &= W_4 & \Leftrightarrow\\
W_2\cdot W_1  &= W_4
\end{align*} 
Since there are no more labeled edges, all constraints are generated and we have to combine them to get the associated R1CS of $C_{3.fac}(\F_{13})$: 
\begin{align*}
 W_4\cdot W_3 & = I_1\\
 W_2\cdot W_1 & = W_4
\end{align*}
This system is equivalent to the R1CS we derived in example XXX. The languages $L_{3.fac\_zk}$ and $L_{3.fac\_circ}$ are therefore equivalent and both the circuit as well as the R1CS are just two different ways of expressing the same language.
\end{example}
\begin{example} To consider a more general transformation, we consider the tiny-jubjub circuit from example XXX  again. A proper circuit is given by
\begin{center}
\digraph[scale=0.4]{G2D}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n4 [label="S_1" labeldistance="4" /*, color=lightgray */];
        n1 -> n4 /*[ color=lightgray ]*/
	n2 -> n6 [label="S_2" /*, color=lightgray */];
	n2 -> n6 /*[ color=lightgray ]*/ ;
	n3 -> n9 /*[ color=lightgray ]*/ ;
	n4 -> n9 [taillabel="S_3", labeldistance="2" /*, color=lightgray */];
	n4 -> n13 [taillabel="S_3", labeldistance="4" /*, color=lightgray */];
	n5 -> n10 /*[ color=lightgray ]*/ ;
	n6 -> n10 [headlabel="S_4", labeldistance="4" /*, color=lightgray */];
	n6 -> n11 [taillabel="S_4", labeldistance="4" /*, color=lightgray */];
	n7 -> n11 /*[ color=lightgray ]*/ ;
	n8 -> n12 /*[ color=lightgray ]*/ ;
	n9 -> n12 /*[ color=lightgray ]*/ ;
	n10 -> n13 /*[ color=lightgray ]*/ ; 
	n11 -> n15 /*[ color=lightgray ]*/ ;
	n12 -> n14 /*[ color=lightgray ]*/ ;	
	n13 -> n14 [xlabel="S_5  "  /*, color=lightgray */];
	n14 -> n15 /*[ color=lightgray ]*/ ;
	n15 -> n16 [label="  S_6=0", labeldistance="2" /*, color=lightgray */];
	n1 [shape=box, label="x" /*, color=lightgray */];
	n2 [shape=box, label="y" /*, color=lightgray */];
	n3 [shape=box, label="10" /*, color=lightgray */];
	n4 [label="*", style=filled /*, color=lightgray */];
	n5 [shape=box, label="8" /*, color=lightgray */];
	n6 [label="*", style=filled /*, color=lightgray */];
	n7 [shape=box, label="12" /*, color=lightgray */];
	n8 [shape=box, label="1" /*, color=lightgray */];
	n9 [label="*" /*, color=lightgray */];
	n10 [label="*" /*, color=lightgray */];
	n11 [label="*" /*, color=lightgray */];	
	n12 [label="+" /*, color=lightgray */];	
	n13 [label="*", style=filled /*, color=lightgray */];
	n14 [label="+" /*, color=lightgray */];
	n15 [label="+", style=filled /*, color=lightgray */];
	n16 [shape=box, label="0" /*, color=lightgray */];		
}
\end{center}
To compute the number of constraints, observe that we have $3$ multiplication gates, that have labeles on their outgoing edges and $1$ addition gate that has a label on its outgoing edge. We therefore have to cmpute $4$ quadratic constraints. 

In order to derive the associated R1CS, we have start with an empty R1CS and then iterate over the set $\{S_1,S_2,S_3,S_4,S_5,S_6=0\}$ of all edge label, in order to generate the constraints. 

Considering edge label $S_1$, we see that the associated edges are not outgoing edges of any algebraic gate and we therefore have to add no new constraint to the system. The same holds true for edge label $S_2$. Looking at edge label $S_3$, we see that the associated edges are outgoing edges of a multiplication gate and that the associated subgraph is given by:
\begin{center}
\digraph[scale=0.4]{G2E}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n4 [label="S_1" labeldistance="4" /*, color=lightgray */];
    n1 -> n4 /*[ color=lightgray ]*/
	n2 -> n6 [label="S_2", color=lightgray];
	n2 -> n6 [ color=lightgray ] ;
	n3 -> n9 [ color=lightgray ] ;
	n4 -> n9 [taillabel="S_3", labeldistance="2" /*, color=lightgray */];
	n4 -> n13 [taillabel="S_3", labeldistance="4" /*, color=lightgray */];
	n5 -> n10 [ color=lightgray ] ;
	n6 -> n10 [headlabel="S_4", labeldistance="4", color=lightgray];
	n6 -> n11 [taillabel="S_4", labeldistance="4", color=lightgray];
	n7 -> n11 [ color=lightgray ] ;
	n8 -> n12 [ color=lightgray ] ;
	n9 -> n12 [ color=lightgray ] ;
	n10 -> n13 [ color=lightgray ] ; 
	n11 -> n15 [ color=lightgray ] ;
	n12 -> n14 [ color=lightgray ] ;	
	n13 -> n14 [xlabel="S_5  ", color=lightgray];
	n14 -> n15 [ color=lightgray ] ;
	n15 -> n16 [label="  S_6=0", labeldistance="2", color=lightgray];
	n1 [shape=box, label="x" /*, color=lightgray */];
	n2 [shape=box, label="y", color=lightgray];
	n3 [shape=box, label="10", color=lightgray ];
	n4 [label="*", style=filled /*, color=lightgray */];
	n5 [shape=box, label="8", color=lightgray];
	n6 [label="*", style=filled, color=lightgray];
	n7 [shape=box, label="12", color=lightgray];
	n8 [shape=box, label="1", color=lightgray];
	n9 [label="*", color=lightgray];
	n10 [label="*", color=lightgray];
	n11 [label="*", color=lightgray];	
	n12 [label="+", color=lightgray];	
	n13 [label="*", style=filled, color=lightgray];
	n14 [label="+", color=lightgray];
	n15 [label="+", style=filled, color=lightgray];
	n16 [shape=box, label="0", color=lightgray];		
}
\end{center}
Both the left and the right input to this multiplication gate are labeled by $S_1$. We therefore have to add the following constraint to the system:
$$
S_1 \cdot S_1 = S_3
$$
Looking at edge label $S_4$, we see that the associated edges are outgoing edges of a multiplication gate and that the associated subgraph is given by:
\begin{center}
\digraph[scale=0.4]{G2F}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n4 [label="S_1" labeldistance="4", color=lightgray];
        n1 -> n4 [ color=lightgray ]
	n2 -> n6 [label="S_2" /*, color=lightgray */];
	n2 -> n6 /* [ color=lightgray ] */ ;
	n3 -> n9 [ color=lightgray ] ;
	n4 -> n9 [taillabel="S_3", labeldistance="2", color=lightgray];
	n4 -> n13 [taillabel="S_3", labeldistance="4", color=lightgray];
	n5 -> n10 [ color=lightgray ] ;
	n6 -> n10 [headlabel="S_4", labeldistance="4" /*, color=lightgray */];
	n6 -> n11 [taillabel="S_4", labeldistance="4" /*, color=lightgray */];
	n7 -> n11 [ color=lightgray ] ;
	n8 -> n12 [ color=lightgray ] ;
	n9 -> n12 [ color=lightgray ] ;
	n10 -> n13 [ color=lightgray ] ; 
	n11 -> n15 [ color=lightgray ] ;
	n12 -> n14 [ color=lightgray ] ;	
	n13 -> n14 [xlabel="S_5  ", color=lightgray];
	n14 -> n15 [ color=lightgray ] ;
	n15 -> n16 [label="  S_6=0", labeldistance="2", color=lightgray];
	n1 [shape=box, label="x", color=lightgray];
	n2 [shape=box, label="y", /* color=lightgray */];
	n3 [shape=box, label="10", color=lightgray];
	n4 [label="*", style=filled, color=lightgray];
	n5 [shape=box, label="8", color=lightgray];
	n6 [label="*", style=filled /*, color=lightgray */];
	n7 [shape=box, label="12", color=lightgray];
	n8 [shape=box, label="1", color=lightgray];
	n9 [label="*", color=lightgray];
	n10 [label="*", color=lightgray];
	n11 [label="*", color=lightgray];	
	n12 [label="+", color=lightgray];	
	n13 [label="*", style=filled, color=lightgray];
	n14 [label="+", color=lightgray];
	n15 [label="+", style=filled, color=lightgray];
	n16 [shape=box, label="0", color=lightgray];		
}
\end{center}
Both the left and the right input to this multiplication gate are labeled by $S_2$ and we therefore have to add the following constraint to the system:
$$
S_2 \cdot S_2 = S_4
$$
Edge label $S_5$ is more interesting. To see if it implies a constraint, we have to construct the associated subgraph first, which consists of all edges and all nodes in all path starting either at a constant input or a labeled edge. We get  
\begin{center}
\digraph[scale=0.4]{G2G}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n4 [label="S_1" labeldistance="4", color=lightgray];
        n1 -> n4 [ color=lightgray ]
	n2 -> n6 [label="S_2", color=lightgray];
	n2 -> n6 [ color=lightgray ] ;
	n3 -> n9 [ color=lightgray ] ;
	n4 -> n9 [taillabel="S_3", labeldistance="2", color=lightgray];
	n4 -> n13 [taillabel="S_3", labeldistance="4" /*, color=lightgray */];
	n5 -> n10 /*[ color=lightgray ]*/ ;
	n6 -> n10 [headlabel="S_4", labeldistance="4" /*, color=lightgray */];
	n6 -> n11 [taillabel="S_4", labeldistance="4", color=lightgray ];
	n7 -> n11 [ color=lightgray ] ;
	n8 -> n12 [ color=lightgray ] ;
	n9 -> n12 [ color=lightgray ] ;
	n10 -> n13 /*[ color=lightgray ]*/ ; 
	n11 -> n15 [ color=lightgray ] ;
	n12 -> n14 [ color=lightgray ] ;	
	n13 -> n14 [xlabel="S_5  "  /*, color=lightgray */];
	n14 -> n15 [ color=lightgray ] ;
	n15 -> n16 [label="  S_6=0", labeldistance="2" , color=lightgray ];
	n1 [shape=box, label="x", color=lightgray];
	n2 [shape=box, label="y", color=lightgray];
	n3 [shape=box, label="10", color=lightgray];
	n4 [label="*", style=filled /*, color=lightgray*/];
	n5 [shape=box, label="8" /*, color=lightgray */];
	n6 [label="*", style=filled /*, color=lightgray */];
	n7 [shape=box, label="12", color=lightgray];
	n8 [shape=box, label="1", color=lightgray];
	n9 [label="*", color=lightgray];
	n10 [label="*" /*, color=lightgray */];
	n11 [label="*", color=lightgray];	
	n12 [label="+", color=lightgray];	
	n13 [label="*", style=filled /*, color=lightgray */];
	n14 [label="+", color=lightgray];
	n15 [label="+", style=filled, color=lightgray];
	n16 [shape=box, label="0", color=lightgray];		
}
\end{center}
The right input to the associated multiplication gate is given by the labeled edge $S_3$. However the left input is not a labeled edge, but has a labeled edge in one of its path. This implies that we have to add a constraint to the system. To comput the left factor of that constraint, we have to compute the output of subgraph associated to the left edge, which is $8\cdot W_2$. This gives the constraint
$$
(S_4 \cdot 8) \cdot S_3 = S_5
$$ 
The last edge label is the constant $S_6=0$. To see if it implies a constraint, we have to construct the associated subgraph, which consists of all edges and all nodes in all path starting either at a constant input or a labeled edge. We get
\begin{center}
\digraph[scale=0.4]{G2H}{
	forcelabels=true;
	center=true;
	splines=ortho;
	nodesep= 2.0;
	n1 -> n4 [label="S_1" /* comment*/ labeldistance="4", color=lightgray];
    n1 -> n4 [ color=lightgray ]
	n2 -> n6 [label="S_2", color=lightgray];
	n2 -> n6 [ color=lightgray ] ;
	n3 -> n9 /*[ color=lightgray ]*/ ;
	n4 -> n9 [taillabel="S_3", labeldistance="2" /*, color=lightgray */];
	n4 -> n13 [taillabel="S_3", labeldistance="4" , color=lightgray ];
	n5 -> n10 [ color=lightgray ] ;
	n6 -> n10 [headlabel="S_4", labeldistance="4" , color=lightgray ];
	n6 -> n11 [taillabel="S_4", labeldistance="4" /*, color=lightgray */];
	n7 -> n11 /*[ color=lightgray ]*/ ;
	n8 -> n12 /*[ color=lightgray ]*/ ;
	n9 -> n12 /*[ color=lightgray ]*/ ;
	n10 -> n13 [ color=lightgray ] ; 
	n11 -> n15 /*[ color=lightgray ]*/ ;
	n12 -> n14 /*[ color=lightgray ]*/ ;	
	n13 -> n14 [xlabel="S_5  "  /*, color=lightgray */];
	n14 -> n15 /*[ color=lightgray ]*/ ;
	n15 -> n16 [label="  S_6=0", labeldistance="2" /*, color=lightgray */];
	n1 [shape=box, label="x", color=lightgray];
	n2 [shape=box, label="y", color=lightgray];
	n3 [shape=box, label="10" /*, color=lightgray */];
	n4 [label="*", style=filled /*, color=lightgray */];
	n5 [shape=box, label="8", color=lightgray];
	n6 [label="*", style=filled /*, color=lightgray */];
	n7 [shape=box, label="12" /*, color=lightgray */];
	n8 [shape=box, label="1" /*, color=lightgray */];
	n9 [label="*" /*, color=lightgray */];
	n10 [label="*" , color=lightgray];
	n11 [label="*" /*, color=lightgray */];	
	n12 [label="+" /*, color=lightgray */];	
	n13 [label="*", style=filled /*, color=lightgray */];
	n14 [label="+" /*, color=lightgray */];
	n15 [label="+", style=filled /*, color=lightgray */];
	n16 [shape=box, label="0" /*, color=lightgray */];		
}
\end{center}
Both the left and the right input are not labeled, but have a labeled edges in their path. This implies that we have to add a constraint to the system. Since the gate is an addition gate, the right factor in the quadratic constraint is always $1$ and the left factor is computed by symbolically executing all inputs to all gates in subcircuit. We get
$$
(12\cdot S_4 + S_5 + 10\cdot S_3 + 1)\cdot 1 = 0
$$
Since there are no more labeled outgoing edges, we are done deriving the constraints. Combining all constraints together, we get the following R1CS:
\begin{align*}
 S_1 \cdot S_1 &= S_3\\
 S_2 \cdot S_2 &= S_4\\
 (S_4\cdot 8)\cdot S_3 &= S_5\\
 (12\cdot S_4 + S_5 + 10\cdot S_3 + 1)\cdot 1 &= 0
\end{align*}
which is equivalent to the R1CS we derived in example XXX. The languages $L_{3.fac\_zk}$ and $L_{3.fac\_circ}$ are therefore equivalent and both the circuit as well as the R1CS are just two different ways to express the same language.
\end{example}
