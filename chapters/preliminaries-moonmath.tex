\chapter{Preliminaries}

Introduction and summary of what we do in this chapter

\section{Cryptological Systems}
The science of information security is referred to as \textit{cryptology}. In the broadest sense, it deals with encryption and decryption processes, with digital signatures, identification protocols, cryptographic hash functions, secrets sharing, electronic voting procedures and electronic money. EXPAND

\section{SNARKS}



\section{complexity theory}
Before we deal with the mathematics behind zero knowledge proof systems, we must first clarify what is meant by the runtime of an algorithm or the time complexity of an entire mathematical problem. This is particularly important for us when we analyze the various snark systems...

For the reader who is interested in complexity theory, we recommend, or example 
%\cite{BE} 
or 
%\cite{AB}
, as well as the references contained therein.

\subsection{Runtime complexity}
The runtime complexity of an algorithm describes, roughly speaking, the amount of elementary computation steps that this algorithm requires in order to solve a problem, depending on the size of the input data.

Of course, the exact amount of arithmetic operations required depends on many factors such as the implementation, the operating system used, the CPU and many more. However, such accuracy is seldom required and is mostly meaningful to consider only the asymptotic computational effort.

In computer science, the runtime of an algorithm is therefore not specified in individual calculation steps, but instead looks for an upper limit which approximates the runtime as soon as the input quantity becomes very large. This can be done using the so-called \textit{Landau notation} (also called big -$\mathcal{O}$-notation) A precise definition
would, however, go beyond the scope of this work and we therefore refer the reader to 
%\cite{AB}
.

For us, only a rough understanding of transit times is important in order to be able to talk about the security of crypographic systems. For example, $\mathcal{O}(n)$ means that the running time of the algorithm to be considered is linearly dependent on the size of the input set $n$, $\mathcal{O}(n^k)$ means that the running time is polynomial and $\mathcal{O}(2^n) $ stands for an exponential running time (%\cite{JB} 
chapter 2.4).


An algorithm which has a running time that is greater than a polynomial is often simply referred to as \textit{slow}.

A generalization of the runtime complexity of an algorithm is the so-called \textit{time complexity of a mathematical problem}, which is defined as the runtime of the fastest possible algorithm that can still solve this problem (
%\cite{AB} 
chapter 3.1).

Since the time complexity of a mathematical problem is concerned with the runtime analysis of all possible (and thus possibly still undiscovered) algorithms, this is often a very difficult and deep-seated question .

For us, the time complexity of the so-called discrete logarithm problem will be important. This is a problem for which we only know slow algorithms on classical computers at the moment, but for which at the same time we cannot rule out that faster algorithms also exist.

