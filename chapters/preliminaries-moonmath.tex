\chapter{Preliminaries}

Introduction and summary of what we do in this chapter

\section{Cryptological Systems}
The science of information security is referred to as \textit{cryptology}. In the broadest sense, it deals with encryption and decryption processes, with digital signatures, identification protocols, cryptographic hash functions, secrets sharing, electronic voting procedures and electronic money. EXPAND

\section{SNARKS}



\section{complexity theory}
Before we deal with the mathematics behind zero knowledge proof systems, we must first clarify what is meant by the runtime of an algorithm or the time complexity of an entire mathematical problem. This is particularly important for us when we analyze the various snark systems...

For the reader who is interested in complexity theory, we recommend, or example 
%\cite{BE} 
or 
%\cite{AB}
, as well as the references contained therein.

\subsection{Runtime complexity}
The runtime complexity of an algorithm describes, roughly speaking, the amount of elementary computation steps that this algorithm requires in order to solve a problem, depending on the size of the input data.

Of course, the exact amount of arithmetic operations required depends on many factors such as the implementation, the operating system used, the CPU and many more. However, such accuracy is seldom required and is mostly meaningful to consider only the asymptotic computational effort.

In computer science, the runtime of an algorithm is therefore not specified in individual calculation steps, but instead looks for an upper limit which approximates the runtime as soon as the input quantity becomes very large. This can be done using the so-called \textit{Landau notation} (also called big -$\mathcal{O}$-notation) A precise definition
would, however, go beyond the scope of this work and we therefore refer the reader to 
%\cite{AB}
.

For us, only a rough understanding of transit times is important in order to be able to talk about the security of crypographic systems. For example, $\mathcal{O}(n)$ means that the running time of the algorithm to be considered is linearly dependent on the size of the input set $n$, $\mathcal{O}(n^k)$ means that the running time is polynomial and $\mathcal{O}(2^n) $ stands for an exponential running time (%\cite{JB} 
chapter 2.4).


An algorithm which has a running time that is greater than a polynomial is often simply referred to as \textit{slow}.

A generalization of the runtime complexity of an algorithm is the so-called \textit{time complexity of a mathematical problem}, which is defined as the runtime of the fastest possible algorithm that can still solve this problem (
%\cite{AB} 
chapter 3.1).

Since the time complexity of a mathematical problem is concerned with the runtime analysis of all possible (and thus possibly still undiscovered) algorithms, this is often a very difficult and deep-seated question .

For us, the time complexity of the so-called discrete logarithm problem will be important. This is a problem for which we only know slow algorithms on classical computers at the moment, but for which at the same time we cannot rule out that faster algorithms also exist.

\section{Hash functions}
We assume that $H: \{0,1\}^* \to {0,1}^k$ is a \textbf{hash function} that maps binary strings of arbitrary length onto strings of length $k$. In addition we define a hash function to be $l$-bounded if it is only able to map from binary strings of length $l$ to binary strings of length $k$. 

STUFF ON CRYPTOGRAPHIC HASH FUNCTIOND

\paragraph{p\&{}p-hash}
In this example we define a $16$-bounded pen\&{}paper hash function that is simple enough to be computed without a computer. We call it the PaP-Hash and will use it throughout the book as a basic example whenever hashing is involved in other example.

The PaP-Hash $\mathcal{H}_{PaP}: \{0,1\}^{16}\to \{0,1\}^4$ is defined in the following way:
\begin{itemize}
\item Decompose the $16$-bit preimage $S=(s_0,s_1,\ldots,s_{15})$ into $4$ chunks $S_i=(s_{4i+0},\ldots,s_{4i+3})$ for $i\in \{0,1,2,3\}$.
\item For each chunk $S_i$ do a circular bitshift $\Zmod{s_{j}\to s_{j+1}}{4}$ for all $s_j\in S_i$
\item Xor all four chunks together $S = S_1\; XOR \; S_2\; XOR \; S_3\; XOR \; S_3$
\item Compute the result $\mathcal{H}_{PaP}(S) = S \; XOR\; (1001)$
\end{itemize}

\begin{example}
Lets compute our PaP-Hash on a concrete example string $S=(1110011101110011)$. Then the decomposition is $S_0=(1110)$, $S_1=(0111)$, $S_2=(0111)$ and $S_3=(0011)$ and after a circular bitshift we get $S'_0=(0111)$, $S'_1=(1011)$, $S'_2=(1011)$ and $S'_3=(1001)$. Xoring everything together we get $S= (0111) \; XOR \; (1011)\; XOR \; (1011)\; XOR \; (1001) = (1100) \; XOR\;
(0010) = (1110)$. So we get $\mathcal{H}_{PaP}(1010 0111 0110 0011) = (1110)$.
\end{example}


\section{Software Used in This Book}

It order to provide an interactive learning experience, and to allow getting hands-on with the concepts described in this book, we give examples for how to program them in the Sage programming language. Sage is a dialect of the learning-friendly programming language Python, which was extended and optimized for computing with, in and over algebraic objects. Therefore, we recommend installing Sage before diving into the following chapters.

The installation steps for various system configurations are described on the sage websit \footnote{\url{https://doc.sagemath.org/html/en/installation/index.html}}. Note however that we use Sage version 9, so if you are using Linux and your package manager only contains version 8, you may need to choose a different installation path, such as using prebuilt binaries.

We recommend the interested reader, who is not familiar with sagemath to read on the many tutorial before starting this book. For example 
%https://doc.sagemath.org/pdf/en/tutorial/SageTutorial.pdf

% Note: Logging Input and Output
% You can use this command to log all input you type, all output, and even play back that input in a future session (by simply reloadingthe log file).


